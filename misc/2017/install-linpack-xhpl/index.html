<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>在MPI集群执行Linpack测试 | Ying的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="使用虚拟机搭建一个3台VM的CentOS 7集群，安装mpi，blas和hpl，执行linpack测试集群的计算性能。尝试编译netlib hpl 2.2时遇到一个错误，导致不能使用mpi。后来发现可以直接使用Intel编译好的xhpl。由于不理解mpi和xhpl的选项，集群的性能比单机还明显低得多，还没找到原因;-(">
<meta property="og:type" content="article">
<meta property="og:title" content="在MPI集群执行Linpack测试">
<meta property="og:url" content="https://ying-zhang.github.io/misc/2017/install-linpack-xhpl/index.html">
<meta property="og:site_name" content="Ying的博客">
<meta property="og:description" content="使用虚拟机搭建一个3台VM的CentOS 7集群，安装mpi，blas和hpl，执行linpack测试集群的计算性能。尝试编译netlib hpl 2.2时遇到一个错误，导致不能使用mpi。后来发现可以直接使用Intel编译好的xhpl。由于不理解mpi和xhpl的选项，集群的性能比单机还明显低得多，还没找到原因;-(">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2017-10-30T03:41:18.346Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="在MPI集群执行Linpack测试">
<meta name="twitter:description" content="使用虚拟机搭建一个3台VM的CentOS 7集群，安装mpi，blas和hpl，执行linpack测试集群的计算性能。尝试编译netlib hpl 2.2时遇到一个错误，导致不能使用mpi。后来发现可以直接使用Intel编译好的xhpl。由于不理解mpi和xhpl的选项，集群的性能比单机还明显低得多，还没找到原因;-(">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <div class="outer">
        <section id="main"><article id="post-install-linpack-xhpl" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      在MPI集群执行Linpack测试
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/misc/2017/install-linpack-xhpl/" class="article-date">
  <time datetime="2017-04-11T16:00:00.000Z" itemprop="datePublished">2017-04-12</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/misc/">misc</a>
  </div>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>使用虚拟机搭建一个3台VM的CentOS 7集群，安装mpi，blas和hpl，执行linpack测试集群的计算性能。<br>尝试编译netlib hpl 2.2时遇到一个错误，导致不能使用mpi。<br>后来发现可以直接使用Intel编译好的xhpl。由于不理解mpi和xhpl的选项，集群的性能比单机还明显低得多，还没找到原因;-(<br><a id="more"></a></p>
<!-- TOC -->
<ul>
<li><a href="#%E5%AE%89%E8%A3%85%E5%92%8C%E8%AE%BE%E7%BD%AEmpich%EF%BC%8Cblas%EF%BC%88atlas%EF%BC%89%EF%BC%8Chpl">安装和设置mpich，blas（atlas），hpl</a><ul>
<li><a href="#%E8%AE%BE%E7%BD%AEmpich">设置mpich</a></li>
<li><a href="#blas-linpacklapack-hpl">BLAS, LINPACK/LAPACK, HPL</a></li>
<li><a href="#%E5%8D%95%E6%9C%BA%E4%B8%8A%E6%89%A7%E8%A1%8C-hpl">单机上执行 HPL</a></li>
</ul>
</li>
<li><a href="#%E4%BD%BF%E7%94%A8intel-mkl-benchmarks">使用Intel MKL Benchmarks</a></li>
<li><a href="#todo-%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%B5%8B%E8%AF%95mpi-xhpl">Todo: 集群中测试mpi xhpl</a></li>
</ul>
<!-- /TOC -->
<h1 id="安装和设置mpich，blas（atlas），hpl"><a href="#安装和设置mpich，blas（atlas），hpl" class="headerlink" title="安装和设置mpich，blas（atlas），hpl"></a>安装和设置mpich，blas（atlas），hpl</h1><p>编译可只在一台VM上进行，然后将编译的结果拷贝到其它VM。</p>
<h2 id="设置mpich"><a href="#设置mpich" class="headerlink" title="设置mpich"></a>设置mpich</h2><p>参考 <a href="http://www.mpich.org/static/downloads/3.2/mpich-3.2-installguide.pdf" target="_blank" rel="external">MPI安装手册</a> 和 <a href="http://www.mpich.org/static/downloads/3.2/mpich-3.2-userguide.pdf" target="_blank" rel="external">MPI用户手册</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># 安装编译器</div><div class="line">yum install -y gcc gcc-gfortran gcc-c++ bzip2 wget</div><div class="line"></div><div class="line"># yum 可以安装mpich-3.2.x86_64，但只有lib，没有bin，所以这里从src编译</div><div class="line">wget http://www.mpich.org/static/downloads/3.2/mpich-3.2.tar.gz</div><div class="line">tar axf mpich-3.2.tar.gz</div><div class="line">cd ~/mpich-3.2</div><div class="line">./configure prefix=/opt/mpich</div><div class="line">make -j 8 &amp;&amp; make install  # make只会编译lib，make install才会编译lib和bin</div><div class="line"></div><div class="line">cp -r examples/ /opt/mpich/</div><div class="line"></div><div class="line"># 把编译结果打包</div><div class="line">cd ~</div><div class="line">tar zcf mpich-3.2-build.tar.gz /opt/mpich/</div><div class="line"># 可以将其保存到主机上</div><div class="line"></div><div class="line">echo &apos;export PATH=$PATH:/opt/mpich/bin&apos;                         &gt;&gt; /etc/profile</div><div class="line">echo &apos;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/mpich/lib&apos;   &gt;&gt; /etc/profile</div><div class="line">source /etc/profile</div><div class="line"></div><div class="line"># 在本机测试一下</div><div class="line">mpiexec -n 3 /opt/mpich/examples/cpi</div></pre></td></tr></table></figure></p>
<p>mpiexec会在单个主机创建N个进程（通过 -n 指定）执行后面的命令（程序），如果通过 -f host_list_file 指定集群节点列表，会把进程分布在这些节点上分布执行。<br>mpiexec执行的可以是普通的命令，这时只是重复地执行N次，但这些命令之间并没有什么联系。<br>如果执行的是一个使用了mpi库的程序，那么程序执行中会彼此通信，协调计算进度，从而充分利用集群的计算资源。<br>要在集群上运行mpi程序，需要所有的节点上都有这个程序的可执行文件，以及需要的数据，配置文件，环境变量等。当然可以将这些文件等拷贝到各节点，但一般会创建一个共享目录，集群中的节点都将共享目录挂载到相同的路径，并将mpi程序及相关文件放到共享目录下。</p>
<h2 id="BLAS-LINPACK-LAPACK-HPL"><a href="#BLAS-LINPACK-LAPACK-HPL" class="headerlink" title="BLAS, LINPACK/LAPACK, HPL"></a>BLAS, LINPACK/LAPACK, HPL</h2><p><a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" target="_blank" rel="external">BLAS（Basic Linear Algebra Subprograms）- wiki</a> 或 <a href="https://zh.wikipedia.org/wiki/BLAS" target="_blank" rel="external">BLAS基础线性代数程序集 - wiki</a> 是一个API标准，有多个开源实现，如Netlib BLAS（Fortran实现），Netlib ATLAS，Intel MKL和ACML等。这里使用ATLAS。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">wget https://downloads.sourceforge.net/project/math-atlas/Stable/3.10.3/atlas3.10.3.tar.bz2</div><div class="line">tar axf atlas3.10.3.tar.bz2   # 需要已安装bzip2</div><div class="line">cd ATLAS</div><div class="line">mkdir build; cd build</div><div class="line">../configure</div><div class="line">make -j 8 &amp;&amp; make install # 编译完成后安装到/usr/local/atlas，包括 lib 和 include</div><div class="line"># 同样将编译的结果也打包保存</div><div class="line">mkdir atlas</div><div class="line">mv bin/ lib/ include/ atlas/</div><div class="line">tar zcf atlas-build.tar.gz atlas/</div></pre></td></tr></table></figure></p>
<p><a href="https://en.wikipedia.org/wiki/LINPACK" target="_blank" rel="external">LINPACK</a>是一个线性代数数值计算库，其中用到了BLAS。不过目前多是使用它的后继<a href="https://en.wikipedia.org/wiki/LAPACK" target="_blank" rel="external">LAPACK</a>。<a href="http://www.netlib.org/benchmark/hpl/" target="_blank" rel="external">HPLinpack（Highly Parallel Computing benchmark，HP不是指惠普公司）</a>是一个使用LINPACK测试集群浮点计算性能的测试基准程序，测试的结果是多少GFPLOPS。</p>
<p>参考 <a href="http://blog.chinaunix.net/uid-20104120-id-4071017.html" target="_blank" rel="external">http://blog.chinaunix.net/uid-20104120-id-4071017.html</a> 。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">wget http://www.netlib.org/benchmark/hpl/hpl-2.2.tar.gz</div><div class="line">tar axf hpl-2.2.tar.gz</div><div class="line">cd hpl-2.2</div><div class="line">cp setup/Make.Linux_PII_CBLAS_gm Make.x86_64</div><div class="line"></div><div class="line"># 编辑 Make.x86_64，修改的内容如下。</div><div class="line"># 因为设置MPdir后有编译错误，所以没有设置。这样编译出来的是单机版的。</div><div class="line"># 注意各值结尾不要有 空格。</div><div class="line"># 虽然在 INCdir、BINdir和LIBdir删掉了$(ARCH)，但最终还创建了x86_64的空目录</div><div class="line">ARCH         = x86_64</div><div class="line"></div><div class="line">TOPdir       = $(HOME)/hpl-2.2</div><div class="line">INCdir       = $(TOPdir)/include</div><div class="line">BINdir       = $(TOPdir)/bin</div><div class="line">LIBdir       = $(TOPdir)/lib</div><div class="line"></div><div class="line">MPdir        =</div><div class="line">MPinc        =</div><div class="line">MPlib        =</div><div class="line"></div><div class="line">LAdir        = /usr/local/atlas  # atlas 执行了make install之后的安装目录</div><div class="line">LAinc        = -I$(LAdir)/include</div><div class="line">LAlib        = $(LAdir)/lib/libcblas.a $(LAdir)/lib/libatlas.a</div><div class="line"></div><div class="line"># 开始编译</div><div class="line">make arch=x86_64 -j 8</div><div class="line"></div><div class="line"># 也将编译的结果也打包保存</div><div class="line">mv hpl hpl.bk.d; mkdir hpl</div><div class="line">mv bin/ lib/ include/ hpl/</div><div class="line">rmdir hpl/bin/x86_64/ hpl/include/x86_64/ hpl/lib/x86_64/</div><div class="line">tar zcf hpl-build.tar.gz hpl/</div></pre></td></tr></table></figure></p>
<h2 id="单机上执行-HPL"><a href="#单机上执行-HPL" class="headerlink" title="单机上执行 HPL"></a>单机上执行 HPL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /root/hpl-2.2/hpl/bin/</div><div class="line">mpiexec -n 4 ./xhpl  # 得到的结果比较差，只有约0.4GFLOPS</div><div class="line"># 因为需要配置HPL.dat，选择合适的参数，另外编译中一些选项也会有影响</div></pre></td></tr></table></figure>
<h1 id="使用Intel-MKL-Benchmarks"><a href="#使用Intel-MKL-Benchmarks" class="headerlink" title="使用Intel MKL Benchmarks"></a>使用Intel MKL Benchmarks</h1><p>参考 <a href="https://software.intel.com/en-us/articles/intel-mkl-benchmarks-suite" target="_blank" rel="external">https://software.intel.com/en-us/articles/intel-mkl-benchmarks-suite</a> 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">wget http://registrationcenter-download.intel.com/akdlm/irc_nas/9752/l_mklb_p_2017.2.015.tgz</div><div class="line">tar axf l_mklb_p_2017.2.015.tgz</div><div class="line">cd l_mklb_p_2017.2.015/benchmarks_2017/linux/mkl/benchmarks/linpack</div><div class="line">./runme_xeon64</div></pre></td></tr></table></figure>
<p>测试的结果约 150GFLOPS。<br>还有一个Windows版的，在主机上测试也是接近的结果。</p>
<h1 id="Todo-集群中测试mpi-xhpl"><a href="#Todo-集群中测试mpi-xhpl" class="headerlink" title="Todo: 集群中测试mpi xhpl"></a>Todo: 集群中测试mpi xhpl</h1><p>直接运行 <code>l_mklb_p_2017.2.015/benchmarks_2017/linux/mkl/benchmarks/mp_linpack</code> 中的 <code>runme_intel64_static</code> 会报不识别 perhost 参数的错误。<br>执行 <code>mpiexec -f hosts -n 12 ./xhpl_intel64_static</code>（xhpl_intel64_static和HPL.dat拷贝到了/root，hosts是节点列表），在其它节点通过<code>top</code>监视，确实执行了xhpl_intel64_static，但输出只有当前机器的，而且只有约4GFLOPS（单机执行xhpl_intel64_static也是这么多，按理应接近4×集群节点数啊），不知道问题具体出在哪里，看来还是需要仔细看文档了。或许MPI，BLAS等全部使用Intel的版本？<br>参考</p>
<ul>
<li><a href="https://software.intel.com/zh-cn/articles/intel-mpi-library-documentation" target="_blank" rel="external">Intel® MPI Library - Documentation</a></li>
<li><a href="https://software.intel.com/en-us/node/528457" target="_blank" rel="external">Intel® Optimized MP LINPACK Benchmark for Clusters</a></li>
<li><a href="https://software.intel.com/en-us/articles/performance-tools-for-software-developers-hpl-application-note" target="_blank" rel="external">HPL application note</a></li>
<li><a href="http://khmel.org/?p=527" target="_blank" rel="external">HPC LINPACK benchmark</a></li>
<li><a href="http://blog.sciencenet.cn/blog-935970-892936.html" target="_blank" rel="external">如何做LINPACK测试及性能优化</a></li>
</ul>

      

      
        
    </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/misc/2017/setup-squid-proxy/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          设置及使用HTTP代理
        
      </div>
    </a>
  
  
    <a href="/cloud/2017/vm-net-2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">再说docker及云中的网络连接</div>
    </a>
  
</nav>

  
</article>

</section>
        <aside id="sidebar">
  <nav class="menus">
  	<ul>
  		<li><a href="/"><i class="icon icon-home"></i></a></li>
  		
			<li><a href="/archives"><i class="icon icon-fenlei"></i></a></li>
  		
  		
  			<li><a href="https://github.com/ying-zhang" target="_blank"><i class="icon icon-github"></i></a></li>
  		
			
  			<li><a href="/atom.xml" target="_blank"><i class="icon icon-tag"></i></a></li>
  		
  	</ul>
  </nav>
  <a id="go-top" href="#"><i class="icon icon-up"></i></a>
</aside>
      </div>
      <footer id="footer">
  
	<div id="footer-info" class="inner">
	  &copy; 2017 Ying ZHANG 
	  - Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	  - Theme <a href="https://github.com/hejianxian/hexo-theme-jane/" target="_blank">Jane</a>
		- Project Source at <a href="https://github.com/ying-zhang/ying-zhang.github.io/" target="_blank">Github</a>
	</div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="https://github.com/ying-zhang" class="mobile-nav-link">Github</a>
  
    <a href="/atom.xml" class="mobile-nav-link">RSS</a>
  
</nav>
    

<script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>