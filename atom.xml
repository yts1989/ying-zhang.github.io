<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ying的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ying-zhang.github.io/"/>
  <updated>2018-01-11T08:39:29.170Z</updated>
  <id>https://ying-zhang.github.io/</id>
  
  <author>
    <name>Ying ZHANG</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>书单</title>
    <link href="https://ying-zhang.github.io/misc/2018/book-list/"/>
    <id>https://ying-zhang.github.io/misc/2018/book-list/</id>
    <published>2018-01-11T16:00:00.000Z</published>
    <updated>2018-01-11T08:39:29.170Z</updated>
    
    <content type="html"><![CDATA[<p>入职半年的Y让我推荐一些书。<br>说来惭愧，自己的水平本来就不怎么样，最近一年更是浑浑噩噩，难说有资格推荐；再者，怕推荐的书单跟网上搜到的结果也不会有太大区别，还怕不合适的书耽误了他的时间，毕竟他的 <strong>时薪</strong> 比市面上绝大部分书的定价都多得多;-)<br>于是拖着过了元旦，Y的饭都吃过了，不能老拖着了……</p><a id="more"></a><h1 id="闲言：少读书"><a href="#闲言：少读书" class="headerlink" title="闲言：少读书"></a>闲言：少读书</h1><p>每周二学校图书馆新书上架，我都跑过去看，一般都会有一两本感兴趣的，借来翻翻，大部分下周二就还了，那时又有一批新书上架了。浏览了一下自己的借阅记录，还有向图书馆荐购的记录，两百多条，但发现并没有学到什么东西，没有比较深的印象；其中值得推荐的，只有几本书，自己也买了，但内容与目前的学习关联不大，只是作为收藏。<br>计算机专业 </p><p>觉得<a href="https://www.zhihu.com/question/20169741/answer/14206148" target="_blank" rel="external">知乎上Halty的一个回答</a> 说得比较有道理：</p><blockquote><p>一家之言：如果只是为了看书而看书，或者为了学习而学习，没有多大的效果和意义。建议你在碰到问题时去翻这本书，个人觉得知识点讲的很透彻。其中面向对象等前几章基础部分要有个了解，可在后期逐步加深；中间的容器，字符串，内部类，javaIO等几章需要运用的很熟练；后面的异常，枚举，RTTI，泛型，注解，并发几章在实际开发中应用的十分普遍，需要好好钻研。至于GUI编程如果是java客户端开发，需要好好钻研，web开发的话，稍微了解一下即可。<br>总而言之，还是那个原则：带着问题去看书，那样目标比较明确，效率也比较高。</p></blockquote><hr><h1 id="2017-买的书"><a href="#2017-买的书" class="headerlink" title="2017 买的书"></a>2017 买的书</h1><p>看过或准备看的：</p><ul><li>机器学习（周志华老师的西瓜书，实际是2016年买的，小组经费）</li><li>深度学习</li><li>深度学习优化与识别</li><li>Netty实战</li><li>R语言实战（第2版）</li><li>Kotlin实战</li><li>Scala程序设计（第2版）</li><li>Go并发编程实战（第2版）</li><li>Linux多线程服务端编程：使用muduo C++网络库</li><li>图解Spark： 核心技术与案例实战</li><li>性能之巅： 洞悉系统、企业与云计算 （小组经费）    </li><li>深入理解计算机系统（第3版） （小组经费）</li><li>SRE:Google运维解密 （小组经费）</li><li>ZooKeeper:分布式过程协同技术详解 （小组经费）</li></ul><p>虽然买了好几本“实战”，但都没实践过……</p><p>下面的仅为收藏：</p><ul><li>垃圾回收算法手册：自动内存管理的艺术</li><li>现代操作系统（原书第4版）</li><li>大型汽轮发电机设计、制造与运行</li></ul><p>《大型汽轮发电机设计、制造与运行》</p><h1 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h1><h2 id="买的书"><a href="#买的书" class="headerlink" title="买的书"></a>买的书</h2><p>《构建之法（第三版）》和《深入理解并行编程》，天猫，共113.60元<br>《设计数据密集型应用（影印版）Designing data-intensive applications》，天猫，67.30元</p><blockquote><p>说明：</p><ul><li>《构建之法》是冲着作者邹欣的名气买的，Y要给他推荐几本书，给他列书单之前，自己还是要浏览一下的。</li><li>《深入理解并行编程》是 <em><a href="(https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html">Is Parallel Programming Hard, And, If So, What Can You Do About It?</a>)</em> 译本，原书是开源的，链接分别为 <a href="http://kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook-1c.2017.11.22a.pdf" target="_blank" rel="external">PDF版</a> 和 <a href="https://github.com/pranith/perfbook" target="_blank" rel="external">Github Repo</a>，同译者2011年版的译本也是公开的，翻了一下，内容还是很充实的，先买来收藏；</li><li>《Designing data-intensive applications》是知乎上看到的，浏览了一下目录，比较全面地介绍了分布式系统各方面的内容，虽然有D版的，还是看实体书吧；</li></ul></blockquote><h2 id="要看的书"><a href="#要看的书" class="headerlink" title="要看的书"></a>要看的书</h2><p>机器学习/深度学习 关于强化学习的章节</p><h1 id="要看的文章"><a href="#要看的文章" class="headerlink" title="要看的文章"></a>要看的文章</h1><p>SoCC，SOSP，EuroSys，ATC 2017</p><p>于2018.01.08</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;入职半年的Y让我推荐一些书。&lt;br&gt;说来惭愧，自己的水平本来就不怎么样，最近一年更是浑浑噩噩，难说有资格推荐；再者，怕推荐的书单跟网上搜到的结果也不会有太大区别，还怕不合适的书耽误了他的时间，毕竟他的 &lt;strong&gt;时薪&lt;/strong&gt; 比市面上绝大部分书的定价都多得多;-)&lt;br&gt;于是拖着过了元旦，Y的饭都吃过了，不能老拖着了……&lt;/p&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>CentOS 7 安装支持认证的Mesos集群</title>
    <link href="https://ying-zhang.github.io/cloud/2017/setup-mesos-with-auth/"/>
    <id>https://ying-zhang.github.io/cloud/2017/setup-mesos-with-auth/</id>
    <published>2017-12-19T16:00:00.000Z</published>
    <updated>2018-01-14T02:16:13.328Z</updated>
    
    <content type="html"><![CDATA[<p>在CentOS 7上安装Mesos集群，设置对Slave和框架的认证，改为普通用户执行任务。Chronos在容器中执行GPU作业。</p><hr><a id="more"></a><blockquote><p>注意：<br>以下的设置都是以root用户权限执行的。<br>机器的操作系统是CentOS 7，机器名n5（<code>/etc/hostname</code>和<code>/etc/hosts</code>都设置了机器名），IP地址10.1.1.5 。<br>为了简便，Mesos Master和Slave在同一台机器上，Zookeeper也是单机运行模式。</p></blockquote><h1 id="Mesos集群基本设置"><a href="#Mesos集群基本设置" class="headerlink" title="Mesos集群基本设置"></a>Mesos集群基本设置</h1><h2 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h2><p>先安装Open JDK 8<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel</div><div class="line"></div><div class="line"># 在/etc/profile末尾增加环境变量，下文会把Zookeeper安装到/opt/zookeeper</div><div class="line">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk</div><div class="line">export PATH=$PATH:$JAVA_HOME/bin:/opt/zookeeper/bin</div></pre></td></tr></table></figure></p><h2 id="安装Zookeeper"><a href="#安装Zookeeper" class="headerlink" title="安装Zookeeper"></a>安装Zookeeper</h2><p>下载并设置Zookeeper，参考[<a href="https://zookeeper.apache.org/doc/r3.4.11/zookeeperStarted.html" target="_blank" rel="external">https://zookeeper.apache.org/doc/r3.4.11/zookeeperStarted.html</a>] 。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">cd ~</div><div class="line">curl -O http://mirrors.nju.edu.cn/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz</div><div class="line">tar axf zookeeper-3.4.11.tar.gz -C /opt/</div><div class="line">rm  zookeeper-3.4.11.tar.gz</div><div class="line">mv  /opt/zookeeper-3.4.11/ /opt/zookeeper</div><div class="line"></div><div class="line"># 因为只使用一台机器，zk设置了server.1=n5:2881:3881，myid为1</div><div class="line"># 更多的机器需要分别修改 myid 和 server.&lt;id&gt;</div><div class="line">cat &gt; /opt/zookeeper/conf/zoo.cfg &lt;&lt;EOF</div><div class="line">tickTime=2000</div><div class="line">initLimit=10</div><div class="line">syncLimit=5</div><div class="line">dataDir=/var/lib/zookeeper</div><div class="line">dataLogDir=/var/log/zookeeper</div><div class="line">clientPort=2181</div><div class="line">server.1=n5:2881:3881</div><div class="line">EOF</div><div class="line"></div><div class="line">mkdir /var/lib/zookeeper /var/log/zookeeper</div><div class="line">echo  1 &gt;/var/lib/zookeeper/myid</div></pre></td></tr></table></figure></p><p>因为前面将<code>/opt/zookeeper/bin</code>加入了<code>$PATH</code>环境变量，所以可以直接输入下面的命令，</p><ul><li><code>zkServer.sh start</code>，启动Zookeeper。</li><li><code>zkServer.sh status</code>，正常的话会输出包含<code>Mode: standalone</code>的信息，即处于单独运行模式。</li><li><code>zkCli.sh -server n5:2181</code> 或 <code>zkCli.sh</code>，以进入Zookeeper的Shell，在其中查看和修改Zookeeper的值。</li></ul><p>设置Zookeeper的Systemd服务配置文件，编辑文件 <code>/lib/systemd/system/zookeeper.service</code> ，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=Apache Zookeeper</div><div class="line">After=network.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=forking</div><div class="line">User=root</div><div class="line">Group=root</div><div class="line">Environment=JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk</div><div class="line">ExecStart=/opt/zookeeper/bin/zkServer.sh start</div><div class="line">ExecStop=/opt/zookeeper/bin/zkServer.sh stop</div><div class="line">ExecReload=/opt/zookeeper/bin/zkServer.sh restart</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure></p><p>启动Zookeeper服务：<code>systemctl enable zookeeper.service; systemctl start zookeeper.service</code><br>确认服务正常启动了：<code>zkServer.sh status</code></p><h2 id="安装Mesos，Marathon和Chronos"><a href="#安装Mesos，Marathon和Chronos" class="headerlink" title="安装Mesos，Marathon和Chronos"></a>安装Mesos，Marathon和Chronos</h2><p>参考《Mesos实战》，通过Mesosphere的源安装Mesos，Marathon和Chronos。<br>由于Mesosphere（dc/os）修改了文档（可能是为了推广dc/os吧），现在的<a href="https://mesos.apache.org" target="_blank" rel="external">Apache Mesos</a>官方文档不太友好，好在安装包源还可以用。</p><blockquote><p>Ubuntu的源是 [<a href="http://repos.mesosphere.io/ubuntu" target="_blank" rel="external">http://repos.mesosphere.io/ubuntu</a>]</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># rpm -Uvh http://repos.mesosphere.io/el/7/noarch/RPMS/mesosphere-el-repo-7-1.noarch.rpm</div><div class="line">yum install -y http://repos.mesosphere.io/el/7/noarch/RPMS/mesosphere-el-repo-7-1.noarch.rpm</div><div class="line">yum install -y mesos marathon chronos haproxy</div></pre></td></tr></table></figure><p>安装的版本分别是（2017-12-21）：Mesos 1.4.1，Marathon 1.5.4，Chronos 2.5.1</p><p>安装后，会创建 Mesos Master 和 Slave 的Systemd服务配置文件，还设置了<code>/etc/mesos/zk</code>文件内容为<code>zk://localhost:2181/mesos</code>，这正是Zookeeper的默认端口，就无需更改了。</p><p>对多个网卡的机器，如果要mesos使用某个特定的网卡，就需要在<code>/etc/default/mesos-master</code>中设置该网卡对应的IP地址，这里是<code>IP=10.1.1.5</code>。还可以在这个文件设置<code>HOSTNAME</code>（机器名）和<code>CLUSTER</code>（mesos集群名）。<br>其实也可以在这个文件设置<code>zk</code>，不过这只对Master有效。</p><p>在<code>/etc/default/mesos</code>这个文件的设置对master和slave都有效。</p><blockquote><p>一些参数既可以直接作为启动命令的命令行参数，也可以作为环境变量写到上面提到的配置文件中。<br>这些配置文件的路径是硬编码在<code>/usr/bin/mesos-init-wrapper</code>这个启动脚本中的。</p></blockquote><p>然后就可以启动服务了，为了简便，这里关闭了系统的防火墙：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl disable firewalld; systemctl stop firewalld </div><div class="line">systemctl restart mesos-master mesos-slave chronos marathon</div><div class="line">systemctl status  mesos-master mesos-slave chronos marathon</div></pre></td></tr></table></figure></p><blockquote><p>Mesosphere的yum仓库中也有zookeeper，可执行<code>yum install -y mesosphere-zookeeper</code>安装，会安装到<code>/opt/mesosphere/zookeeper/bin/</code>，并生成<code>zookeeper.service</code>的systemd服务（当然需要配置server.id，并手动启用服务）。</p></blockquote><p>通过（其它机器的）浏览器访问<code>http://10.1.1.5:5050</code>，应该就可以打开Mesos Web UI了，在Framworks中会列出Chronos，访问<code>http://10.1.1.5:4400</code>，可以打开Chronos Web UI。但<strong>Marathon没有启动成功</strong>。</p><h2 id="处理Marathon服务启动问题"><a href="#处理Marathon服务启动问题" class="headerlink" title="处理Marathon服务启动问题"></a>处理Marathon服务启动问题</h2><p>参考 [<a href="https://github.com/mesosphere/marathon" target="_blank" rel="external">https://github.com/mesosphere/marathon</a>] 。<br>修改Marathon的Systemd服务配置文件<code>/usr/lib/systemd/system/marathon.service</code>，主要是在启动命令增加了<code>master</code>和<code>zk</code>参数（注意参数与值之间是用空格分隔的，不要写成<strong>等号“=”</strong>；zk的路径是marathon，不是mesos），用户改为<code>root</code>，并改正了<code>mkdir</code>和<code>chmod</code>的完整路径。<br>完整内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=Scheduler for Apache Mesos</div><div class="line">Requires=network.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=simple</div><div class="line">WorkingDirectory=/usr/share/marathon</div><div class="line">EnvironmentFile=/etc/default/marathon</div><div class="line">Environment=&quot;JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk&quot; </div><div class="line">ExecStart=/usr/share/marathon/bin/marathon      \</div><div class="line">    --master n5:5050 --zk zk://n5:2181/marathon</div><div class="line"></div><div class="line">ExecReload=/usr/bin/kill -HUP $MAINPID</div><div class="line">Restart=always</div><div class="line">RestartSec=60</div><div class="line">SuccessExitStatus=</div><div class="line">User=root</div><div class="line">ExecStartPre=/usr/bin/mkdir -p /run/marathon</div><div class="line">ExecStartPre=/usr/bin/chmod 755 /run/marathon</div><div class="line">PermissionsStartOnly=true</div><div class="line">LimitNOFILE=1024</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure><p>保存上述设置文件后，执行<code>systemctl daemon-reload; systemctl restart marathon</code>重启服务，稍等一会儿，Mesos Web UI的Framworks列表中就有Marathon了。Marathon Web UI地址是<code>http://10.1.1.5:8080</code>。</p><h1 id="清理Mesos集群"><a href="#清理Mesos集群" class="headerlink" title="清理Mesos集群"></a>清理Mesos集群</h1><blockquote><p>这节是为强迫症患者准备的。</p></blockquote><p>清理运行任务记录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">systemctl stop mesos-master mesos-slave</div><div class="line">rm -rf /var/mesos /var/log/mesos</div><div class="line">mkdir  /var/mesos /var/log/mesos</div><div class="line"></div><div class="line">/opt/zookeeper/bin/zkCli.sh #进入zk的shell，执行下面的命令</div><div class="line">rmr /mesos</div><div class="line">rmr /chronos</div><div class="line">rmr /marathon</div><div class="line">quit # 退出zk的shell</div><div class="line"></div><div class="line">systemctl start mesos-master mesos-slave</div></pre></td></tr></table></figure></p><p>清理mesos下载的镜像文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">systemctl stop mesos-master mesos-slave</div><div class="line">rm -rf /tmp/mesos</div><div class="line">mkdir  /tmp/mesos</div><div class="line"></div><div class="line">systemctl start mesos-master mesos-slave</div></pre></td></tr></table></figure></p><h1 id="支持GPU资源"><a href="#支持GPU资源" class="headerlink" title="支持GPU资源"></a>支持GPU资源</h1><h2 id="Mesos-Slave的设置"><a href="#Mesos-Slave的设置" class="headerlink" title="Mesos Slave的设置"></a>Mesos Slave的设置</h2><blockquote><p>当然，首先要有GPU硬件，且安装了硬件驱动。可参考<a href="/cloud/2017/setup-tensorflow-gpu-centos7/">CentOS 7 安装TensorFlow GPU深度学习环境</a>。<br>参考：[<a href="http://mesos.apache.org/documentation/latest/gpu-support/" target="_blank" rel="external">http://mesos.apache.org/documentation/latest/gpu-support/</a>]</p></blockquote><p>对GPU的支持是Mesos Slave负责的，在<code>/etc/default/mesos-slave</code>中增加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ISOLATION=&quot;gpu/nvidia,filesystem/linux,docker/runtime,cgroups/cpu,cgroups/mem,network/cni,cgroups/perf_event,posix/disk,cgroups/devices&quot;</div></pre></td></tr></table></figure></p><p>由于使用GPU的容器需要Nvidia提供的运行时插件，Mesos只支持自己的容器引擎加载GPU：它可以使用Docker的镜像文件，但运行时是Mesos实现的，而不是Docker。<br>由此导致容器中的默认环境变量没有包括Nvidia的可执行文件和动态库的路径，为了方便，在Slave设置一个默认的环境变量，同样是在<code>/etc/default/mesos-slave</code>中，增加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">executor_environment_variables=/etc/mesos/slave-executor-env.json</div></pre></td></tr></table></figure></p><p>其中<code>/etc/mesos/slave-executor-env.json</code>是手动创建的文件，内容为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">&quot;PATH&quot;:&quot;/opt/anaconda3/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,</div><div class="line">&quot;LD_LIBRARY_PATH&quot;:&quot;/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64&quot;,</div><div class="line">&quot;http_proxy&quot;: &quot;http://n147:3128&quot;,</div><div class="line">&quot;https_proxy&quot;:&quot;http://n147:3128&quot;,</div><div class="line">&quot;TZ&quot;:&quot;GMT-8&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>先停止Slave服务，删除旧的临时文件<code>rm -f /var/mesos/meta/slaves/latest</code>，<br>然后重启服务<code>systemctl restart mesos-slave</code>，刷新Mesos Web UI，应该就可以看到新增的GPU资源了。</p><h2 id="Marathon的设置"><a href="#Marathon的设置" class="headerlink" title="Marathon的设置"></a>Marathon的设置</h2><p>在Marathon的Systemd服务配置文件的<code>ExecStart</code>处增加参数，完整的启动命令是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ExecStart=/usr/share/marathon/bin/marathon      \</div><div class="line">    --master n5:5050 --zk zk://n5:2181/marathon \</div><div class="line">    --enable_features gpu_resources</div></pre></td></tr></table></figure></p><p>重启服务<code>systemctl daemon-reload; systemctl restart marathon</code>，可以在Marathon Web UI的about页面 [<a href="http://10.1.1.5:8080/ui/#/apps?modal=about" target="_blank" rel="external">http://10.1.1.5:8080/ui/#/apps?modal=about</a>] 确认启用了GPU。</p><h2 id="Chronos的设置"><a href="#Chronos的设置" class="headerlink" title="Chronos的设置"></a>Chronos的设置</h2><p><a href="https://github.com/mesos/chronos" target="_blank" rel="external">Mesos官方版的Chronos</a> 还不支持GPU，我们使用一个修改过的Fork [<a href="https://github.com/reneploetz/chronos" target="_blank" rel="external">https://github.com/reneploetz/chronos</a>] ，从源码编译（需预先安装Maven）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">cd ~</div><div class="line">git clone https://github.com/reneploetz/chronos.git</div><div class="line"></div><div class="line">cd chronos</div><div class="line"># 自带的 ./build-release.sh 脚本是在Docker容器中构建的，这里直接用Maven编译</div><div class="line"># 需要预先安装Maven。构建的版本号是3.0.3。</div><div class="line"></div><div class="line">curl --silent --location https://rpm.nodesource.com/setup_8.x | sudo bash -</div><div class="line">yum install -y nodejs</div><div class="line">mvn clean package -Dmaven.test.skip=true</div></pre></td></tr></table></figure></p><p>下面停用从官方源安装的Chronos服务，从命令行启动支持GPU的Chronos（当然，也可以修改Chronos的Systemd服务配置文件，使用支持GPU的Chronos）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">systemctl disable chronos; systemctl stop chronos</div><div class="line"></div><div class="line">nohup java -jar /root/chronos/target/chronos-3.0.3-SNAPSHOT.jar \</div><div class="line">  --master zk://n5:2181/mesos --http_port=4400                  \</div><div class="line">  --enable_features=gpu_resources &gt;/dev/null                    &amp;</div></pre></td></tr></table></figure></p><p>启动后再次打开Chronos Web UI，<a href="http://10.1.1.5:4400" target="_blank" rel="external">http://10.1.1.5:4400</a> ，发现与官方最新版的不一样了。<br>添加一个Scheduled Job，Job Name，Command都可以随便填。Web UI中不能指定GPU资源量，需要在Job的JSON配置文件中设置。创建Job后，修改其JSON配置文件，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;name&quot;: &quot;host-gpu-test&quot;,</div><div class="line">  &quot;command&quot;: &quot;nvidia-smi &gt; /root/nvidia-smi-out.txt ; whoami; id; pwd&quot;,</div><div class="line">  &quot;shell&quot;: true,</div><div class="line">  &quot;executor&quot;: &quot;&quot;,</div><div class="line">  &quot;executorFlags&quot;: &quot;&quot;,</div><div class="line">  &quot;taskInfoData&quot;: &quot;&quot;,</div><div class="line">  &quot;retries&quot;: 0,</div><div class="line">  &quot;owner&quot;: &quot;&quot;,</div><div class="line">  &quot;ownerName&quot;: &quot;&quot;,</div><div class="line">  &quot;description&quot;: &quot;&quot;,</div><div class="line">  &quot;cpus&quot;: 0.1,</div><div class="line">  &quot;disk&quot;: 256,</div><div class="line">  &quot;mem&quot;: 128,</div><div class="line">  &quot;gpus&quot;: 1,</div><div class="line">  &quot;disabled&quot;: false,</div><div class="line">  &quot;softError&quot;: false,</div><div class="line">  &quot;dataProcessingJobType&quot;: false,</div><div class="line">  &quot;fetch&quot;: [],</div><div class="line">  &quot;uris&quot;: [],</div><div class="line">  &quot;environmentVariables&quot;: [],</div><div class="line">  &quot;arguments&quot;: [],</div><div class="line">  &quot;highPriority&quot;: false,</div><div class="line">  &quot;runAsUser&quot;: &quot;root&quot;,</div><div class="line">  &quot;concurrent&quot;: false,</div><div class="line">  &quot;constraints&quot;: [],</div><div class="line">  &quot;schedule&quot;: &quot;R1//P1Y&quot;,</div><div class="line">  &quot;scheduleTimeZone&quot;: &quot;&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>修改了Job设置后，在Web UI点击绿色的Run按钮执行Job。</p><ul><li>为使用GPU资源，设置<code>&quot;gpus&quot;: 1</code>，机器上共安装了两个GPU，Mesos可以按整数个的粒度分配GPU。</li><li>Chronos针对的是定时周期作业，这里只需要执行一次，所以设置了<code>&quot;retries&quot;: 0, &quot;schedule&quot;: &quot;R1//P1Y&quot;</code>，即只重复1次，间隔1年，失败后重试0次。</li><li>执行的命令是<code>&quot;command&quot;: &quot;nvidia-smi &gt; /root/nvidia-smi-out.txt ; whoami; id; pwd&quot;</code>。</li><li>注意到<code>&quot;runAsUser&quot;: &quot;root&quot;</code>，即用<strong>主机上的root用户账号来运行这个Job</strong>，将<code>nvidia-smi</code>的输出重定向到<code>/root/nvidia-smi-out.txt</code>，Job执行成功后，可以在Host查看这个文件，确认Chronos对GPU的支持运行正常。</li><li>通过执行<code>whoami; id</code>也可以<strong>确认是root账号</strong>。</li><li>但<code>pwd</code>输出的则是Mesos Slave创建的沙盒的完整路径，看来是没有<code>chroot</code>。</li></ul><p>要查看任务输出到终端的内容，</p><ul><li>需要在Mesos Web UI的Frameworks列表点击Chronos的ID，</li><li>然后在Completed Tasks中选择任务ID对应的Sandbox链接，</li><li>再打开<code>stdout</code>或<code>stderr</code>的链接。</li></ul><blockquote><p>Windows 上用 Chrome v63 打开 Edit Job 的界面，编辑光标总是错位，但在MacOS的Chrome则正常。。。<br>所以先用 VS Code 编辑好再粘贴过去吧。</p></blockquote><h1 id="对Slave，Framwork的验证"><a href="#对Slave，Framwork的验证" class="headerlink" title="对Slave，Framwork的验证"></a>对Slave，Framwork的验证</h1><h2 id="Mesos-Master的设置"><a href="#Mesos-Master的设置" class="headerlink" title="Mesos Master的设置"></a>Mesos Master的设置</h2><blockquote><p>参考：<br><a href="http://mesos.readthedocs.io/en/latest/authentication/" target="_blank" rel="external">http://mesos.readthedocs.io/en/latest/authentication/</a><br><a href="http://mesos.readthedocs.io/en/latest/authorization/" target="_blank" rel="external">http://mesos.readthedocs.io/en/latest/authorization/</a> </p></blockquote><p>注意到Chronos的Job是以root账号执行的，可以直接执行Host的命令，操作Host上的文件路径。</p><p>下面设置开启对Slave和Framwork的验证，并让Framwork使用低权限的账号，禁止使用root账号。<br>在<code>/etc/default/mesos-master</code>中增加下面的环境变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">authenticate=true</div><div class="line">authenticate_slaves=true</div><div class="line">credentials=/etc/mesos/master-credentials.json</div><div class="line">acls=/etc/mesos/master-acls.json</div></pre></td></tr></table></figure></p><p><code>/etc/mesos/master-credentials.json</code>是允许接入Mesos集群的账号和密码（这个与Host的账号系统无关），内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123;       </div><div class="line">  &quot;credentials&quot;:[</div><div class="line">    &#123;</div><div class="line">      &quot;principal&quot;:&quot;MesosPrincipal&quot;,</div><div class="line">      &quot;secret&quot;:&quot;f0e4c2f76c58916ec258f246851bea091d14d4247a2fc3e186&quot;</div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>文件中，</p><ul><li><code>&quot;principal&quot;:&quot;MesosPrincipal&quot;</code>是用户名，</li><li><code>&quot;secret&quot;:&quot;....&quot;</code>是<strong>明文的密码</strong>，是用<code>sha256sum</code>或<code>openssl rand -hex 32</code>计算的一个字符串的哈希值。当然，随便设置的密码都可以，但其中不能有英文的冒号“:”。</li></ul><p>Slave和Framwork都可以用这个用户名密码。<br>可以在Master设置为不同的Slave和Framwork使用不同的用户名密码，这里为了简便，只使用这一组。</p><p><code>/etc/mesos/master-acls.json</code>是访问控制列表，与Host的账号及上面设置的Principal都相关，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&#123;       </div><div class="line">  &quot;run_tasks&quot;:[</div><div class="line">    &#123;</div><div class="line">      &quot;principals&quot;:&#123;&quot;values&quot;:[&quot;MesosPrincipal&quot;]&#125;,</div><div class="line">      &quot;users&quot;:&#123;&quot;values&quot;:[&quot;mesos&quot;]&#125;</div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      &quot;principals&quot;:&#123;&quot;type&quot;:&quot;NONE&quot;&#125;,</div><div class="line">      &quot;users&quot;:&#123;&quot;values&quot;:[&quot;root&quot;]&#125;</div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>这个文件将Host的root账号映射到NONE，从而禁止以root账号执行命令；并将MesosPrincipal映射到Host的mesos账号，我们需要在主机上创建该账号：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">groupadd -g 1000 mesos</div><div class="line"></div><div class="line"># 对允许mesos账号ssh登录，并且可以使用scp，sftp的机器。添加用户后还需要passwd mesos设置密码</div><div class="line">useradd -d /home/mesos -s /bin/bash  -u 1000 -g 1000 mesos</div><div class="line"></div><div class="line"># 对禁止mesos账号ssh登录的机器。没有设置密码。</div><div class="line">useradd -d /dev/null -s /sbin/nologin -u 1000 -g 1000 mesos</div></pre></td></tr></table></figure></p><p>因为要在集群的<strong>多个机器上使用相同账号</strong>，并且还要在<strong>容器中创建相同的账号</strong>，需要保证mesos账号的UID和GID在各机器上是相同的。</p><blockquote><p>集群的账号管理，应该考虑使用LDAP了。。。</p></blockquote><p>重启Mesos Master服务：<code>systemctl restart mesos-master</code>。因为没有设置Slave登录的用户名密码，所以这时Slave离线了。</p><h2 id="Mesos-Slave的设置-1"><a href="#Mesos-Slave的设置-1" class="headerlink" title="Mesos Slave的设置"></a>Mesos Slave的设置</h2><p>在<code>/etc/default/mesos-slave</code>文件添加下面的内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">credential=/etc/mesos/slave-credential-pair</div></pre></td></tr></table></figure></p><p>其中<code>/etc/mesos/slave-credential-pair</code>是登录到Master的用户名密码，这里只使用了MesosPrincipal这一个账号。</p><blockquote><p>注意：按照官方文档的说法，这个账号文件不能以空行结尾，即不能使用VIM等编辑器编辑。<br>而且格式与Master的不同，因为Master可以保存多组用户名密码，但Salve只需要提供一组就可以了。<br>这里使用<code>echo</code>写入值。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo -n &quot;MesosPrincipal f0e4c2f76c58916ec258f246851bea091d14d4247a2fc3e186&quot; &gt; /etc/mesos/slave-credential-pair</div></pre></td></tr></table></figure><p>重启Mesos Slave服务：<code>systemctl restart mesos-slave</code>，这时在Mesos Web UI应该就可以看到恢复上线的Slave了。</p><h2 id="Marathon的设置-1"><a href="#Marathon的设置-1" class="headerlink" title="Marathon的设置"></a>Marathon的设置</h2><blockquote><p>参考：<a href="https://mesosphere.github.io/marathon/docs/framework-authentication.html" target="_blank" rel="external">https://mesosphere.github.io/marathon/docs/framework-authentication.html</a></p></blockquote><p>在Marathon的Systemd服务配置文件的<code>ExecStart</code>处增加参数，完整的启动命令是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ExecStart=/usr/share/marathon/bin/marathon          \</div><div class="line">    --master n5:5050 --zk zk://n5:2181/marathon     \</div><div class="line">    --enable_features gpu_resources                 \</div><div class="line">    --mesos_authentication                          \</div><div class="line">    --mesos_authentication_principal MesosPrincipal \</div><div class="line">    --mesos_authentication_secret_file /etc/mesos/credential-secret-only</div></pre></td></tr></table></figure></p><p>其中文件<code>/etc/mesos/credential-secret-only</code>只包含密码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo -n f0e4c2f76c58916ec258f246851bea091d14d4247a2fc3e186 &gt; /etc/mesos/credential-secret-only</div></pre></td></tr></table></figure></p><p>重启服务<code>systemctl daemon-reload; systemctl restart marathon</code>，之后可以正常打开Marathon Web UI [<a href="http://10.1.1.5:8080/" target="_blank" rel="external">http://10.1.1.5:8080/</a>] 。<br>还可以在Mesos Web UI的Framworks中看到使用的Principal是MesosPrincipal。</p><h2 id="Chronos的设置-1"><a href="#Chronos的设置-1" class="headerlink" title="Chronos的设置"></a>Chronos的设置</h2><blockquote><p>参考：<a href="https://mesos.github.io/chronos/docs/configuration.html" target="_blank" rel="external">https://mesos.github.io/chronos/docs/configuration.html</a></p></blockquote><p>从命令行启动支持GPU的Chronos：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">nohup java -jar /root/chronos/target/chronos-3.0.3-SNAPSHOT.jar         \</div><div class="line">  --master=zk://n5:2181/mesos --http_port=4400                          \</div><div class="line">  --enable_features=gpu_resources                                       \</div><div class="line">  --mesos_authentication_principal=MesosPrincipal                       \</div><div class="line">  --mesos_authentication_secret_file=/etc/mesos/credential-secret-only  \</div><div class="line">  --http_credentials=&quot;ChronosUser:SomePassword&quot; &gt;/dev/null              &amp;</div></pre></td></tr></table></figure></p><p>其中使用的<code>/etc/mesos/credential-secret-only</code>跟Marathon的是同一个文件。<br>还设置了登录Chronos Web UI的用户名密码<code>ChronosUser:SomePassword</code>。这个只是针对Chronos的，而且是HTTP Basic验证，聊胜于无吧。</p><p>之后再以root账号执行Chronos的作业，将会一直显示<code>Queued</code>。<br>将用户改为<code>&quot;runAsUser&quot;: &quot;mesos&quot;</code>才会正常执行，而且需要root权限的命令会在<code>stderr</code>输出权限错误的信息。</p><h1 id="扩展到集群"><a href="#扩展到集群" class="headerlink" title="扩展到集群"></a>扩展到集群</h1><p>集群中可以设置多个Zookeeper节点，多个Master节点，以达到高可用（需是单数个节点）。<br>需要在每个机器上创建mesos这个账号，并保证UID，GID与其它机器相同，并将Master或Slave相关的配置文件等拷贝到对应的机器上。<br>Marathon和Chronos框架只需在某一台机器上启动即可，也可以启动多个实例，设置高可用模式。</p><h1 id="文件服务器设置"><a href="#文件服务器设置" class="headerlink" title="文件服务器设置"></a>文件服务器设置</h1><p>为了方便其它用户使用mesos账号拷贝文件到集群，需要搭建一个文件服务器。<br>集群已经把所有机器的硬盘加入了<a href="https://wiki.centos.org/SpecialInterestGroup/Storage/gluster-Quickstart" target="_blank" rel="external">Gluster分布式文件系统</a>，路径是<code>/gluster/volume2</code>，在其中新建文件夹<code>/gluster/volume2/data</code>作为用户上传文件的工作目录。<br>所有机器上都能同步看到这个相同的文件路径。</p><blockquote><p>GlusterFS有Linux系统的Native Client，可以<a href="https://docs.gluster.org/en/latest/Administrator%20Guide/SSL/" target="_blank" rel="external">使用SSL/TLS安全验证</a> 。由于不支持MacOS和Windows，且设置不方便，故不使用这种方式。</p></blockquote><h2 id="SFTP-【X】"><a href="#SFTP-【X】" class="headerlink" title="SFTP 【X】"></a>SFTP 【X】</h2><p>本来想允许某台机器的mesos账号ssh登录，这样也就默认开启了scp和sftp，可以使用Filezilla等FTP工具传文件。<br>但是，又想限制SFTP只能读写自己的<code>$HOME</code>，比如把mesos的<code>$HOME</code>设置为<code>/gluster/volume2/data</code>，只能读写这个文件夹。<br>OpenSSH确实可以<a href="https://bensmann.no/restrict-sftp-users-to-home-folder" target="_blank" rel="external">通过<code>chroot</code>限制某个组的用户 <strong>只能读</strong> 它的$HOME（或某个特定的文件夹）</a> 。<br>又来了但是，<code>chroot</code>后，对这个用户而言，整个文件系统就只有<code>$HOME</code>下的那些文件了，没有<code>/bin</code>、<code>/usr</code>等，也就无法执行<code>ssh</code>或者<code>scp</code>；而<code>sftp</code>还可以用，是因为设置中改用了OpenSSH内置的sftp功能，而不是外部的sftp程序（ <code>/usr/libexec/openssh/sftp-server</code>）；<br>而且必须把<code>$HOME</code>的owner设置为<code>root</code>，权限只能是<code>755</code>或<code>750</code>，就是说即便把owner group改为mesos，它也只能对自己的<code>$HOME</code>有读权限，但 <strong>不能写</strong>；</p><p>虽然可以在<code>$HOME</code>新建子目录用于mesos读写，但感觉还是不爽，试试别的方案。</p><h2 id="NFS-on-Gluster-【X】"><a href="#NFS-on-Gluster-【X】" class="headerlink" title="NFS on Gluster 【X】"></a>NFS on Gluster 【X】</h2><p>NFS是 *nix 系统上的文件共享服务。Linux，Windows和MacOS也都支持挂载NFS。问题是<a href="http://joshuawise.com/kerberos-nfs" target="_blank" rel="external">需要额外的组件（Kerberos）才能支持用户登录验证</a>，否则是公开访问的！</p><p>Gluster有专门的NFS组件<code>nfs-ganesha</code>，比使用系统默认的NFS组件好一点。不使用Kerberos的<code>nfs-ganesha</code>设置过程如下：</p><blockquote><p>参考：</p><ul><li><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Storage/2.1/html/Administration_Guide/gluster-nfs_and_kernel-nfs_services.html" target="_blank" rel="external">Manually Configuring nfs-ganesha Exports - Red Hat document</a></li><li><a href="http://docs.gluster.org/en/latest/Administrator%20Guide/NFS-Ganesha%20GlusterFS%20Integration/" target="_blank" rel="external">Configuring NFS-Ganesha over GlusterFS</a></li></ul></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install -y glusterfs-ganesha nfs-ganesha nfs-ganesha-gluster</div><div class="line">/usr/libexec/ganesha/ganesha.nfsd -N NIV_MAJ  # 修改Log级别，减少日志量</div></pre></td></tr></table></figure><p>修改配置文件<code>/etc/ganesha/ganesha.conf</code>，其中：</p><ul><li>Gluster卷的路径是<code>/gluster/volume2</code>，共享的是其子目录<code>/gluster/volume2/data</code>；</li><li>映射到的 NFS 路径是<code>10.1.1.5:/data</code>，所有客户端都映射到 mesos 账号的UID 1000和GID 1000；</li><li>NFS 的版本是v3，没有用v4。</li></ul><p>完整内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">EXPORT&#123;</div><div class="line">      Export_Id = 1;</div><div class="line">      Path = &quot;/data&quot;;</div><div class="line">      FSAL &#123;</div><div class="line">           name = GLUSTER;</div><div class="line">           hostname=&quot;localhost&quot;;</div><div class="line">           volume=&quot;volume2&quot;;</div><div class="line">           volpath=&quot;/data&quot;;</div><div class="line">           &#125;</div><div class="line">      Access_type = RW;</div><div class="line">      Disable_ACL = true;</div><div class="line">      Squash=&quot;All_Anonymous&quot;;</div><div class="line">      Pseudo=&quot;/data&quot;;</div><div class="line">      Protocols = &quot;3&quot;;</div><div class="line">      Transports = &quot;UDP&quot;,&quot;TCP&quot;;</div><div class="line">      SecType = &quot;sys&quot;;</div><div class="line">      </div><div class="line">      anonymous_uid = 1000;</div><div class="line">      anonymous_gid = 1000;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">NFS_Core_Param &#123;</div><div class="line">    NSM_Use_Caller_Name = true;</div><div class="line">    Clustered = false;</div><div class="line">    Rquota_Port = 875;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>继续执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">gluster volume set volume2 features.cache-invalidation on</div><div class="line">systemctl restart nfs-ganesha</div></pre></td></tr></table></figure></p><p>执行<code>showmount -e localhost</code>查看是否正常，输出中应包含 <code>/data (everyone)</code></p><p>在客户端挂载NFS到<code>/mnt</code> ： <code>sudo mount -t nfs 10.1.1.5:/data /mnt</code>，卸载<code>sudo umount -f /mnt</code>。</p><p>NFSv4的话，如果客户端与服务器的域名不一致，会把用户映射为<code>nobody</code>和<code>nogroup</code>；<br>NFSv3则会根据服务器的UID和GID，在客户端找有没有对应的账号：比如服务端用的mesos账号，UID是1000，碰巧客户端存在UID 1000的账号sosem，那就会显示sosem，没有找到对应的就直接显示UID或GID。<br>如果服务端的<strong>匿名用户对共享文件有读写权限</strong>，客户端总是可以在本机执行<code>sudo</code>命令随意读写远程的NFS文件，因为客户端的操作最终都是以匿名用户映射的账号在服务器执行的。<br>而为了能上传文件，就得开放写权限。<br>总之，NFS本身的安全机制实在是鸡肋，另选其它方案吧。</p><h2 id="Samba-SMB-CIFS文件共享"><a href="#Samba-SMB-CIFS文件共享" class="headerlink" title="Samba SMB/CIFS文件共享"></a>Samba SMB/CIFS文件共享</h2><p>Samba 文件共享非常方便（参考<a href="/misc/2016/remote/">局域网的远程操作</a>对应小节），</p><ul><li>Samba服务的配置很方便，自带用户验证，可以方便地设置共享目录、权限、用户（独立的密码）；</li><li>映射成网络驱动器就跟读写本地硬盘体验一样；</li><li>Linux、MacOS、Windows都自带了客户端，可以直接挂载，Android手机也有ES文件浏览器App支持挂载。</li></ul><p>但是，2017年5月份的勒索病毒WanaCrypt会通过445这个文件共享的默认端口攻击Windows机器，于是 <strong>网络管理员禁止了445端口</strong>。<br>好在可以修改Samba服务的默认端口，绕过封锁。Linux、MacOS和Android的ES文件浏览器都支持修改端口号。<br>但是，除了Windows只能使用445端口，无法修改。</p><p>设置步骤在<a href="/misc/2016/remote/">局域网的远程操作</a>对应小节介绍过，这里再简单重复一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install -y samba</div></pre></td></tr></table></figure></p><p>修改 <code>/etc/samba/smb.conf</code>，完整的内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># Run &apos;testparm&apos; to verify the config is correct after you modified it.</div><div class="line">[global]</div><div class="line">  workgroup = SAMBA</div><div class="line">  security = user</div><div class="line">  passdb backend = tdbsam</div><div class="line">  printing = cups</div><div class="line">  printcap name = cups</div><div class="line">  load printers = no</div><div class="line">  cups options = raw</div><div class="line">  smb ports = 4455</div><div class="line"></div><div class="line">[data]</div><div class="line">  comment = mesos work path</div><div class="line">  path = /gluster/volume2/data</div><div class="line">  writeable = yes</div><div class="line">  create mask = 0664</div><div class="line">  directory mask = 0775</div><div class="line">; browseable = yes # 在Windows资源管理器不可见，只能通过输入完整的地址来访问</div><div class="line">  valid users = mesos</div></pre></td></tr></table></figure></p><p>然后设置mesos在Samba系统的密码：<code>smbpasswd -a mesos</code>。<br>之后启动smbd服务：<code>systemctl enable smb; systemctl start smb</code>。</p><p>Linux客户端需要执行<code>yum install -y samba-client cifs-utils</code> 安装必要的软件。<br>以挂载到<code>/mnt</code>为例，命令是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo mount -t cifs //10.1.1.5/data /mnt -o user=mesos,pass=MesosSambaPassword,port=4455,rw,iocharset=utf8</div></pre></td></tr></table></figure></p><p>或写入<code>/etc/fstab</code>末尾，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">//10.1.1.5/data /mnt cifs user=mesos,pass=MesosSambaPassword,port=4455,rw,iocharset=utf8 0 0</div></pre></td></tr></table></figure></p><p>这样系统启动后就会自动挂载了，或执行<code>sudo mount -a</code>手动挂载<code>/etc/fstab</code> 。</p><p>可以在Linux桌面的文件管理器<strong>地址栏</strong>，或MacOS Finder的 <strong>连接服务器 Command+K</strong> 对话框输入<code>smb://10.1.1.5:4455/data</code>来访问Samba共享文件夹（会弹出账号密码窗口）。<br>Windows的地址格式是<code>\\10.1.1.5\data</code>，但由于不支持非445的端口号，所以无法访问。。。</p><h2 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a>FTP</h2><p>Windows和Linux客户端可以通过普通的FTP来上传文件，文件管理器可以直接打开FTP地址，也可以安装Filezilla。<br>MacOS不能直接在Finder（访达）打开FTP站点，需要安装Filezilla。</p><p>服务端，在CentOS服务器执行<code>yum install -y vsftpd</code>，配置文件是<code>/etc/vsftpd/vsftpd.conf</code>，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">anonymous_enable=NO</div><div class="line">local_enable=YES</div><div class="line">write_enable=YES</div><div class="line">local_umask=022</div><div class="line">dirmessage_enable=YES</div><div class="line">xferlog_enable=YES</div><div class="line">connect_from_port_20=YES</div><div class="line">xferlog_std_format=YES</div><div class="line">chroot_local_user=YES</div><div class="line">chroot_list_enable=YES</div><div class="line">chroot_list_file=/etc/vsftpd/chroot_list</div><div class="line">allow_writeable_chroot=YES</div><div class="line">listen=NO</div><div class="line">listen_ipv6=YES</div><div class="line">pam_service_name=vsftpd</div><div class="line">userlist_enable=YES</div><div class="line">tcp_wrappers=YES</div></pre></td></tr></table></figure></p><p>其中限制用户只能读写自己的<code>$HOME</code>。执行<code>touch /etc/vsftpd/chroot_list</code>建一个空的占位文件。<br>需要设置Host的mesos账号：</p><ul><li><code>$HOME</code>设置为<code>/gluster/volume2/data</code>，并设置为owner，有读写权限;</li><li>还要设置<code>mesos</code>账号的密码 <code>passwd mesos</code>。</li></ul><p>FTP是不加密的，使用主机上的账号和密码登录，可以写在地址里，即<br><code>ftp://mesos:MesosHostPassword@10.1.1.5</code>，<br>或<br><code>ftp://mesos@10.1.1.5</code>，在弹出的窗口输入密码。</p><h1 id="Chronos在容器中执行GPU作业"><a href="#Chronos在容器中执行GPU作业" class="headerlink" title="Chronos在容器中执行GPU作业"></a>Chronos在容器中执行GPU作业</h1><p>前面使用Chronos提交了基于Host命令的作业，更好的办法是把运行环境打包成容器，以便于部署和管理。<br>我们创建了一个<a href="https://github.com/icsnju/dlkit/blob/master/Dockerfile" target="_blank" rel="external"><code>icsnju/dlkit</code>的Docker镜像</a>，然而由于网络原因，还有镜像本身太大了，在Docker Hub上没有构建成功，后来是在VPS上构建的，再Push到我们用<a href="https://github.com/vmware/harbor" target="_blank" rel="external">Harbor</a>搭建的本地镜像仓库。</p><p>为了配合Mesos使用低权限的mesos账号执行任务，需要在Docker镜像中也添加同名，同UID和GID的mesos用户，将其设置为可免输密码执行<code>sudo</code>命令，并将默认用户切换为mesos。<br>Dockerfile内容如下，构建出的镜像tag设为<code>local/dlkit:mesos</code>，下面会用到这个镜像。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">FROM local/dlkit:latest</div><div class="line">RUN  echo &apos;deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse&apos; &gt; /etc/apt/sources.list ; \</div><div class="line">     rm /etc/apt/sources.list.d/*      ; \</div><div class="line">     apt-get update                    ; \</div><div class="line">     apt-get install -y sudo           ; \</div><div class="line">     apt-get clean; apt-get autoremove ; \</div><div class="line">     rm -rf /var/lib/apt/lists/*       ; \</div><div class="line">     groupadd -g 1000 mesos            ; \</div><div class="line">     useradd  -m -u 1000 -g 1000 mesos ; \</div><div class="line">     echo &quot;mesos ALL=(ALL) NOPASSWD: ALL&quot; &gt; /etc/sudoers.d/mesos</div><div class="line"></div><div class="line">USER mesos</div></pre></td></tr></table></figure></p><p>下面使用<a href="https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py" target="_blank" rel="external">Keras的MNIST CNN示例</a><br>Chronos提交GPU的容器作业配置文件内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;name&quot;: &quot;mnist-cnn-demo&quot;,</div><div class="line">  &quot;command&quot;: &quot;cd /data/mnist ; env; python mnist_cnn.py | tee out-`date +%Y%m%dT%H%M%S`.txt&quot;,</div><div class="line">  &quot;shell&quot;: true,</div><div class="line">  &quot;executor&quot;: &quot;&quot;,</div><div class="line">  &quot;executorFlags&quot;: &quot;&quot;,</div><div class="line">  &quot;taskInfoData&quot;: &quot;&quot;,</div><div class="line">  &quot;retries&quot;: 0,</div><div class="line">  &quot;owner&quot;: &quot;&quot;,</div><div class="line">  &quot;ownerName&quot;: &quot;&quot;,</div><div class="line">  &quot;description&quot;: &quot;&quot;,</div><div class="line">  &quot;cpus&quot;: 10,</div><div class="line">  &quot;disk&quot;: 1000,</div><div class="line">  &quot;mem&quot;: 10240,</div><div class="line">  &quot;gpus&quot;: 1,</div><div class="line">  &quot;disabled&quot;: false,</div><div class="line">  &quot;softError&quot;: false,</div><div class="line">  &quot;dataProcessingJobType&quot;: false,</div><div class="line">  &quot;fetch&quot;: [],</div><div class="line">  &quot;uris&quot;: [],</div><div class="line">  &quot;environmentVariables&quot;: [],</div><div class="line">  &quot;arguments&quot;: [],</div><div class="line">  &quot;highPriority&quot;: false,</div><div class="line">  &quot;runAsUser&quot;: &quot;mesos&quot;,</div><div class="line">  &quot;concurrent&quot;: false,</div><div class="line">  &quot;container&quot;: &#123;</div><div class="line">    &quot;type&quot;: &quot;MESOS&quot;,</div><div class="line">    &quot;image&quot;: &quot;local/dlkit:mesos&quot;,</div><div class="line">    &quot;network&quot;: &quot;BRIDGE&quot;,</div><div class="line">    &quot;networkInfos&quot;: [],</div><div class="line">    &quot;volumes&quot;: [</div><div class="line">      &#123;</div><div class="line">        &quot;hostPath&quot;: &quot;/gluster/volume2/data&quot;,</div><div class="line">        &quot;containerPath&quot;: &quot;/data&quot;,</div><div class="line">        &quot;mode&quot;: &quot;RW&quot;</div><div class="line">      &#125;</div><div class="line">    ],</div><div class="line">    &quot;forcePullImage&quot;: false,</div><div class="line">    &quot;parameters&quot;: []</div><div class="line">  &#125;,</div><div class="line">  &quot;constraints&quot;: [],</div><div class="line">  &quot;schedule&quot;: &quot;R1//P1Y&quot;,</div><div class="line">  &quot;scheduleTimeZone&quot;: &quot;&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>其中，</p><ul><li><code>&quot;command&quot;: &quot;cd /data/mnist ; env; python mnist_cnn.py | tee out-`date +%Y%m%dT%H%M%S`.txt&quot;</code> 为便于排错，用<code>env; pwd</code>来输出环境信息，实际计算结果则重定向到<code>result`date +%Y%m%dT%H%M%S`.txt&quot;</code>文件中；</li><li><code>&quot;cpus&quot;: 10,  &quot;disk&quot;: 1000,  &quot;mem&quot;: 10240,  &quot;gpus&quot;: 1</code>，这是为容器分配的资源，如果机器上有2个GPU，那么最多可以申请2个，申请更多的话，Job会一直排队等待资源Offer，无法执行。CPU和内存资源也要多申请一些，防止OOM；</li><li><code>&quot;runAsUser&quot;: &quot;mesos&quot;</code>，以容器内的mesos账号执行命令；</li><li><code>container</code>一节设置了使用的镜像，类型必须写成<code>MESOS</code>，而不能是<code>DOCKER</code>；</li><li>还要在<code>volumes</code>中设置Host上传文件的路径到容器路径的映射，路径名中不能有<code>-</code>，否则无法挂载，导致容器无法启动。</li></ul><p>提交后就可以在容器中执行GPU作业了。</p><blockquote><p>在容器中执行任务已经支持了运行时的隔离，但文件服务使用的是同一个目录，没有隔离。<br>其实只要额外开发一个提交任务的页面，不让用户设置路径名，然后为每个用户设置不同的文件路径，就可以支持多用户的隔离了。</p><p>PS，Mesos是Docker之前开发的，之后也没有充分利用Docker的隔离能力，设置上有点麻烦。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在CentOS 7上安装Mesos集群，设置对Slave和框架的认证，改为普通用户执行任务。Chronos在容器中执行GPU作业。&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>CentOS 7 安装TensorFlow GPU深度学习环境</title>
    <link href="https://ying-zhang.github.io/cloud/2017/setup-tensorflow-gpu-centos7/"/>
    <id>https://ying-zhang.github.io/cloud/2017/setup-tensorflow-gpu-centos7/</id>
    <published>2017-12-03T16:00:00.000Z</published>
    <updated>2018-01-06T05:39:46.161Z</updated>
    
    <content type="html"><![CDATA[<p>在CentOS 7上安装Nvidia GTX 1080 Ti显卡的驱动，以及TensorFlow GPU等深度学习开发环境。<br>就这么突然，跟深度学习扯上边了；-)</p><hr><a id="more"></a><p>上周老板突然说要给机房的Dell服务器分别装两个显卡，让我去看一下，然后把支持GPU的深度学习开发环境搭起来。装显卡是供应商的一个小哥动手的，基本顺利，遇到的小问题是电源供电不足，需要改一下iDrac中的电源设置，将服务器的两路电源互为备用模式改为两路同时供电，这样功率才够跑两个显卡。</p><p>网上一搜，就有不少CentOS上搭环境的文章了，但 1）相关开源项目发展太快，2）不同需求的用户可以有针对性的简化配置过程，所以我把集群上实测过的步骤记录下来。因为自己完全是门外汉，所以还没有涉及具体的深度学习知识。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>网上相关文章的步骤大多是先安装驱动，再安装CUDA，还需要安装C++编译器（g++或msvc），再安装cuDNN库，最后通过<code>pip</code>或<code>conda</code>再安装tensorflow。<a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="external">TensorFlow官网上的安装说明</a>以及 <a href="https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/tensorflow/" target="_blank" rel="external">Nvidia官网上的安装说明</a>亦是如此。</p><p>CUDA（Compute Unified Device Architecture，统一计算架构）是针对GPU计算加速的开发工具包，就像Windows SDK，或者JDK一样，一些深度学习库（比如TensorFlow）的底层是C++调用的CUDA库，它们提供给深度学习开发者的多是 Python 包装过的接口。一般的开发者直接用这些Python库就可以设计出多种多样的深度学习模型，不再需要跟CUDA打交道。</p><p>如果<strong>不需要从源码编译TensorFlow</strong>，就没必要安装NVIDIA官网上的那个一个多GB的CUDA包和cuDNN库。直接通过<del><code>pip</code>或</del><code>conda</code>安装的<code>tensorflow-gpu</code>库就自带了对应版本的cuda动态链接库，包括 <strong>libnvrtc-builtins.so，libnvrtc.so，libnvToolsExt.so，libnvvm.so，libcudart.so，libcublas.so，libcudnn.so，libcurand.so，libcufft.so，libcusolver.so，libcusparse.so</strong> 等，还有<strong>Intel MKL库</strong>（Linux的是<code>.so</code>文件，Windows的是<code>.dll</code>文件）。</p><p>最近（2017-12-4）Nvidia官网上的CUDA版本已经是9.0，而TensorFlow 1.4 使用的是cuda 8.0，cuDNN则是6.0，python又有2.7、3.5、3.6版。各种版本组合起来还有点麻烦呢。我们先从显卡驱动开始。</p><h1 id="安装显卡驱动"><a href="#安装显卡驱动" class="headerlink" title="安装显卡驱动"></a>安装显卡驱动</h1><p>先看看显卡硬件是不是安装好了，执行<code>lspci | grep NVIDIA</code>，可见已经安装了两个GeForce GTX 1080 Ti显卡：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@n170 ~]# lspci | grep NVIDIA</div><div class="line">03:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)</div><div class="line">03:00.1 Audio device: NVIDIA Corporation GP102 HDMI Audio Controller (rev a1)</div><div class="line">82:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)</div><div class="line">82:00.1 Audio device: NVIDIA Corporation GP102 HDMI Audio Controller (rev a1)</div></pre></td></tr></table></figure></p><p>然后安装驱动：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 参考：https://www.dedoimedo.com/computers/centos-7-nvidia-second.html</div><div class="line"># 及 https://www.youtube.com/watch?v=C9Yf71qh0i4</div><div class="line"></div><div class="line">sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</div><div class="line">sudo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm</div><div class="line">sudo yum install   nvidia-detect  # 这个命令的输入就是 kmod-nvidia，所以不安装也可以。。。</div><div class="line">sudo yum install $(nvida-detect)</div><div class="line">sudo yum install   kmod-nvidia</div><div class="line">sudo reboot # 重启是必须的</div></pre></td></tr></table></figure></p><blockquote><p>Ubuntu的命令是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 参考：https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/tensorflow/</div><div class="line"></div><div class="line">sudo add-apt-repository ppa:graphics-drivers/ppa </div><div class="line">sudo apt update</div><div class="line">sudo apt install nvidia- # 敲到 nvidia- 后按一下Tab键，稍等一会，会列出补全项，显示目前最新的是387，</div><div class="line"># 就是说完整的命令是</div><div class="line"></div><div class="line">sudo apt install nvidia-387</div><div class="line"></div><div class="line"># 注意，不要选择 378 版，否则会造成无限重试登录</div><div class="line"># 安装后也要重启系统</div></pre></td></tr></table></figure></p></blockquote><p>重启后查看驱动是否安装正确，执行<code>nvidia-smi</code>（还可以执行<code>watch -n 1 nvidia-smi</code>持续监控）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">[root@n170 ~]# nvidia-smi</div><div class="line">Mon Dec  4 16:03:57 2017       </div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 384.98                 Driver Version: 384.98                    |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |</div><div class="line">|  0%   29C    P8     8W / 250W |      0MiB / 11172MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   1  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |</div><div class="line">|  0%   30C    P8     9W / 250W |      0MiB / 11172MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">                                                                               </div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID   Type   Process name                             Usage      |</div><div class="line">|=============================================================================|</div><div class="line">|  No running processes found                                                 |</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure></p><p>还可以执行<code>cat /proc/driver/nvidia/version</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@n170 ~]# cat /proc/driver/nvidia/version </div><div class="line">NVRM version: NVIDIA UNIX x86_64 Kernel Module  384.98  Thu Oct 26 15:16:01 PDT 2017</div><div class="line">GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC)</div></pre></td></tr></table></figure></p><p><code>gpustat</code>是一个输出格式比较简单的工具，通过<code>pip install gpustat</code>安装后，输出格式如下（其中n170是机器名）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@n170 ~]# gpustat</div><div class="line">n170  Mon Dec  4 16:07:10 2017</div><div class="line">[0] GeForce GTX 1080 Ti | 28&apos;C,   0 % |     0 / 11172 MB |</div><div class="line">[1] GeForce GTX 1080 Ti | 31&apos;C,   0 % |     0 / 11172 MB |</div></pre></td></tr></table></figure></p><h1 id="安装-Anaconda-和-Python-3-6"><a href="#安装-Anaconda-和-Python-3-6" class="headerlink" title="安装 Anaconda 和 Python 3.6"></a>安装 Anaconda 和 Python 3.6</h1><p>这里选择的Python版本是3.6，但不是从Python官网或yum安装的，而是Anaconda集成环境内置的版本，这个集成环境还有<code>conda</code>包管理器，<code>jupyter notebook</code>和<code>numpy</code>，<code>pandas</code>等一些常用的包。</p><p>Anaconda官网下载页是[<a href="https://www.anaconda.com/download/" target="_blank" rel="external">https://www.anaconda.com/download/</a>] ，不过我们从清华的镜像站下载，这样下载速度快一点[<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.0.1-Linux-x86_64.sh" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.0.1-Linux-x86_64.sh</a>] 。虽然这个安装文件后缀是<code>.sh</code>，但实际的二进制安装文件都打包在里面了，有525MB。</p><p>安装过程需要用到<code>bzip2</code>，先安装一下<code>sudo yum install -y bzip2</code><br>执行 <code>bash Anaconda3-5.0.1-Linux-x86_64.sh</code> 开始安装，敲回车显示 license agreement ，敲几次空格翻到底，然后输入<code>yes</code>接受协议，再敲回车，安装到默认的路径<code>$HOME/anaconda3</code>，如果这个路径已经存在，就会安装失败，需要删掉或另选路径。<br>安装脚本还会在<code>.bashrc</code>的<code>PATH</code>环境变量加上安装路径。安装结束后，执行<code>source .bashrc</code>，更新<code>PATH</code>环境变量，这时系统的<code>python</code>命令已经变成Anaconda安装的Python 3.6了（因为安装程序把<code>$HOME/anaconda3/bin</code>加在了<code>PATH</code>最前面）。</p><blockquote><p>为了让其他用户也能使用Anaconda，可将其安装到系统目录，如<code>/opt/anaconda3</code>，并修改系统的<code>PATH</code><br>静默模式安装Anaconda：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cd ~</div><div class="line">curl -kO https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.0.1-Linux-x86_64.sh</div><div class="line">bash ~/Anaconda3-5.0.1-Linux-x86_64.sh -b -p   /opt/anaconda3</div><div class="line">echo &apos;export PATH=/opt/anaconda3/bin:$PATH&apos; &gt;&gt; /etc/profile</div><div class="line">source /etc/profile</div></pre></td></tr></table></figure></p><p>更改<code>conda</code>源：执行<br><code>conda config --add channels &#39;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&#39;</code><br><code>conda config --set show_channel_urls yes</code><br>这两个命令其实是把配置项写到了<code>~/.condarc</code>文件，还可以在这里设置http代理：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">channels:</div><div class="line">  - defaults</div><div class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</div><div class="line">show_channel_urls: true</div><div class="line"></div><div class="line">proxy_servers:</div><div class="line">    http:  http://127.0.0.1:1080</div><div class="line">    https: http://127.0.0.1:1080</div><div class="line"></div><div class="line">ssl_verify: False</div></pre></td></tr></table></figure></p></blockquote><h1 id="安装-TensorFlow"><a href="#安装-TensorFlow" class="headerlink" title="安装 TensorFlow"></a>安装 TensorFlow</h1><p>执行 <code>conda install tensorflow-gpu</code>，注意，安装的版本是 <code>1.3.0-py36cuda8.0cudnn6.0_1</code> ，不是最新的<code>1.4.0</code>版，不过好处是开箱即用，就这句命令就搞定了。cuda相关的动态库都已经安装在了<code>$HOME/anaconda3/lib</code>。</p><p>执行一下官网的测试例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@n170 ~]# python</div><div class="line">Python 3.6.2 |Anaconda custom (64-bit)| (default, Sep 22 2017, 02:03:08) </div><div class="line">[GCC 7.2.0] on linux</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt; import tensorflow as tf</div><div class="line">&gt;&gt;&gt; hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</div><div class="line">&gt;&gt;&gt; sess = tf.Session()</div><div class="line">&gt;&gt;&gt; print(sess.run(hello))</div><div class="line">b&apos;Hello, TensorFlow!&apos;</div><div class="line">&gt;&gt;&gt; with tf.Session():</div><div class="line">...     a=tf.constant([1.0, 1.0, 1.0, 1.0])</div><div class="line">...     b=tf.constant(2.0, shape=[4])</div><div class="line">...     out=tf.add(a,b)</div><div class="line">...     print(&quot;result:&quot;,out.eval())</div><div class="line">... </div><div class="line">result: [ 3.  3.  3.  3.]</div></pre></td></tr></table></figure></p><h2 id="TensorFlow-CPU版"><a href="#TensorFlow-CPU版" class="headerlink" title="TensorFlow CPU版"></a>TensorFlow CPU版</h2><p>TensorFlow 使用<strong>CPU</strong>的官方版本没有使用SSE等向量化指令，执行时有Warning<br><code>W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</code> 。</p><p>禁用该Warning可添加环境变量 <code>export TF_CPP_MIN_LOG_LEVEL=2</code>。<br>当然最好的解决方案是 <strong>重新构建</strong>TensorFlow，开启SSE，AVX的编译选项。</p><blockquote><p>参考：</p><ul><li><a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="external">https://www.tensorflow.org/install/install_sources</a></li><li><a href="https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture" target="_blank" rel="external">https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture</a></li><li><a href="https://github.com/tensorflow/tensorflow/issues/8037" target="_blank" rel="external">https://github.com/tensorflow/tensorflow/issues/8037</a> </li><li><a href="https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions" target="_blank" rel="external">https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions</a></li></ul></blockquote><p>下面的构建过程是在Ubuntu 16.04的容器中完成的。涉及到机器的指令集，CPU型号是 i7-6700 CPU @ 3.40GHz。</p><h3 id="安装bazel"><a href="#安装bazel" class="headerlink" title="安装bazel"></a>安装bazel</h3><p>安装必要的软件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">apt install git wget</div><div class="line">apt install openjdk-8-jdk pkg-config zip g++ zlib1g-dev unzip python</div></pre></td></tr></table></figure></p><p>修改系统默认的python为上面安装的python3.5<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ln -s /usr/bin/python3.5 /usr/local/bin/python</div></pre></td></tr></table></figure></p><p>下载bazel的安装包，<br><a href="https://github.com/bazelbuild/bazel/releases/download/0.9.0/bazel-0.9.0-without-jdk-installer-linux-x86_64.sh" target="_blank" rel="external">https://github.com/bazelbuild/bazel/releases/download/0.9.0/bazel-0.9.0-without-jdk-installer-linux-x86_64.sh</a></p><p>然后执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bash bazel-0.9.0-without-jdk-installer-linux-x86_64.sh</div></pre></td></tr></table></figure></p><h3 id="构建TensorFlow-CPU版"><a href="#构建TensorFlow-CPU版" class="headerlink" title="构建TensorFlow CPU版"></a>构建TensorFlow CPU版</h3><p>下载1.4版的源码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone -b r1.4 --depth=1 https://github.com/tensorflow/tensorflow</div></pre></td></tr></table></figure></p><p>安装必要的python包。<br>注意，<strong>不要用pip3安装numpy</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apt install python3-numpy python3-dev python3-pip python3-wheel</div></pre></td></tr></table></figure></p><p>配置构建参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd tensorflow</div><div class="line">./configure</div></pre></td></tr></table></figure></p><p>敲回车接受默认的bazel和python3.5路径，后面的其它选项均选择 n，最后提示使用本机的CPU架构作为编译选项，因为现在的CPU都是支持SSE，AVX等指令的，所以也就会在构建选项中加入了相应的选项。</p><p>开始构建，约二十多分钟<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bazel build --incompatible_load_argument_is_label=false \</div><div class="line"> -c opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package</div></pre></td></tr></table></figure></p><p>构建完成后，将其打包为whl<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">bazel-bin/tensorflow/tools/pip_package/build_pip_package /root/</div><div class="line">ls /root</div><div class="line"></div><div class="line"># tensorflow-1.4.1-cp35-cp35m-linux_x86_64.whl</div></pre></td></tr></table></figure></p><p>保存上面打包好的whl，可以安装到其它Linux系统上。</p><hr><h1 id="在docker容器中的TensorFlow环境"><a href="#在docker容器中的TensorFlow环境" class="headerlink" title="在docker容器中的TensorFlow环境"></a>在docker容器中的TensorFlow环境</h1><p>需要安装<code>nvidia-container-runtime</code>插件，才能正确运行支持GPU的容器。参考：[<a href="https://github.com/NVIDIA/nvidia-docker]。" target="_blank" rel="external">https://github.com/NVIDIA/nvidia-docker]。</a></p><blockquote><p>注意，安装过程会 <strong>覆盖</strong> <code>/etc/docker/daemon.json</code> 配置文件！需要提前备份。</p></blockquote><p>安装步骤是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">curl -s -L https://nvidia.github.io/nvidia-docker/centos7/x86_64/nvidia-docker.repo | tee /etc/yum.repos.d/nvidia-docker.repo</div><div class="line"></div><div class="line">yum install -y nvidia-docker2</div><div class="line">pkill -SIGHUP dockerd</div></pre></td></tr></table></figure></p><p>安装后的<code>/etc/docker/daemon.json</code> 如下（阿里云的仓库镜像是后来添加的）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@n170 ~]# cat /etc/docker/daemon.json</div><div class="line">&#123;</div><div class="line">    &quot;registry-mirrors&quot;: [&quot;https://lmigye0h.mirror.aliyuncs.com&quot;],</div><div class="line">    &quot;runtimes&quot;: &#123;</div><div class="line">        &quot;nvidia&quot;: &#123;</div><div class="line">            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,</div><div class="line">            &quot;runtimeArgs&quot;: []</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>其中增加了<code>nvidia-container-runtime</code>这个运行时插件，这是<code>nvidia-docker</code> v2 的实现方式了，运行一个容器验证一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">nvidia-docker run --rm nvidia/cuda nvidia-smi</div><div class="line">nvidia-docker run --rm -e NVIDIA_VISIBLE_DEVICES=1 nvidia/cuda nvidia-smi</div></pre></td></tr></table></figure></p><p>其中环境变量<code>NVIDIA_VISIBLE_DEVICES</code>是指定GPU设备的可见性，可以是 0,1,… 这样逗号分隔的一个或多个GPU id，也可以是all或none。<br>参考：<a href="https://github.com/nvidia/nvidia-container-runtime#nvidia_visible_devices" target="_blank" rel="external">https://github.com/nvidia/nvidia-container-runtime#nvidia_visible_devices</a></p><p>其实<code>nvidia-docker</code>只是一个包装脚本，实际执行的命令是<code>docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</code> 。</p><p>至于TensorFlow的容器，执行<br><code>docker run --runtime=nvidia -ti --rm -p 8000:8888 -p 6006:6006 tensorflow/tensorflow:latest-gpu</code></p><p>这个镜像有3.36GB。8888端口是jupyter notebook的，6006是tensorboard的端口，因为我的这台机器的8888端口被占用了，所以映射到了8000。<br>容器启动后，会输出jupyter notebook的访问token，在浏览器输入主机的IP（假设为2.2.2.170）和映射端口号（这里是8000，不是默认的8888），即<br><a href="http://2.2.2.170:8000/?token=79f542ddc22fb567f3c2900c9310f1ce30847d5c5f927cba" target="_blank" rel="external">http://2.2.2.170:8000/?token=79f542ddc22fb567f3c2900c9310f1ce30847d5c5f927cba</a><br>就会打开jupyter notebook，里面有三个TensorFlow入门介绍的ipynb文件，这样就可以编辑运行Python代码了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[root@n170 ~]# docker run --runtime=nvidia -ti --rm -p 8000:8888 -p 6006:6006 tensorflow/tensorflow:latest-gpu</div><div class="line">[I 03:08:12.136 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret</div><div class="line">[W 03:08:12.159 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.</div><div class="line">[I 03:08:12.165 NotebookApp] Serving notebooks from local directory: /notebooks</div><div class="line">[I 03:08:12.165 NotebookApp] 0 active kernels</div><div class="line">[I 03:08:12.165 NotebookApp] The Jupyter Notebook is running at:</div><div class="line">[I 03:08:12.165 NotebookApp] http://[all ip addresses on your system]:8888/?token=79f542ddc22fb567f3c2900c9310f1ce30847d5c5f927cba</div><div class="line">[I 03:08:12.165 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</div><div class="line">[C 03:08:12.166 NotebookApp] </div><div class="line">    </div><div class="line">    Copy/paste this URL into your browser when you connect for the first time,</div><div class="line">    to login with a token:</div><div class="line">        http://localhost:8888/?token=79f542ddc22fb567f3c2900c9310f1ce30847d5c5f927cba</div><div class="line"></div><div class="line">[root@n170 ~]# # 按 Ctrl + p, q 键退出容器交互终端，容器仍在后台运行</div></pre></td></tr></table></figure><blockquote><p>Anaconda 中也有jupyter notebook，在主机执行命令<code>jupyter notebook</code>就会运行后台服务，并启动浏览器打开页面。<br>默认只能允许localhost访问，如果需要设置别的机器也可以通过主机的IP地址访问notebook，可以参考 [<a href="http://jupyter-notebook.readthedocs.io/en/stable/public_server.html" target="_blank" rel="external">http://jupyter-notebook.readthedocs.io/en/stable/public_server.html</a>] 。</p><p>notebook中用matplotlib画图，如果不想写<code>plt.show()</code>，可以在代码前加上<code>%matplotlib inline</code>指令，这样执行<code>plt.plot(...)</code>就会输出图形。</p></blockquote><h1 id="pip-和-cuda"><a href="#pip-和-cuda" class="headerlink" title="pip 和 cuda"></a>pip 和 cuda</h1><p>如果不是按上面小节的步骤使用<code>conda</code>，就要按照教程的步骤，先安装cuda了。</p><blockquote><p>pip换源，在文件<code>$HOME/.pip/pip.conf</code> 或 <code>/etc/pip.conf</code> 中添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[global]</div><div class="line">trusted-host = mirrors.tuna.tsinghua.edu.cn</div><div class="line">index-url = https://mirrors.tuna.tsinghua.edu.cn/pypi/simple</div></pre></td></tr></table></figure></p></blockquote><p>没有安装cuda，直接执行<code>pip install tensorflow-gpu</code>，取决于系统的Python版本，不论2.7，3.5或3.6版，都可以安装对应1.4.0版本，但执行上面的测试例子，就会报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">... ...</div><div class="line">ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory</div><div class="line">... ...</div></pre></td></tr></table></figure></p><p>就是说找不到cuda的动态库。</p><blockquote><p>前面也提到了，其实cuda类似JDK，但Nvidia没有把cuda的动态库打包单独提供（类似JRE）。<br><code>conda</code>自己打包了需要的动态库（cudatoolkit，cudnn），可以一键安装，但<code>pip</code>就没有这么贴心了，需要安装完整版的cuda SDK。</p></blockquote><p>需要下载的文件和具体安装步骤可见[<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external">https://developer.nvidia.com/cuda-downloads</a>] ，目前Nvidia官网提供的是cuda 9.0（不知向前兼容性如何），旧版cuda的下载链接是[<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="external">https://developer.nvidia.com/cuda-toolkit-archive</a>] ，还要注册一下，然后下载并安装cuDNN的库。</p><p>如果之前没有安装显卡驱动的话，按上面官网的介绍，以为上面的步骤会把cuda 9.0和内核驱动一起安装上，而且确实安装了名为<code>nvidia-kmod</code>的包，但重启后执行<code>nvidia-smi</code>，发现并没有安装成功，不知是什么问题。<br>所以还是要按照更前面小节的步骤从elrepo安装<code>kmod-nvidia</code>包，不过安装过程会有包冲突，需要根据提示信息卸载：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo yum erase 1:nvidia-kmod-384.81-2.el7.x86_64</div><div class="line">sudo yum erase 1:xorg-x11-drv-nvidia-384.81-1.el7.x86_64</div><div class="line">sudo yum-config-manager --disable cuda-9-0-local</div></pre></td></tr></table></figure></p><p>之后再重新执行安装<code>kmod-nvidia</code>的命令，重启后验证安装是否正确。再增加环境变量<br><code>export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:$LD_LIBRARY_PATH</code><br>TensorFlow应该就可以找到需要的动态库了。</p><h1 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h1><p>参考：[<a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software" target="_blank" rel="external">https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software</a>]</p><p>比较常见的几个框架有：</p><ul><li>Tensorflow : Google的项目，参考TensorFlow OSDI`2016的论文，设计目标是在大规模集群和异构硬件（GPU，TPU，ASIC等）上支持深度学习网络的训练和应用</li><li>MXNet：由<a href="https://www.cs.cmu.edu/~muli/" target="_blank" rel="external">CMU的李沐博士</a> ，<a href="https://homes.cs.washington.edu/~tqchen/" target="_blank" rel="external">华盛顿大学的陈天齐博士</a> 等开发的项目，他还有一篇博客介绍了<a href="http://mli.github.io/2015/12/03/mxnet-overview/" target="_blank" rel="external">MXNet设计和实现</a> 。目前是Apache的孵化项目，Amazon也在推广MXNet（李沐博士在Amazon工作）。在MXNet的基础上，他们还发布了<a href="http://mp.weixin.qq.com/s/_9aY-7aTZDOjeWFKntLnXA" target="_blank" rel="external">更灵活的前端Gluon（胶子）</a> 和<a href="https://zhuanlan.zhihu.com/p/29914989" target="_blank" rel="external">更可拓展的后端NNVM compiler</a></li><li>Cognitive Toolkit（CNTK）：这是微软的深度学习项目</li><li>Theano：蒙特利尔大学MILA实验室开发的项目，2017年11月15日发布1.0版后就不再继续开发</li><li>PyTorch：是基于Lua的Torch项目的Python版本，由Facebook开发</li><li>Caffe2，Caffe：是由<a href="http://daggerfs.com" target="_blank" rel="external">UC Berkeley的贾扬清博士</a> 开发的，他已经在Facebook工作，所以Caffe2也是Facebook的一个项目</li><li>Keras：这个项目是对一些深度学习项目的更高层抽象和统一包装，官方支持的后端有TensorFlow，CNTK和Theano，有些深度学习项目也会提供对Keras的支持。当然，有的项目，像PyTorch，本身的抽象就比较高层，与Keras相当，另外像MXNet自己也有类似的前端Gluon。</li></ul><p>从Wiki上的比较列表来看，对深度学习框架的关注点主要有：是否支持GPU加速，支持分布式集群，自动推导梯度，支持的网络类型（CNN，RNN等），是否有预先训练的模型等。<br>问了两位搞机器学习方向的同学，他们觉得TensorFlow偏底层，工程化，不如PyTorch写代码直观，Keras虽然理念很好，但性能上还差一点。他们目前还是用单机的GPU来训练模型，跑一次也要不少时间，但还没有准备搞TensorFlow那种分布式计算集群。</p><p>对DNN的了解太少了，要抓紧时间啊！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在CentOS 7上安装Nvidia GTX 1080 Ti显卡的驱动，以及TensorFlow GPU等深度学习开发环境。&lt;br&gt;就这么突然，跟深度学习扯上边了；-)&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>【译文修订】使用Borg在Google管理大规模集群</title>
    <link href="https://ying-zhang.github.io/cloud/2017/eurosys15-borg-cn/"/>
    <id>https://ying-zhang.github.io/cloud/2017/eurosys15-borg-cn/</id>
    <published>2017-10-30T16:00:00.000Z</published>
    <updated>2018-01-14T02:27:40.757Z</updated>
    
    <content type="html"><![CDATA[<p>发表于EuroSys 2015的 <strong><em>Large-scale cluster management at Google with Borg</em></strong> 详细介绍了Google的Borg资源管理器。已经有网友“难易（HardySimpson）” 翻译了此文，这里对其稍作修订。之前读过两三遍此文，每遍都感觉有新的体会，这次修订就是为了更仔细地读一遍。</p><a id="more"></a><hr><h1 id="Large-scale-cl-uster-management-at-Google-with-Borg"><a href="#Large-scale-cl-uster-management-at-Google-with-Borg" class="headerlink" title="Large-scale cl uster management at Google with Borg"></a>Large-scale cl uster management at Google with Borg</h1><h1 id="使用Borg在Google管理大规模集群"><a href="#使用Borg在Google管理大规模集群" class="headerlink" title="使用Borg在Google管理大规模集群"></a>使用Borg在Google管理大规模集群</h1><p>作者：Abhishek Vermay, Luis Pedrosaz, Madhukar Korupolu, David Oppenheimer, Eric Tune, John Wilkes</p><p>EuroSys’15, <a href="http://dx.doi.org/10.1145/2741948.2741964" target="_blank" rel="external">http://dx.doi.org/10.1145/2741948.2741964</a></p><p><a href="http://research.google.com/pubs/pub43438.html" target="_blank" rel="external">http://research.google.com/pubs/pub43438.html</a> 如果不方便翻墙，可以直接 <a href="https://ying-zhang.github.io/doc/EuroSys15_Borg.pdf">下载的英文全文PDF</a></p><p><a href="https://ying-zhang.github.io/doc/EuroSys15_Borg_CN_Ying_201711.pdf"><strong>中文译文全文PDF</strong></a></p><p>原译者：难易 <a href="https://my.oschina.net/HardySimpson/blog/515398" target="_blank" rel="external">https://my.oschina.net/HardySimpson/blog/515398</a> </p><blockquote><p><strong>修订：Ying 2017-10-31 ~ 2017-11-09</strong></p></blockquote><p>最近又看到一版译文<a href="http://geek.csdn.net/news/detail/189597" target="_blank" rel="external">深度译文｜Google的大规模集群管理系统Borg - 王勇桥</a> 。<br>对比一下摘要吧，</p><blockquote><p>难易：<br>谷歌的Borg系统群集管理器运行几十万个以上的jobs，来自几千个不同的应用，跨多个集群，每个集群有上万个机器。<br>它通过管理控制、高效的任务包装、超售、和进程级别性能隔离实现了高利用率。它支持高可用性应用程序与运行时功能，最大限度地减少故障恢复时间，减少相关故障概率的调度策略。Borg简化了用户生活，通过提供一个声明性的工作规范语言，名称服务集成，实时作业监控，和分析和模拟系统行为的工具。<br>我们将会展现Borg系统架构和特点，重要的设计决策，定量分析它的一些策略，和十年以来的运维经验和学到的东西。</p></blockquote><hr><blockquote><p>王：<br>Google的Borg系统是一个运行着成千上万项作业的集群管理器，它同时管理着很多个应用集群，每个集群都有成千上万台机器，这些集群之上运行着Google的很多不同的应用。Borg通过准入控制，高效的任务打包，超额的资源分配和进程级隔离的机器共享，来实现超高的资源利用率。它通过最小化故障恢复时间的运行时特性和减少相关运行时故障的调度策略来支持高可用的应用程序Borg通过提供一个作业声明的标准语言，命名服务的集成机制，实时的作业监控，以及一套分析和模拟系统行为的工具来简化用户的使用。</p></blockquote><hr><blockquote><p>原文：<br>Google’s Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. It achieves high utilization by combining admission control, efficient task-packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time, and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior.<br>We present a summary of the Borg system architecture and features, important design decisions, a quantitative analysis of some of its policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it.</p></blockquote><hr><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Google的Borg系统是一个集群管理器。它在多个万台机器规模的集群上运行着来自几千个不同的应用的几十万个作业。</p><p>Borg通过准入控制、高效的任务装箱、超售、机器共享、以及进程级别的性能隔离，实现了高利用率。它为高可用应用提供了可以减少故障恢复时间的运行时特性，以及降低关联故障概率的调度策略。Borg提供了声明式的作业描述语言、名字服务集成、实时作业监控、分析和模拟系统行为的工具。这些简化了用户的使用。</p><p>本文介绍了Borg系统架构和特性，重要的设计决策，对某些策略选择的定量分析，以及十年来的运营经验和教训。</p><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>我们内部称为Borg的集群管理系统，负责接收、调度、启动、重启和监控Google所有的应用。本文介绍它是如何实现的。</p><p>Borg提供了三个主要的好处：（1）隐藏资源管理和故障处理细节，使用户可以专注于应用开发；（2）高可靠性和高可用性的运维，并支持应用程序也能够如此；（3）让我们可以在几万台机器上高效地运行负载。Borg不是第一个涉及这些问题的系统，但它是少有的运行在如此大规模、具有弹性、完善的系统之一。</p><p>本文围绕这些主题来编写，总结了十多年来我们在生产环境运行Borg的一些定性观察。</p><p><img width="600" src="/img/borg-fig-01.png" alt="图1. Borg的架构。图中只画出了数千个工作节点的很小一部分"></p><h1 id="2-用户视角"><a href="#2-用户视角" class="headerlink" title="2. 用户视角"></a>2. 用户视角</h1><p>Borg的用户是Google的开发人员以及运行Google应用和服务的系统管理员（站点可靠性工程师，SRE）。用户以作业（Job）的方式将他们的工作提交给Borg。作业由一个或多个任务（Task）组成，每个任务执行相同的二进制程序。每个作业只运行在一个Borg单元（Cell）里。Cell是一组机器的管理单元。下面的小节将介绍用户视角看到的Borg系统的主要特性。</p><blockquote><p>SRE的职责比系统管理员多得多：他们是负责Google生产服务的工程师。他们也设计和实现包括自动化系统等软件，管理应用、服务基础设施和平台，以保证在Google如此大的规模下的高性能和高可靠性。</p></blockquote><h2 id="2-1-工作负载"><a href="#2-1-工作负载" class="headerlink" title="2.1 工作负载"></a>2.1 工作负载</h2><p>Borg Cell主要运行两种异构的工作负载。第一种是应该“永不”停止的长期运行的服务，处理持续时间较短但对延迟敏感的请求（从几微秒到几百毫秒）。这些服务用于面向最终用户的产品，如Gmail、Google Docs、网页搜索，以及内部基础设施服务（例如Bigtable）。第二种是批处理作业，执行时间从几秒到几天，对短期性能波动不敏感。不同Cell中这两种负载的混合程度不同，取决于其主要租户（例如，有些Cell就以批处理作业为主）。工作负载也随时间变化：批处理作业不断提交或结束，而很多面向终端用户的服务表现出昼夜周期性的使用模式。Borg需要都处理好这些情况。</p><p>Borg的代表性负载是一个公开的2011年5月整月的记录数据集[80]。这个数据集已经获得了广泛的分析[1, 26, 27, 57, 68]。</p><p>最近几年，以Borg为基础构建了很多应用框架，包括我们内部的MapReduce系统[23]、FlumeJava[18]、Millwheel[3]和Pregel[59]。这些框架大多有一个控制器来提交Master Job，还有多个Worker Job。前两个框架类似于YARN的应用管理器[76]。我们的分布式存储系统，例如GFS[34]和它的后继者CFS、Bigtable[19]、以及Megastore[8]，都是运行在Borg上的。</p><p>本文中，我们把高优先级的Borg作业称为为生产作业（prod），其它的则是非生产的（non-prod）。大多数长期服务是prod的，大部分批处理作业是non-prod的。一个典型Cell里，prod作业分配了约70%的总CPU资源，占总CPU使用量约60%；分配了约55%的总内存资源，占总内存使用量约85%。§5.5节表明分配量和使用量的差异是值得注意的。</p><h2 id="2-2-集群（Cluster）和单元（Cell）"><a href="#2-2-集群（Cluster）和单元（Cell）" class="headerlink" title="2.2 集群（Cluster）和单元（Cell）"></a>2.2 集群（Cluster）和单元（Cell）</h2><p>一个Cell里的机器属于同一个集群。集群由数据中心级的高性能光纤的组网来定义。一个集群位于数据中心的一栋建筑内，而一个数据中心有多栋建筑（注：这些关系会有少数例外情况）。一个集群通常包括一个大的Cell，还可能有一些小规模的测试用或其它特殊用途的Cell。我们尽力避免任何单点故障。</p><p>排除测试用的Cell，中等规模的Cell约有一万台机器；有些Cell还要大得多。Cell中的机器从多个维度看都是异构的：大小（CPU、内存，硬盘，网络）、处理器类型、性能、以及是否有外网IP地址或SSD等。Borg负责决定任务在Cell中的哪些机器上执行、为其分配资源、安装程序及依赖、监控健康状态并在失败后重启，从而使用户几乎不必关心机器异构性。</p><h2 id="2-3-作业（Job）和任务（Task）"><a href="#2-3-作业（Job）和任务（Task）" class="headerlink" title="2.3 作业（Job）和任务（Task）"></a>2.3 作业（Job）和任务（Task）</h2><p>一个Borg 作业的属性有：名字、拥有者和任务个数。作业可以有一些约束来强制其任务运行在有特定属性的机器上，比如处理器架构、操作系统版本、是否有外网IP地址等。约束可以是硬性的或者柔性的，柔性约束表示偏好，而非需求。一个作业可以推迟到前一个作业结束后再开始（ying：即依赖顺序）。一个作业只在一个Cell中运行。</p><p>每个任务对应着一组Linux进程，运行在一台机器上的一个容器内[62]。绝大部分Borg的工作负载没有运行在虚拟机里，因为我们不想付出虚拟化的开销。而且，在Borg设计的那个时期，我们有很多处理器还不支持硬件虚拟化呢。</p><p>任务也有一些属性，如资源需求量，在作业中的序号等。一个作业中的任务大多有相同的属性，但也可以被覆盖 —— 例如特定任务的命令行参数。各维度的资源（CPU核、内存、硬盘空间、硬盘访问速度、TCP端口（注：Borg负责管理一台机器上的可用端口并将其分配给任务）等。可以互相独立的以细粒度指定。我们不强制使用固定大小的资源桶或槽（见§5.4）。Borg运行的程序都是静态链接的，以减少对运行环境的依赖，这些程序组织成由二进制文件和数据文件构成的包，由Borg负责安装。</p><p>用户通过向Borg发送RPC来控制作业。RPC大多是从命令行工具、其它作业、或我们的监控系统（§2.6）发出的。大多作业描述文件使用一种声明式配置语言BCL。BCL是GCL[12]的一个变种，即增加了一些Borg专有的关键字，而GCL会生成若干protobuf文件[67]。GCL还提供了匿名函数以支持计算，这样就能让应用根据环境调整自己的配置。有上万个超过一千行的BCL配置文件，系统中累计运行了千万行BCL。Borg的作业配置与Aurora的配置文件[6]相似。</p><p>图2展示了作业和任务整个生命周期的状态变化。</p><p><img width="600" src="/img/borg-fig-02.png" alt="图2. 作业和任务的状态图。用户可以触发提交，杀死和更新动作"></p><p>要想在运行时改变一个作业中若干或全部任务的属性，用户可以向Borg提交一个新的作业配置，并命令Borg将任务更新到新的配置。更新是轻量级的，非原子性的事务，在事务结束（提交）之前可以很容易地撤销。更新通常是滚动执行的，而且可以限制由更新导致的任务中断（被重新调度或抢占）的数量；超过限值的变更会被跳过。</p><p>一些任务更新（如更新二进制程序）需要重启任务；另外一些更新（如增加资源需求或修改约束）可能使该任务不适合运行在当前机器上，导致停止并重新调度该任务；还有一些更新（如修改优先级）总是可以执行的，不需要重启或者移动任务。</p><p>任务可以要求被Unix的<code>SIGKILL</code>立即杀死之前获得<code>SIGTERM</code>信号通知，这样它们还有时间清理资源、保存状态、结束当前请求、拒绝新请求。但如果抢占者设置了延迟限值，就可能来不及发通知。实践中，80%的情况下能发出通知信号。</p><h2 id="2-4-分配（Allocs）"><a href="#2-4-分配（Allocs）" class="headerlink" title="2.4 分配（Allocs）"></a>2.4 分配（Allocs）</h2><p>Borg的alloc（allocation的缩写）是一台机器上的预留资源，可以用来执行一个或多个任务；不管有没有被使用，这些资源都算分配出去了。Allocs可以给将来的任务预留资源，或在任务停止和重启的间隔保持资源，以及将不同作业的多个任务绑定在同一台机器上 —— 例如一个Web服务器实例和附加的将其URL日志从本机硬盘拷贝到分布式文件系统的保存日志任务。Alloc的资源像一台机器上的那样来管理；运行在同一个Alloc内的多个任务共享其资源。如果一个Alloc需要迁移到其它机器上，那么它的任务也要跟着重新调度。</p><p>一个Alloc集，即一组在多台机器上预留了资源的Alloc，类似于一个作业。一旦创建了一个Alloc集，就可以向其提交若干作业。简便起见，我们用<strong>任务</strong>表示一个Alloc或者一个顶层任务（即运行在Alloc之外的任务），用<strong>作业</strong>表示一个普通作业或者Alloc集。</p><h2 id="2-5-优先级、配额和准入控制"><a href="#2-5-优先级、配额和准入控制" class="headerlink" title="2.5 优先级、配额和准入控制"></a>2.5 优先级、配额和准入控制</h2><p>当出现超过系统容量的工作负载会产生什么情况？我们对此的解决方案是优先级和配额。</p><p>每个作业都有一个小的正整数表示的优先级。高优先级的任务可以优先获取资源，甚至抢占（杀死）低优先级的任务。Borg为不同用途定义了不重叠的优先级区间，包括（优先级降序）：<strong>监控、生产、批处理、尽力（即测试的或免费的）</strong>。本文中，prod作业的优先级是监控和生产两个区间。</p><p>虽然一个被抢占的任务通常会被重新调度到Cell的其它机器上，但级联抢占也可能发生：如果某个任务抢占了一个优先级稍低的任务，而后者又抢占了另一个优先级稍低的，如此往复。为避免这种情况，我们禁止<code>生产</code>区间的任务互相抢占。细粒度（ying：相比于区间的粗粒度）的优先级在其它场景下也很有用 —— 如MapReduce的Master 任务的优先级比其管理的Worker高一点，以提高其可靠性。</p><p>优先级表示了Cell中运行或等待的作业之间的相对重要性。配额则用来决定准许哪个作业可以被调度。配额是指定优先级和时间段（典型是几个月）的一个资源向量（CPU，内存，硬盘等）。配额限制了用户的作业一次可以申请资源的最大数量（如：20TB内存，以prod优先级，从现在到7月末，在xx Cell内）。配额检查是准入控制的一部分，不是调度的：配额不足的作业提交时当即就会被拒绝。</p><p>高优先级的配额比低优先级的成本要高。生产级的配额限于一个Cell的实际可用资源量，因此用户提交了不超过配额的生产级作业时，不考虑资源碎片和约束，可以预期这个作业一定会运行。尽管我们鼓励用户不要购买超过其需求的配额，但很多用户仍然超买了，这样他们就不用担心将来应用的用户量增长可能导致的配额短缺。我们的应对方案是对低优先级资源配额的超售：所有用户的0优先级配额是无限的，尽管这无法实现。低优先级的作业虽然被接收了，但可能由于资源不足而一直等待。</p><p>配额分配是Borg之外的系统处理的，与我们的物理容量规划紧密相关。容量规划的结果反映在各数据中心的价格和可用配额上。只有在其要求的优先级有足够的配额，用户的作业才能被接收。采用配额使得主导资源公平性（DRF）[29, 35, 36, 66]这样的策略不是那么必要了。</p><p>Borg的容量系统可以给某些用户一些特殊权限。例如，允许管理员删除或修改Cell里的任意作业，或者允许某个用户操作特定的内核特性或Borg行为（如对其作业禁用资源估计。§5.5）。</p><h2 id="2-6-命名和监控"><a href="#2-6-命名和监控" class="headerlink" title="2.6 命名和监控"></a>2.6 命名和监控</h2><p>仅仅创建和放置任务是不够的：一个服务的客户端和其它系统需要能找到它们，即使该服务被重新放置到另一台机器之后。为实现该需求，Borg为每个任务创建了一个固定的BNS名字（BNS，Borg name Service），这个名字包括了Cell名，作业名和任务序号。Borg把任务的主机名和端口写入Chubby[14]的一个持久化高可用文件里，以BNS名字为文件名。这个文件被RPC用来发现任务的实际地址。BNS名字也是任务的DNS名字的基础部分，例如，cc Cell的ubar用户的jfoo 作业的第50个任务可以通过<code>50.jfoo.ubar.cc.borg.google.com</code>来访问。每当状态改变时，Borg还会把作业的大小和任务的健康信息写入到Chubby，这样负载均衡器就知道如何路由请求了。</p><p>几乎每个任务都有一个内置的HTTP服务器，用来发布任务的健康信息和几千个性能指标（如RPC延时）。Borg监控这些健康检查的URL，重启那些没有立刻响应或返回HTTP错误码的任务。监控工具跟踪其它数据并显示在仪表盘上，当违反服务水平目标（SLO）时报警。</p><p>用户可以使用一个称为Sigma的Web界面来检查他的所有作业的状态，针对某个Cell，或者深入某个作业及任务，检查其资源行为、详细日志、执行历史和最终结果。我们的应用产生大量的日志，它们都会被自动的滚动以避免耗尽硬盘空间。任务退出后，日志会保留一小段时间以帮助调试。如果一个作业没有运行起来，Borg会提供一个挂起原因的标注，以及建议如何修改作业的资源请求，以使其更适合Cell。我们发布了如何使资源请求更容易被调度的指南。</p><p>Borg将所有的作业提交、任务事件、以及每个任务的详细资源使用都记录在Infrastore里。Infrastore是一个可扩展的只读数据存储，通过Dremel[61]提供了类似SQL的交互式接口。这些数据用以支持基于使用量的收费，调试作业和系统故障，以及长期容量规划。公开的Google集群负载数据集[80]也来自于这些数据。</p><p>所有这些特性帮助用户理解和调试Borg及其作业的行为，并帮助我们的SRE实现每人管理超过上万台机器。</p><h1 id="3-Borg架构"><a href="#3-Borg架构" class="headerlink" title="3. Borg架构"></a>3. Borg架构</h1><p>一个Borg的Cell包括一组机器，一个逻辑上集中的控制器，称为Borgmaster，以及运行在每台机器上的称为Borglet的代理进程（见图1）。Borg的组件都是用C++实现的。</p><h2 id="3-1-Borgmaster"><a href="#3-1-Borgmaster" class="headerlink" title="3.1 Borgmaster"></a>3.1 Borgmaster</h2><p>Cell的Borgmaster由两个进程组成：Borgmaster主进程和一个单独的调度进程（§3.2）。Borgmaster主进程处理客户端的RPC，包括修改状态（如创建作业），或提供只读数据（如查找作业）。它还管理着系统中所有对象（机器、任务、Allocs等）的状态，与Borglet通信，并提供一个Web UI（作为Sigma的备份）。</p><p>Borgmaster在逻辑上是单个进程，但实际上有5个副本。每个副本在内存维护着Cell状态的拷贝，该状态同时保存在由这些副本的本地硬盘组成的一个基于Paxos[55]的高可用、分布式存储上。每个Cell中仅有一个选举出来的Master，它同时作为Paxos的Leader和状态修改者，处理所有变更Cell状态的请求，例如提交作业或者结束某台机器上的一个任务。当Cell启动或者上一个Master故障时，新的Master会通过Paxos算法选举出来；新Master会获取一个Chubby锁，这样其它的系统就可以找到它。选举并转移到新的Master通常需要10秒，但在大的Cell里可能需要长达1分钟，因为需要重构一些内存状态。当一个副本从宕机恢复后，它会动态地从其它最新的Paxos副本中重新同步自己的状态。</p><p>某个时刻的Borgmaster状态被称为检查点（Checkpoint），以定期快照加变更日志的形式保存在Paxos存储里。检查点有很多用途：如重建过去任意时刻的Borgmaster状态（例如，在接收一个触发了Borg故障的请求之前，这样就可以用来调试）；极端情况下可以手工修复检查点；构建一个持久的事件日志供日后查询；或用于离线仿真。</p><p>一个高保真的Borgmaster模拟器，称为Fauxmaster，可以读取检查点文件。Fauxmaster的代码拷贝自线上的Borgmaster，还有对Borglet的存根接口。它接收RPC来改变状态，执行操作，例如“调度所有等待的任务”。我们用它来调试故障，像跟在线的Borgmaster那样与模拟器交互，用模拟的Borglet重放检查点文件里的真实交互。用户可以单步执行并观察系统过去确实发生了的状态变化。Fauxmaster也用于容量规划（可以接收多少个此类型的作业？），以及在实际更改Cell配置前做可行性检查（这个变更会导致关键作业异常退出吗？）</p><h2 id="3-2-调度"><a href="#3-2-调度" class="headerlink" title="3.2 调度"></a>3.2 调度</h2><p>当提交一个作业后，Borgmaster会把它保存在持久的Paxos存储上，并将这个作业的所有任务加入等待队列中。调度器异步地扫描等待队列，将任务分配到满足作业约束且有足够资源的机器上（调度是针对任务的，而非作业）。队列扫描从高优先级到低优先级，同优先级则以轮转的方式处理，以保证用户间的公平，并避免队首的大型作业阻塞队列。调度算法有两个部分：<strong>可行性检查</strong>，找到一组可以运行任务的机器；<strong>评分</strong>，从中选择一个合适的机器。</p><p>在可行性检查阶段，调度器会找到一组满足任务约束且有足够可用资源的机器 —— 可用资源包括已经分配给低优先级任务但可以抢占的资源。在评分阶段，调度器确定每台可行机器的适宜性。评分考虑了用户特定的偏好，但主要取决于内置的标准：例如最小化被抢占任务的个数和优先级，选择已经有该任务安装包的机器，尽可能使任务分散在不同的供电和故障域，以及装箱（Packing）质量（在单台机器上混合高、低优先级的任务，以允许高优先级任务在负载尖峰扩容）等。</p><p>Borg早期使用修改过的E-PVM[4]算法来评分。这个算法对异构的资源生成等效的成本值，放置任务的目标是使成本的变化量最小。在实践中，E-PVM会把负载分散到所有机器，为负载尖峰预留出资源 —— 这样的代价是增加了碎片，特别是对需要大部分机器的大型任务而言；我们有时称其为“最差匹配”。</p><p>与之相反的是“最佳匹配”，把机器上的任务塞的越满越好。这就“空”出一些没有用户作业的机器（它们仍运行存储服务），这样放置大型任务就比较直接了。但是，如果用户或Borg错误估计了资源需求，紧实的装箱会对此造成（性能上的）惩罚。这种策略不利于有突发负载的应用，而且对申请少量CPU的批处理作业特别不友好，这些作业申请少量CPU本来是为了更容易被调度执行，并抓住机会使用空闲资源：20%的non-prod 任务申请少于0.1个CPU核。</p><p>我们目前的评分模型是混合的，试图减少搁浅（Stranded）的资源（指一台机器因某些类型资源全部分配了，导致未能分配的其它类型资源）。对我们的负载而言，这个模型比“最佳匹配”提升了3%-5%的装箱效率（以[78]定义的方式评价）。</p><p>如果评分后选中的一台机器仍没有足够的资源来运行新任务，Borg会抢占低优先级的任务，从最低优先级向上逐级抢占，直到资源足够运行该任务。被抢占的任务放回到调度器的等待队列里，而不是被迁移或休眠（注：例外情况是，为Google Compute Engine提供虚拟机的任务会被迁移）。</p><p>任务的启动延迟（从提交作业到任务开始运行之间的时间段）是我们持续重点关注的。这个时间差别很大，中位数约25秒。安装软件包耗费了其中80%的时间：一个已知的瓶颈就是软件包写入时对本地硬盘的竞争。为了减少任务启动时间，调度器偏好将任务分配到已经有必需的软件包（程序及数据）的机器：大部分包是只读的，所以可以被共享和缓存（这是Borg调度器唯一的一种数据局部性支持）。另外，Borg通过树形和类似BT的协议并发地将软件包分发到多个机器上。</p><p>此外，调度器采用多种技术使其能够扩展到数万台机器的Cell（§3.4）。</p><h2 id="3-3-Borglet"><a href="#3-3-Borglet" class="headerlink" title="3.3 Borglet"></a>3.3 Borglet</h2><p>Borglet是部署在Cell每台机器上的本机Borg代理。它负责启动和停止任务；重启失败的任务；通过OS内核设置来管理本地资源；滚动调试日志；把本机的状态上报给Borgmaster和其它监控系统。</p><p>Borgmaster每过几秒就会轮询每个Borglet来获取机器的当前状态，并向其发送请求。这让Borgmaster能控制通信频率，省去了显式的流量控制机制，而且防止了恢复风暴[9]。</p><p>选举出来的Master负责准备发送给Borglet的消息，并根据Borglet的响应更新Cell的状态。为了性能扩展性，每个Borgmaster副本会运行一个无状态的链接分片（Link Shard）来处理部分Borglet的通信；Borgmaster选举后重新计算链接的分片。为了保证容错（Resiliency），Borglet总是汇报全部状态，但是Link Shard只汇报变化值，从而聚合、压缩这些信息，减少Master更新的负担。</p><p>如果某个Borglet几次没有响应轮询请求，该机器会被标记为宕机，其上运行的所有任务会被重新调度到其它机器。如果通讯恢复了，Borgmaster会让这个Borglet杀掉已经被重新调度出去的任务，以避免重复。即便无法与Borgmaster通信，Borglet仍会继续正常运行。所以即使所有的Borgmaster都出故障了，正在运行的任务和服务还会保持运行。</p><h2 id="3-4-扩展性"><a href="#3-4-扩展性" class="headerlink" title="3.4 扩展性"></a>3.4 扩展性</h2><p>我们还没有遇到Borg这种集中式架构的终极扩展上限。我们顺利突破了遇到的每个限制。一个单独的Borgmaster可以管理有数千台机器的Cell，有若干Cell每分钟有10000多个到达的任务。一个繁忙的Borgmaster使用10~14个CPU核以及50GB内存。我们用了几项技术来实现这种扩展性。</p><p>早期版本的Borgmaster使用一个简单的，同步的循环来处理请求、调度任务，并与Borglet通信。为了处理更大的Cell，我们把调度器分离为一个单独的进程，这样它就可以与其它的Borgmaster功能并行执行，而这些其它的功能有多副本以容错。一个调度器副本使用一份缓存的Cell状态拷贝，重复执行下面的操作：从选举出来的Master获取状态改变（包括已分配的和等待中的工作）；更新自己的本地拷贝；执行一遍调度来分配任务；将分配信息发送给Master。Master会接受并应用这些分配，但如果分配不适合（例如，是基于过时的状态做出的），就会等待调度器的下一遍调度。这与Omega[69]使用的乐观并发控制思路很相似，而且我们最近还给Borg添加了对不同负载类型使用不同调度器的功能。</p><p>为了改进响应时间，Borglet使用独立的线程分别进行通信和响应只读RPC。为了更好的性能，我们将这些请求划分给5个Borgmaster副本（§3.3）。总的效果是，UI响应时间的99%分位数小于1秒，而Borglet轮询间隔的95%分位数小于10秒。</p><p>一些提高Borg调度器扩展性的方法如下：</p><p><strong>缓存评分</strong>：计算一台机器的可行性和评分是比较昂贵的，所以Borg会一直缓存评分，直到这台机器或者任务的属性发生了变化 —— 例如，这台机器上的某个任务结束了，一些属性修改了，或者任务的需求改变了。忽略小额的资源变化可以减少缓存失效。</p><p><strong>任务等价类（Equivalence classes）</strong>：一般来说，同一个Borg 作业的任务有相同的请求和约束。任务等价类即一组有相同需求的任务。Borg只对等价类中的一个任务进行可行性检查和评分，而不是对等待的每个任务去检查一遍所有机器的可行性并对可行的机器评分。</p><p><strong>适度随机</strong>：在一个大的Cell中，对所有机器都去计算一遍可行性和评分是很浪费的。调度器会随机地检查机器，直到找到足够多的可用机器来评分，然后从中挑选出最好的一个。这减少了任务启动和退出所需的评分次数及导致的缓存失效，加快了任务分配过程。适度随机有点类似Sparrow[65]的批量采样技术（ying：Sparrow的批量采样考虑的是机器上的任务队列长度），但Borg还处理了优先级、抢占、异构性和安装软件包的成本。</p><p>在我们的实验中（§5），从零开始调度整个Cell的工作负载只要几百秒，但禁用上面几项技术的话，3天都不够。正常情况下，半秒之内就能完成一遍等待队列的在线调度。</p><h1 id="4-可用性"><a href="#4-可用性" class="headerlink" title="4. 可用性"></a>4. 可用性</h1><p><img width="600" src="/img/borg-fig-03.png" alt="图3. 不同类型任务的异常退出率及原因（包括抢占、资源不足、机器故障、机器关机、其它）。数据从2013-08-01开始。"></p><p>大型系统里故障是很常见的[10, 11, 12]。图3展示了在15个样本Cell里任务异常退出的原因分类。在Borg上运行的应用需要能处理这种事件，可采用的技术有多副本、保存持久状态到分布式存储，或定期快照（如果可行的话）等。当然，我们也尽可能的缓解异常事件的影响。例如，Borg提供了：</p><ul><li>自动重新调度异常退出的任务，如果必要，可以放置到另一台机器上去运行</li><li>把一个作业的任务分散到不同的可用域，例如机器、机架、供电域层次，以减少关联失效</li><li>在机器/OS升级等维护活动期间，限制任务受影响的速率，以及同一作业中同时中止的任务的个数</li><li>使用声明式的预期状态表示和幂等的变更操作，这样故障的客户端可以无损地重复提交故障期间漏掉的请求</li><li>对于失联的机器上的任务，限制重新调度的速率，因为大规模的机器故障和网络分区是很难区分的</li><li>回避造成崩溃的 &lt;任务：机器&gt; 组合</li><li>通过不断重新执行日志保存任务（§2.4），恢复已写入本地硬盘的关键中间数据，就算这个日志关联的Alloc已经终止或调度到其它机器上了。用户可以设置系统保持重复尝试多久，通常是几天时间。</li></ul><p>Borg的一个关键设计特性是：就算Borgmaster或者Borglet挂掉了，已经运行的任务还会继续运行下去。不过，保持Master正常运行仍然重要，因为在它退出后就无法提交新的作业，无法更新运行作业的状态，也不能重新调度故障机器上的任务。</p><p>Borgmaster使用多项的技术支持其获得99.99%的实际可用性：多副本应对机器故障；准入控制应对超载；使用简单、底层的工具部署实例，以减少外部依赖。Cell彼此是独立的，减少了关联误操作和故障传播的机会。同时这也是我们不扩大Cell规模的主要考虑，而并非是扩展性的限制。</p><h1 id="5-利用率"><a href="#5-利用率" class="headerlink" title="5. 利用率"></a>5. 利用率</h1><p>Borg的一个主要目标就是有效地利用Google的大量机器（这是一大笔财务投资）：让效率提升几个百分点就能省下几百万美元。这一节讨论和评估了一些Borg使用的策略和技术。</p><h2 id="5-1-评估方法"><a href="#5-1-评估方法" class="headerlink" title="5.1 评估方法"></a>5.1 评估方法</h2><p>作业有部署约束，而且需要处理负载尖峰（尽管比较少见）；机器是异构的；我们回收服务型作业的资源来运行批处理作业。因此，我们需要一个比“平均利用率”更高级的指标来评估我们的策略选择。大量实验后，我们选择了Cell压缩量（Compaction）：给定一个负载，我们不断地移除机器，直到无法容纳该负载，从而得知所需最小的Cell规模。从空集群开始部署该负载并重复多次，以减少特殊情况的影响。终止条件是明确的，对比可以自动化，避免了生成和建模合成负载的陷阱[31]。[78]提供了评估技术的定量比较，其中的细节非常微妙。</p><p>我们不可能在线上Cell进行实验，但是我们用了Fauxmaster来获得高保真的模拟效果，它使用了真实生产Cell和负载的数据，包括所有约束、实际限制、预留和使用量数据（§5.5）。实验数据提取自2014-10-01 14:00 PDT的Borg快照（其它快照也有类似的结论）。我们首先排除了特殊用途的、测试用的、小型的（少于5000台机器）的Cell，然后从剩下的Cell中选取了15个样本，抽样尽量关于Cell的大小均匀分布。</p><p>为了保持机器异构性，在Cell压缩实验中，我们随机地移除机器。为了保持工作负载的异构性，我们保留了所有负载（除了那些绑定到特定机器的服务和存储任务，如Borglet）。我们把那些需要超过原Cell大小一半的作业的硬性限制改成柔性的，允许不超过0.2%的任务一直等待，这是针对一些特别“挑剔”的，只能放置在很少的特定机器上的任务；充分的实验表明结果是可复现的，波动很小。如果需要一个大型的Cell，就把原Cell复制几倍；如果需要更多的Cell，也是复制原Cell。</p><p>每个实验都用不同的随机数种子对每个Cell重复了11次。图中，我们用误差线线来表示所需机器数量的最大和最小值，选择90%分位数作为结果 —— 平均值或中位数不能反映系统管理员所期望的充分把握。我们认为Cell压缩率是一个公平一致的比较调度策略的方法，而且可以直接转化为成本/收益的结果：更好的策略只需要更少的机器来运行相同的负载。</p><p>我们的实验关注于即时的调度（装箱），而不是重放一段长时间的负载记录。部分原因是避免处理开放或闭合的队列模型[71, 79]的困难；部分是传统的完成时间不适用于长时间运行的服务；部分是这样可以提供明确的比较结果；部分是因为我们认为不会对结果产生显著影响；还有部分现实原因，我们发现一次实验使用了20万个Borg CPU核 —— 即便对Google而言，这个成本也不是个小数目。</p><p><img width="600" src="/img/borg-fig-04.png" alt="图4. 压缩的效果。15个Cell在压缩后相比原规模的百分比累积分布（CDF）"></p><p>生产环境中，我们特意保留了一些裕度（Headroom），以应对负载增长、偶然的“黑天鹅”事件、负载尖峰、机器故障、硬件升级、以及大范围的局部故障（如供电母线短路）。图4显示了如果应用Cell压缩，实际的Cell可以压缩到多小。下文的图使用这些压缩后的大小作为基准值。</p><h2 id="5-2-Cell共享"><a href="#5-2-Cell共享" class="headerlink" title="5.2 Cell共享"></a>5.2 Cell共享</h2><p>几乎所有的机器都同时运行prod和non-prod的任务：在共享的Cell里是98%的机器，在所有Borg管理的机器里是83%（有一些是Cell专用的）。</p><p><img src="/img/borg-fig-05.png" alt="图5. 将prod和non-prod工作划分到不同的集群将需要更多的机器。两幅图中的百分比都是相对于单个集群所需机器的最少数量而言的"></p><p>鉴于很多外部组织将面向用户的作业和批处理作业分别运行在不同的集群上，我们检查一下如果我们也这么做会怎样。图5表明，在一个中等大小的Cell上，分开运行prod和non-prod的工作负载将需要增加20-30%的机器。这是因为prod的作业通常会保留一些资源来应对极少发生的负载尖峰，但大多情况下用不到这些资源。Borg回收了这些用不到的资源（§5.5），来运行non-prod的工作，所以总体我们只需要更少的机器。</p><p><img width="600" src="/img/borg-fig-06.png" alt="图6. 将用户分开到不同的集群也会需要更多的机器"></p><p>大部分Borg Cell被数千个用户共享使用。图6展示了为什么要共享。测试中，如果一个用户消费了超过10TiB（或100TiB）的内存，我们就把这个用户的工作负载分离到另一个Cell中。我们目前的共享策略是有效的：即使100TiB的阈值，也需要2-16倍的Cell，增加20-150%的机器。将资源池化再次显著地节省了成本。</p><p>但是，把很多不相关的用户和作业类型放置到同一台机器上可能会造成CPU冲突，我们是否需要更多的机器来补偿？为评估这一点，我们看一下固定机器类型和时钟频率，任务的CPI（Cycles per Instruction，执行每条指令平均所需时钟数，越大则程序执行越慢）在其它环境条件不同的影响下是如何变化的。在这种实验条件下，CPI是一个可比较的指标，而且可以表征性能冲突，因为2倍的CPI意味着一个CPU密集型程序需要2倍的执行时间。数据是在一周内从约12000个随机选择的prod任务获取的，使用了[83]中介绍的硬件剖析工具记录5分钟内的时钟数和指令数，并调整了采样的权重，使CPU时间的每秒都均等处理。结果并非直截了当的：</p><p>（1）我们发现CPI在同一个时间段内和下面两个量正相关：这台机器上总的CPU使用量，以及这个机器上同时运行的任务个数（基本上独立）；每向一台机器上增加一个任务，就会使其它任务的CPI增加0.3%（从数据拟合的线性模型给出的预测值）；将一台机器的CPU使用量增加10%，就会增加2%弱的CPI。尽管相关性在统计意义上是显著的，也只是解释了CPI变化的5%。还有其它的因素，支配着CPI的变化，例如，应用程序固有的差别，以及特殊的干扰模式[24, 83]。</p><p>（2）比较从共享Cell和只运行几种应用的少数专用Cell获取的CPI采样，我们看到共享Cell里的CPI平均值为1.58（σ=0.35，标准差），专用Cell的CPI平均值是1.53（σ=0.32） —— 也就是说，共享Cell的性能差3%。</p><p>（3）为了搞定不同Cell的应用会有不同的工作负载，或者会有幸存者偏差（或许对冲突更敏感的程序会被挪到专用Cell里面去），我们观察了Borglet的CPI。所有Cell的所有机器上都运行着Borglet。我们发现专用Cell里Borlet的CPI平均值是1.20（σ=0.29），而共享Cell里的CPI平均值为1.43（σ=0.45），表明在专用Cell上比在共享Cell上快1.19倍，不过这个结果忽略了专用Cell中的机器负载较轻的因素，即稍偏向专用Cell。</p><p>这些实验表明了仓库级别的性能比较是复杂的，强化了[51]中的观察，但也说明共享并没有显著增加运行程序的开销。</p><p>不过，就算从结果中最差的数据来看，共享还是有益的：比起CPU的降速，共享比各个划分方案都减少了机器，这一点更重要，而且共享的收益适用于包括内存和硬盘等各种资源，不仅仅是CPU。</p><h2 id="5-3-大型Cell"><a href="#5-3-大型Cell" class="headerlink" title="5.3 大型Cell"></a>5.3 大型Cell</h2><p><img src="/img/borg-fig-07.png" alt="图7. 将Cell分成更小的规模将需要更多的机器"></p><p>Google建立了大型Cell，一是为了允许运行大型任务，二是为了减少资源碎片。为测试减少碎片的效果，我们把负载从一个Cell分散多个较小的Cell中 —— 首先将作业随机排列，然后轮流分配到各小的Cell中。图7确认了使用小型Cell需要增加相当多的机器。</p><h2 id="5-4-细粒度资源请求"><a href="#5-4-细粒度资源请求" class="headerlink" title="5.4 细粒度资源请求"></a>5.4 细粒度资源请求</h2><p>Borg用户请求的CPU单位是0.001个核，内存和硬盘的单位是字节。（一个核实际上是一个CPU的超线程，对不同机器类型的性能进行了标准化）。图8表明用户充分利用了细粒度：请求的CPU核和内存数量的“特别偏好值”是很少的，这些资源也没有明显的相关性。这与[68]里的分布非常相似，除了我们在90%分位数及以上的内存请求多一点之外。</p><p>尽管IaaS普遍只提供一组固定尺寸的容器或虚拟机[7, 33]，但不符合我们的需求。为说明这一点，我们对prod的作业和Alloc（§2.4）申请的CPU核和内存分别向上取整到最接近的2的幂，形成固定大小的“桶”，最小的桶有0.5个核和1GiB内存。图9显示一般情况下这样需要增加30-50%的资源。上限的情形是，有的大型任务即便将Cell扩大为未压缩尺寸的四倍也无法容纳，只好为其分配一整台机器。下限是允许这些任务一直等待。（这比[37]给出的将近100%的额外开销要小一些，因为我们支持不止4种尺寸的桶，而且允许CPU和内存分别改变）。</p><h2 id="5-5-资源回收"><a href="#5-5-资源回收" class="headerlink" title="5.5 资源回收"></a>5.5 资源回收</h2><p>作业可以声明一个资源<strong>限额（Limit）</strong>，是每个任务能获得的资源上限。Borg会用它来检查用户是否有足够的配额来接受该作业，并检查某个机器是否有足够的可用资源来运行任务。因为Borg通常会杀死那些试图使用超出内存和硬盘申请值的任务，或者限制其CPU使用量不超过申请值，所以有的用户会为任务申请超过实际需要的资源，就像有的用户会购买超过实际需要的配额一样。另外，一些任务只是偶尔需要使用它们申请的所有资源（例如，在一天中的高峰期或者受到了拒绝服务攻击），但大多时候用不了。</p><p>与其把那些分配出来但暂时没有被用到的资源浪费掉，我们估计了一个任务会用多少资源，然后把剩余的资源回收给那些可以忍受低质量资源的任务，例如批处理作业。这整个过程称为<strong>资源再利用</strong>。这个估值称为任务的资源<strong>预留（Reservation）</strong>。Borgmaster每隔几秒就会根据Borglet获取的细粒度资源使用量信息来计算一次预留值。最初的预留资源被设置为资源限额；在300秒之后，也就过了启动阶段，预留资源会缓慢的下降到实际使用量加上一个安全值。在实际使用量超过它时，预留值会迅速增加。</p><p>Borg调度器使用资源限额来计算prod级任务（注：准确的说，是高优先级的、延迟敏感的任务，见§6.2）是否可以执行（§3.2），所以这些任务不依赖于回收的资源，也与资源超售无关；对于non-prod的任务，运行任务使用的资源在预留值之内，这样新任务就可以使用回收的资源。</p><p>一台机器有可能因为预留（预测）错误而导致运行时资源不足 —— 即使所有的任务都在资源限额之内。如果这种情况发生了，我们会杀掉或者限制non-prod任务，但从来不对prod任务下手。</p><p>图10表明，如果没有资源回收，将需要更多的机器。在一个中等规模的Cell中大概有20%的工作负载（§6.2）使用了回收的资源。</p><p>图11可以看到更多的细节，其中有预留值、使用量与限额的比例。当资源紧张时，超出内存限额的任务首先会被抢占，不论优先级有多高，所以很少有任务超过内存限额。另一方面，CPU使用量是可以被轻易限制住的，所以短时的毛刺虽然会导致使用量超过预留值，但这没什么损害。</p><p>图11表明了资源回收可能还过于保守：在预留值和实际使用量中间还有一大段差距。为了测试这一点，我们选择了一个线上Cell，（第一周作为参照基准，）第二周将其估计算法的参数调整为比较<strong>激进</strong>的设置，即把安全裕度留小一点；第三周采取的是介于激进和基准之间的<strong>适度</strong>策略，最后一周恢复到基准策略。</p><p><img src="/img/borg-fig-08-09-10-11.png" alt=""></p><p>图12展示了结果。第二周的预留值明显更接近实际使用量，第三周稍大一点，最大的是第一周和第四周。和预期的一样，第二周和第三周的OOM比率轻微地增加了（注：第3周后期的异常情况与本次实验无关）。在评估了这个结果后，我们认为利大于弊，于是在其它Cell上也采用了<strong>适度</strong>策略的资源回收参数。</p><p><img src="/img/borg-fig-12.png" alt="图12. 更激进的资源估计可以回收更多资源，但会稍增加OOM事件"></p><h1 id="6-隔离"><a href="#6-隔离" class="headerlink" title="6. 隔离"></a>6. 隔离</h1><p>50%的机器运行了9个以上的任务；处于90%分位数的机器则有大约25个任务，4500个线程[83]。虽然在应用之间共享机器会增加利用率，但也需要一个比较好的机制来保证任务之间不产生干扰。这同时适用于安全和性能两个方面。</p><h2 id="6-1-安全隔离"><a href="#6-1-安全隔离" class="headerlink" title="6.1 安全隔离"></a>6.1 安全隔离</h2><p>我们使用Linux的chroot作为同一台机器上不同任务之间的主要安全隔离机制。仅当某台机器有用户运行的任务时，为了允许远程调试，我们以前会自动分发（或废除）SSH秘钥，使用户可以访问这台机器。对大多数用户来说，现在被替换为<code>borgssh</code>命令，这个程序和Borglet协同构建一个SSH通道，连接到与任务运行在同一个chroot和cgroup下的Shell，这样限制就更加严格了。</p><p>Google的AppEngine（GAE）[38]和Google Compute Engine（GCE）使用VM和安全沙箱技术运行外部的软件。我们把每个运行在KVM进程中的VM作为一个Borg任务来运行。</p><h2 id="6-2-性能隔离"><a href="#6-2-性能隔离" class="headerlink" title="6.2 性能隔离"></a>6.2 性能隔离</h2><p>早期的Borglet使用了一种相对原始的资源隔离措施：事后检查内存、硬盘和CPU使用量，终止使用过多内存和硬盘的任务，或者降低使用过多CPU的任务的Linux CPU优先级。不过，一些粗暴的任务还是能很容易地影响到同台机器上其它任务的性能，于是有的用户就会多申请资源来让Borg减少与其共存的任务数量，降低了资源利用率。资源回收可以弥补一些损失，但不是全部，因为涉及到安全裕度。在极端情况下，用户会要求使用专属的机器或者Cell。</p><p>目前，所有Borg任务都运行在基于Linux cgroup的资源容器[17, 58, 62]里。Borglet控制着这些容器的设置。有了OS内核的帮助，控制能力得到了改善。即使这样，偶尔还是有低级别的资源冲突发生（例如内存带宽或L3缓存污染），见[60, 83]。</p><p>为了应对超载和超售，Borg任务有一个应用类别（appclass）。最重要的区分是延迟敏感（LS）的应用和本文中称为批处理（batch）的其它类别。LS任务包括面向用户的应用和需要快速响应的共享基础设施。高优先级的LS任务得到最高优待，可以暂时让批处理任务等待几秒种。</p><p>第二个区分是：<strong>可压缩</strong>资源（例如CPU，硬盘I/O带宽），都是基于速率的，可以通过降低一个任务的服务质量而不是杀死它来回收；<strong>不可压缩</strong>资源（例如内存、硬盘空间），这些一般来说不杀掉任务是没办法回收的。如果一个机器用光了不可压缩资源，Borglet马上就会开始杀死任务，从低优先级开始，直到能满足剩下的资源预留。如果机器用完了可压缩资源，Borglet会限制使用量（偏好LS任务），这样不用杀死任何任务也能处理短期负载尖峰。如果情况没有改善，Borgmaster会从这个机器上移除一个或多个任务。</p><p>Borglet有一个用户态的控制循环，负责以下操作：为容器确定内存量，prod任务基于预测值，而non-prod任务则基于内存压力；处理来自内核的OOM事件；当任务试图分配超过其自身限额的内存时，或者超售的机器上确实耗尽内存时，都会杀死任务。Linux激进的文件缓存让我们的实现复杂得多，因为需要精确计算内存使用量。</p><p>为了增强性能隔离，LS任务可以预留整个物理CPU核，以阻止别的LS任务来使用它们。批处理任务被允许运行在任何核上，但是相比LS任务，批处理任务只分配了很少的调度份额。Borglet动态地调整贪婪的LS任务的资源上限，以保证它们不会把批处理任务饿上几分钟，必要时有选择的使用CFS带宽控制[75]；仅用份额来表示是不够的，因为我们有多个优先级。</p><p><img width="600" src="/img/borg-fig-13.png" alt="图13. 调度延迟与负载的关系。即一个就绪线程需要等待超过1 ms才能运行的比率，与机器繁忙程度的关系。每组数据条中，左侧是延迟敏感的任务，右侧是批处理任务。只有很少的比率需要等5 ms以上，超过10 ms就极少了。这是2013年12月从一个代表性的Cell中获取的一个月的数据；误差线是每天的波动"></p><p>同Leverich[56]一样，我们发现标准的Linux CPU调度器（CFS）需要大幅调整才能同时支持低延迟和高利用率。为了减少调度延迟：我们内部版本的CFS对每个<code>cgroup</code>都有单独的负载历史[16]；允许LS任务抢占批处理任务；当一个CPU有多个就绪的LS任务时，减少其调度数量。幸运的是，我们的大多应用使用每个线程处理一个请求的模型，这样就缓解了长期的负载不均衡。我们节俭地使用<code>cpusets</code>给有特别严格的延迟需求的应用分配CPU核。这些努力的一些效果展示在图13中。我们持续在这方面投入，增加感知NUMA、超线程、能耗（如[81]）的线程放置和CPU管理，改进Borglet的控制精确度。</p><p>任务被允许在其上限之内消费资源。大部分任务还允许去使用超出上限的可压缩资源，例如CPU，以利用空闲资源。只有5%的LS任务禁止这么做，主要是为了改善可预测性；小于1%的批处理任务也禁止了。使用超量内存默认是被禁止的，因为这会增加任务被杀掉的概率，不过即使这样，10%的LS任务解除了这个限制，79%的批处理任务也解除了，因为这是MapReduce框架的默认设置。这补偿了资源回收（§5.5）的后果。批处理任务很乐意使用空闲的或回收的内存：大多情况下这样运作得很好，即使偶尔批处理任务会被急需资源的LS任务杀掉。</p><h1 id="7-相关工作"><a href="#7-相关工作" class="headerlink" title="7. 相关工作"></a>7. 相关工作</h1><p>数十年来，资源调度已经在多种场景得到了研究，如广域高性能计算网格、工作站网络、和大规模服务器集群等。我们这里只关注最相关的大规模服务器集群这个场景。</p><p>最近的一些研究分析了来自于Yahoo!、Google和Facebook的集群记录数据[20, 52, 63, 68, 70, 80, 82]，展现了这些现代的数据中心和工作负载固有的异构性和大规模带来的挑战。[69]包含了对集群管理器架构的分类。</p><p>Apache Mesos[45]将资源管理和任务放置功能拆分到一个集中资源管理器（类似于去掉调度器的Bormaster）和多种“框架”（比如Hadoop[41]和Spark[73]）之间，两者基于供应（Offer）机制交互。Borg则把这些功能集中在一起，使用基于请求的机制，而且扩展性相当好。DRF[29, 35, 36, 66]最初是为Mesos开发的；Borg则使用优先级和准入配额来替代。Mesos开发者已经宣布了他们扩展Mesos的雄心壮志：预测性资源分配和回收，以及解决[69]中发现的一些问题。</p><p>YARN[76]是一个针对Hadoop的集群管理器。每个应用都有一个另外的管理器，与中央资源管理器谈判所需资源；这跟大约2008年开始Google的MapReduce作业已经使用的向Borg获取资源的模式如出一辙。YARN的资源管理器最近才支持容错。一个相关的开源项目是Hadoop Capacity Scheduler（基于容量的调度器）[42]，提供了多租户下的容量保证、多层队列、弹性共享和公平调度。YARN最近扩展支持了多种资源类型、优先级、抢占和高级准入控制[21]。Tetris（俄罗斯方块）研究原型[40]支持完成时间感知的作业装箱。</p><p>Facebook的Tupperware[64]，是一个在集群中调度<code>cgroup</code>容器的类Borg系统；只有少量细节披露出来了，看起来它也提供了某种形式的资源回收功能。Twitter开源的Aurora[5]是一个类似Borg的，用于长期运行服务的调度器，运行与Mesos之上，其配置语言和状态迁移与Borg类似。</p><p>微软的Autopilot[48]为其集群提供了“自动化的软件供应和部署；系统监控；以及采取修复行为处理软硬件故障”的功能。Borg生态系统提供了相似的特性，不过篇幅所限，不再深入讨论；作者Isaard概括了很多我们也赞成的最佳实践。</p><p>Quincy[49]使用了一个网络流模型来提供公平性和数据局部性感知的调度，应用在几百个节点的集群的数据处理DAG上。Borg使用配额和优先级在用户间共享数据，可以扩展到上万台机器。Quincy可以直接处理执行图，而Borg需要在其上层另外构建。</p><p>Cosmos[44]聚焦在批处理上，强调了用户可以公平获取他们已经捐献给集群的资源。每个作业分别有一个管理器来获取资源；只有很少公开的细节。</p><p>微软的Apollo系统[13]为每个短期批处理作业分别使用单独的调度器，以获得高吞吐量，其集群规模看起来与Borg的Cell相当。Apollo投机地执行低优先级后台任务来提升资源利用率，代价是有时有长达多日的队列延迟。Apollo的各节点都一个关于开始时间的预测矩阵，其行列分别为CPU和内存两个资源维度。调度器会综合开始时间、估计的启动开销、获取远程数据的开销来决定部署位置，并用一个随机延时来减少冲突。Borg使用的是中央调度器，基于之前的分配来决定部署位置，可以处理更多的资源维度，而且更关注高可用、长期运行的应用；Apollo也许能处理比Borg更高的任务到达率。</p><p>阿里巴巴的伏羲（Fuxi）[84]支持数据分析的负载，从2009年就开始运行了。类似Borgmaster，一个集中的FuxiMaster（也做了容错多副本）从节点上获取可用资源的信息、接受应用的资源请求，然后匹配两者。伏羲的增量调度策略与Borg的任务等价类是相反的：伏羲用最新的可用资源匹配等待队列里的任务（ying：Borg是用任务匹配资源）。类似Mesos，伏羲允许定义“虚拟资源”类型。只有对合成工作负载的实验结果是公开的。</p><p>Omega[69]支持多个并发的调度器，粗略相当于没有持久存储和链接分片的Borgmaster。Omega调度器使用乐观并发控制的方式去操作一个共享的集群预期的和观察的状态表示。集群状态存储在一个集中持久存储中，用单独的连接组件与Borglet同步。Omage架构设计为支持多种不同的工作负载，它们有自己特定的RPC接口、状态迁移和调度策略（例如长期运行的服务、多个框架批处理作业、如集群存储这样的基础服务、Google云平台上的虚拟机）。相反，Borg提供了一种通用方案，同样的RPC接口、状态迁移、调度策略，为支持多种不同的负载，其规模和复杂度逐渐增加，但目前来说可扩展性还不算一个问题（§3.4）。</p><p>Google的开源项目Kubernetes系统[53]把应用放在Docker容器内[28]，再分发到多个机器上。它即可以运行在物理机上（像Borg那样），也可以运行在多个云供应商（比如Google Compute Engine，GCE）的主机上。Kubernetes正在快速开发中，它的很多开发者也参与开发了Borg。Google提供了一个托管的版本，称为Google Container Engine（GKE）[39]。我们会在下一节里面讨论Kubernetes从Borg中学到了哪些东西。</p><p>在高性能计算社区对这个领域有长期的研究传统（如Maui, Moab, Platform LSF[2, 47, 50]）；但是这和Google Cell所面对的规模、工作负载、容错性是不同的。总体而言，为达到高用率，这些系统需要让任务在一个很长的队列中等待。</p><p>虚拟化供应商，例如VMware[77]，和数据中心方案供应商，例如HP和IBM[46]提供了典型情况下可以扩展到一千台机器规模的集群管理解决方案。另外，一些研究小组的原型系统以多种方式提升了调度质量（如[25, 40, 72, 74]）。</p><p>最后，正如我们所指出的，大规模集群管理的另外一个重要部分是自动化和无人化。[43]指出，失效预案、多租户、健康检查、准入控制，以及可重启性对实现单个运维人员管理更多的机器的目标是必要的。Borg的设计哲学也是这样的，而且支撑了我们的每个SRE管理数万台机器。</p><blockquote><p>Borg从它的前任继承了很多东西，即我们内部的全局工作队列（Global Work Queue）系统，它最初是由Jeff Dean，Olcan Sercinoglu, 和Percy Liang开发的。<br>Conder[85]曾被广泛应用于收集空闲资源，其ClassAds机制[86]支持声明式的语句和自动属性匹配。</p></blockquote><h1 id="8-经验教训和未来工作"><a href="#8-经验教训和未来工作" class="headerlink" title="8. 经验教训和未来工作"></a>8. 经验教训和未来工作</h1><p>在这一节中我们介绍了十多年来我们在生产环境运行Borg得到的定性的经验教训，然后介绍设计Kubernetes[53]是如何吸收这些经验的。</p><h2 id="8-1-教训"><a href="#8-1-教训" class="headerlink" title="8.1 教训"></a>8.1 教训</h2><p>我们从一些Borg作为反面警示的特性开始，然后介绍Kubernetes的替代方案。</p><p><strong>将作业作为唯一的任务分组机制比较受限</strong></p><p>Borg没有内置的方法将多个作业组成单个实体来管理，或将相关的服务实例关联起来（例如，测试通道和生产通道）。作为一个技巧，用户把他们的服务拓扑编码到作业的名字中，然后构建了更高层的管理工具来解析这些名字。这个问题的另外一面是，没办法指向服务的任意子集，这就导致了僵硬的语义，以至于无法滚动升级或改变作业的实例数。</p><p>为了避免这些困难，Kubernetes不再使用作业这个概念，而是用标签（Label）来组织它的调度单元（Pod）。标签是任意的键值对，用户可以对系统的任何对象打上标签。Borg作业可以等效地通过对一组Pod打上 &lt;作业：作业名&gt; 这样的标签来实现。其它有用的分组方式也可以用标签来表示，例如服务、层级、发布类型（如，生产、就绪、测试）。Kubernetes用标签查询的方式来选取待操作的目标对象。这样就比固定的作业分组更加灵活。</p><p><strong>同一台机器的任务共享一个IP太复杂了</strong></p><p>Borg中，同一台机器上的所有任务都使用主机的同一个IP地址，共享端口空间。这就带来几个麻烦：Borg必须把端口当做资源来调度；任务必须先声明它需要多少端口，而且需要支持启动时传入可用端口号；Borglet必须强制端口隔离；命名和RPC系统必须像IP一样处理端口（ying：最后这一点我认为是必要的）。</p><p>多亏了Linux的namespace、虚拟机、IPv6和软件定义网络SDN的出现，Kubernetes可以用一种更用户友好的方式来消解这些复杂性：每个Pod和Service都自己的IP地址，允许开发者选择端口而不是让他们的软件支持基础设施的分配，这也消除了基础设施管理端口的复杂性。</p><p><strong>给资深用户优化而忽略了初级用户</strong></p><p> Borg提供了一大堆针对“资深用户”的特性，这样他们就可以仔细地调节他们程序的运行方式（BCL规范约有230个参数）：开始的目的是为了支持Google的大型资源用户，提升他们的效率会带来显著的效益。但不幸的是，这么复杂的API让初级用户用起来很复杂，而且限制了API的演化。我们的解决方案是在Borg上又做了一些自动化的工具和服务，从实验中决定合理的配置。由于应用支持容错，实验可以自由进行：即使自动化出了问题，也只是小麻烦，不会导致灾难。</p><h2 id="8-2-经验"><a href="#8-2-经验" class="headerlink" title="8.2 经验"></a>8.2 经验</h2><p>另一方面，有不少Borg的设计特性是非常有益的，而且经历了时间考验。</p><p><strong>Alloc是有用的</strong></p><p>Borg的Alloc抽象适用于广泛使用的保存日志模式（§2.4），另一个流行的模式是：一个简单的数据加载任务定期更新Web服务器使用的数据。Alloc和软件包机制允许这些辅助服务由不同的小组开发。Kubernetes对应于Alloc的概念是Pod，它是对一个或多个容器的资源封装，其中的容器共享Pod的资源，而且总是被调度到同一台机器上。Kubernetes使用Pod里的辅助容器来替代Alloc里面的任务，不过思路是一样的。</p><p><strong>集群管理不只是任务管理</strong></p><p>虽然Borg的主要角色是管理任务和机器的生命周期，但Borg上的应用还从其它的集群服务中收益良多，例如名字服务和负载均衡。Kubernetes用Service这个抽象概念来支持名字服务和负载均衡：Service有一个名字和用标签选出的多个Pod的动态集合。集群中的任何容器都可以通过Service名连接到该服务。幕后，Kubernetes自动将连接到该Service的负载分散到与其标签匹配的Pod之间，由于Pod挂掉后会被重新调度到其它机器上，Kubernetes还会跟踪这些Pod的位置。</p><p><strong>自省是至关重要的</strong></p><p>虽然Borg总体上是工作良好的，但出了问题后，定位根本原因是非常有挑战性的。Borg的一个关键设计选择是把所有的调试信息暴露给用户而不是隐藏起来：Borg有几千个用户，所以“自助”是调试的第一步。虽然一些用户的依赖项让我们难以废弃一些特性或修改内部策略，但这还是成功的，我们还没找到其它实际的替代方式。为管理大量的数据，我们提供了多个层次的UI和调试工具，这样用户就可以快速定位与其作业相关的异常事件，深入挖掘来自其应用和基础设施本身的详细事件和错误日志。</p><p>Kubernetes也计划引入Borg的大部分自省技术。和Kubernetes一起发布了很多工具，比如用于资源监控的cAdvisor[15]，它基于Elasticsearch/Kibana[30]和Fluentd[32]聚合日志。Master可以用来查询某个对象的状态快照。Kubernetes提供了一致机制，所有可以记录事件的组件（例如，被调度的Pod、出错的容器）都可以被客户端访问。</p><p><strong>Master是分布式系统的核心</strong></p><p>Borgmaster最初设计为一个单体的系统，随着时间发展，它演变成了一组服务生态系统的核心。用户作业管理的管理是由这些服务协同完成的。比如，我们把调度器和主要的UI（Sigma）分离成单独的进程，增加了一组服务，包括准入控制、纵向和横向扩展、任务重新装箱、周期性作业提交（cron）、工作流管理，用于离线查询的系统活动归档等。总体而言，这让我们能扩展工作负载和特性集合，但无需牺牲性能和可维护性。</p><p>Kubernetes的架构走的更远一些：它的核心是一个仅处理请求和操作底层状态目标的API服务。集群管理逻辑构建为一个小型的、可组合的微服务，作为API服务的客户端，如故障后仍维持Pod副本个数在期望值的副本管理器，以及管理机器生命周期的节点管理器。</p><h2 id="8-3-总结"><a href="#8-3-总结" class="headerlink" title="8.3 总结"></a>8.3 总结</h2><p>在过去十年间，所有几乎所有的Google集群负载都迁移到了Borg上。我们仍在持续改进它，并把经验应用到了Kubernetes上。</p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>这篇文章的作者负责撰写文章，并完成了评估实验。几十位设计、实现和维护Borg组件和生态系统的工程师才是它成功的关键。我们在这里列出直接参与设计、实现和维护Borgmaster及Borglet的人员。如果有遗漏，我们深表歉意。</p><p>早期版本的Borgmaster设计和实现人员有：Jeremy Dion和Mark Vandevoorde，以及Ben Smith, Ken Ashcraft, Maricia Scott, Ming-Yee Iu 和 Monika Henzinger。早期版本的Borglet主要是由Paul Menage设计和实现的（ying：见[62]）。</p><p>后续的参与者包括：Abhishek Rai, Abhishek Verma, Andy Zheng, Ashwin Kumar, Beng-Hong Lim, Bin Zhang, Bolu Szewczyk, Brian Budge, Brian Grant, Brian Wickman, Chengdu Huang, Cynthia Wong, Daniel Smith, Dave Bort, David Oppenheimer, David Wall, Dawn Chen, Eric Haugen, Eric Tune, Ethan Solomita, Gaurav Dhiman, Geeta Chaudhry, Greg Roelofs, Grzegorz Czajkowski, James Eady, Jarek Kusmierek, Jaroslaw Przybylowicz, Jason Hickey, Javier Kohen, Jeremy Lau, Jerzy Szczepkowski, John Wilkes, Jonathan Wilson, Joso Eterovic, Jutta Degener, Kai Backman, Kamil Yurtsever, Kenji Kaneda, Kevan Miller, Kurt Steinkraus, Leo Landa, Liza Fireman, Madhukar Korupolu, Mark Logan, Markus Gutschke, Matt Sparks, Maya Haridasan, Michael Abd-El-Malek, Michael Kenniston, Mukesh Kumar, Nate Calvin, Onufry Wojtaszczyk, Patrick Johnson, Pedro Valenzuela, Piotr Witusowski, Praveen Kallakuri, Rafal Sokolowski, Richard Gooch, Rishi Gosalia, Rob Radez, Robert Hagmann, Robert Jardine, Robert Kennedy, Rohit Jnagal, Roy Bryant, Rune Dahl, Scott Garriss, Scott Johnson, Sean Howarth, Sheena Madan, Smeeta Jalan, Stan Chesnutt, Temo Arobelidze, Tim Hockin, Todd Wang, Tomasz Blaszczyk, Tomasz Wozniak, Tomek Zielonka, Victor Marmol, Vish Kannan, Vrigo Gokhale, Walfredo Cirne, Walt Drummond, Weiran Liu, Xiaopan Zhang, Xiao Zhang, Ye Zhao, Zohaib Maya.</p><p>Borg SRE团队也是非常重要的，包括：Adam Rogoyski, Alex Milivojevic, Anil Das, Cody Smith, Cooper Bethea, Folke Behrens, Matt Liggett, James Sanford, John Millikin, Matt Brown, Miki Habryn, Peter Dahl, Robert van Gent, Seppi Wilhelmi, Seth Hettich, Torsten Marek, 和 Viraj Alankar。Borg配置语言（BCL）和<code>borgcfg</code>工具最初是Marcel van Lohuizen 和 Robert Griesemer开发的。</p><blockquote><p>我们不小心漏掉了 Brad Strand, Chris Colohan, Divyesh Shah, Eric Wilcox, 和 Pavanish Nirula。</p></blockquote><p>谢谢我们的审稿人（尤其是Eric Brewer, Malte Schwarzkopf 和 Tom Rodeheffer），以及我们的导师Christos Kozyrakis，对这篇论文的反馈。</p><h1 id="勘误"><a href="#勘误" class="headerlink" title="勘误"></a>勘误</h1><p>2015-04-23</p><p>定稿后，我们发现了若干无意的疏漏和歧义。（ying：译文已将勘误内容放置到对应章节。补充的两条参考文献序号有冲突，放在了列表之后，并继续编号为85，86）</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] O. A. Abdul-Rahman and K. Aida. <strong>Towards understanding the usage behavior of Google cloud users: the mice and elephants phenomenon.</strong>In Proc. IEEE Int’l Conf. on Cloud Computing Technology and Science (CloudCom), pages 272–277, Singapore, Dec. 2014.<br>[2] Adaptive Computing Enterprises Inc., Provo, UT. <strong>Maui Scheduler Administrator’s Guide</strong>, 3.2 edition, 2011.<br>[3] T. Akidau, A. Balikov, K. Bekiroğlu, S. Chernyak, J. Haberman, R. Lax, S. McVeety, D. Mills, P. Nordstrom,and S. Whittle. <strong>MillWheel: fault-tolerant stream processing at internet scale</strong> In Proc. Int’l Conf. on Very Large Data Bases (VLDB), pages 734–746, Riva del Garda, Italy, Aug.2013.<br>[4] Y. Amir, B. Awerbuch, A. Barak, R. S. Borgstrom, and A. Keren. <strong>An opportunity cost approach for job assignment in a scalable computing cluster</strong> IEEE Trans. Parallel Distrib.Syst., 11(7):760–768, July 2000.<br>[5] <strong>Apache Aurora</strong>. <a href="http://aurora.incubator.apache.org/" target="_blank" rel="external">http://aurora.incubator.apache.org/</a>, 2014.<br>[6] <strong>Aurora Configuration Tutorial</strong>. <a href="https://aurora.incubator.apache.org/documentation/latest/configuration-tutorial/" target="_blank" rel="external">https://aurora.incubator.apache.org/documentation/latest/configuration-tutorial/</a>, 2014.<br>[7] AWS. <strong>Amazon Web Services VM Instances</strong>. <a href="http://aws.amazon.com/ec2/instance-types/" target="_blank" rel="external">http://aws.amazon.com/ec2/instance-types/</a>, 2014.<br>[8] J. Baker, C. Bond, J. Corbett, J. Furman, A. Khorlin, J. Larson, J.-M. Leon, Y. Li, A. Lloyd, and V. Yushprakh. <strong>Megastore: Providing scalable, highly available storage for interactive services</strong> In Proc. Conference on Innovative Data Systems Research (CIDR), pages 223–234, Asilomar, CA, USA, Jan. 2011.<br>[9] M. Baker and J. Ousterhout. <strong>Availability in the Sprite distributed file system</strong> Operating Systems Review,25(2):95–98, Apr. 1991.<br>[10] L. A. Barroso, J. Clidaras, and U. Hölzle. <strong>The datacenter as a computer: an introduction to the design of warehouse-scale machines</strong> Morgan Claypool Publishers, 2nd edition, 2013.<br>[11] L. A. Barroso, J. Dean, and U. Holzle. <strong>Web search for a planet: the Google cluster architecture</strong> In IEEE Micro, pages 22–28, 2003.<br>[12] I. Bokharouss. <strong>GCL Viewer: a study in improving the understanding of GCL programs</strong> Technical report, Eindhoven Univ. of Technology, 2008. MS thesis.<br>[13] E. Boutin, J. Ekanayake, W. Lin, B. Shi, J. Zhou, Z. Qian, M. Wu, and L. Zhou. <strong>Apollo: scalable and coordinated scheduling for cloud-scale computing</strong> In Proc. USENIX Symp. on Operating Systems Design and Implementation (OSDI), Oct. 2014.<br>[14] M. Burrows. <strong>The Chubby lock service for loosely-coupled distributed systems</strong> In Proc. USENIX Symp. on Operating Systems Design and Implementation (OSDI), pages 335–350,Seattle, WA, USA, 2006.<br>[15] <strong>cAdvisor</strong>. <a href="https://github.com/google/cadvisor" target="_blank" rel="external">https://github.com/google/cadvisor</a>, 2014<br>[16] <strong>CFS per-entity load patches</strong>. <a href="http://lwn.net/Articles/531853" target="_blank" rel="external">http://lwn.net/Articles/531853</a>, 2013.<br>[17] <strong>cgroups</strong>. <a href="http://en.wikipedia.org/wiki/Cgroups" target="_blank" rel="external">http://en.wikipedia.org/wiki/Cgroups</a>, 2014.<br>[18] C. Chambers, A. Raniwala, F. Perry, S. Adams, R. R. Henry, R. Bradshaw, and N. Weizenbaum. <strong>FlumeJava: easy, efficient data-parallel pipelines</strong> In Proc. ACM SIGPLAN Conf. on Programming Language Design and Implementation (PLDI), pages 363–375, Toronto, Ontario, Canada, 2010.<br>[19] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber. <strong>Bigtable: a distributed storage system for structured data</strong> ACM Trans. on Computer Systems, 26(2):4:1–4:26, June 2008.<br>[20] Y. Chen, S. Alspaugh, and R. H. Katz. <strong>Design insights for MapReduce from diverse production workloads</strong> Technical Report UCB/EECS–2012–17, UC Berkeley, Jan. 2012.<br>[21] C. Curino, D. E. Difallah, C. Douglas, S. Krishnan, R. Ramakrishnan, and S. Rao. <strong>Reservation-based scheduling: if you’re late don’t blame us!</strong>In Proc. ACM Symp. on Cloud Computing (SoCC), pages 2:1–2:14, Seattle, WA, USA, 2014.<br>[22] J. Dean and L. A. Barroso. <strong>The tail at scale</strong> Communications of the ACM, 56(2):74–80, Feb. 2012.<br>[23] J. Dean and S. Ghemawat. <strong>MapReduce: simplified data processing on large clusters</strong> Communications of the ACM, 51(1):107–113, 2008.<br>[24] C. Delimitrou and C. Kozyrakis. <strong>Paragon: QoS-aware scheduling for heterogeneous datacenters</strong> In Proc. Int’l Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), Mar. 201.<br>[25] C. Delimitrou and C. Kozyrakis. <strong>Quasar: resource-efficient and QoS-aware cluster management</strong> In Proc. Int’l Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pages 127–144, Salt Lake City, UT, USA, 2014.<br>[26] S. Di, D. Kondo, and W. Cirne. <strong>Characterization and comparison of cloud versus Grid workloads</strong> In International Conference on Cluster Computing (IEEE CLUSTER), pages 230–238, Beijing, China, Sept. 2012.<br>[27] S. Di, D. Kondo, and C. Franck. <strong>Characterizing cloud applications on a Google data center</strong> In Proc. Int’l Conf. on Parallel Processing (ICPP), Lyon, France, Oct. 2013.<br>[28] <strong>Docker Project</strong>. <a href="https://www.docker.io/" target="_blank" rel="external">https://www.docker.io/</a>, 2014.<br>[29] D. Dolev, D. G. Feitelson, J. Y. Halpern, R. Kupferman, and N. Linial. <strong>No justified complaints: on fair sharing of multiple resources</strong> In Proc. Innovations in Theoretical Computer Science (ITCS), pages 68–75, Cambridge, MA, USA, 2012.<br>[30] <strong>ElasticSearch</strong>. <a href="http://www.elasticsearch.org" target="_blank" rel="external">http://www.elasticsearch.org</a>, 2014.<br>[31] D. G. Feitelson. <strong>Workload Modeling for Computer Systems Performance Evaluation</strong> Cambridge University Press, 2014.<br>[32] <strong>Fluentd</strong>. <a href="http://www.fluentd.org/" target="_blank" rel="external">http://www.fluentd.org/</a>, 2014.<br>[33] <strong>GCE. Google Compute Engine</strong>. <a href="http://cloud.google.com/products/compute-engine/" target="_blank" rel="external">http://cloud.google.com/products/compute-engine/</a>, 2014.<br>[34] S. Ghemawat, H. Gobioff, and S.-T. Leung. <strong>The Google File System</strong> In Proc. ACM Symp. on Operating Systems Principles (SOSP), pages 29–43, Bolton Landing, NY, USA, 2003. ACM.<br>[35] A. Ghodsi, M. Zaharia, B. Hindman, A. Konwinski, S. Shenker, and I. Stoica. <strong>Dominant Resource Fairness: fair allocation of multiple resource types</strong> In Proc. USENIX Symp. on Networked Systems Design and Implementation (NSDI), pages 323–326, 2011.<br>[36] A. Ghodsi, M. Zaharia, S. Shenker, and I. Stoica. <strong>Choosy: max-min fair sharing for datacenter jobs with constraints</strong> In Proc. European Conf. on Computer Systems (EuroSys), pages 365–378, Prague, Czech Republic, 2013.<br>[37] D. Gmach, J. Rolia, and L. Cherkasova. <strong>Selling T-shirts and time shares in the cloud</strong> In Proc. IEEE/ACM Int’l Symp. on Cluster, Cloud and Grid Computing (CCGrid), pages 539–546, Ottawa, Canada, 2012.<br>[38] <strong>Google App Engine</strong>. <a href="http://cloud.google.com/AppEngine" target="_blank" rel="external">http://cloud.google.com/AppEngine</a>, 2014.<br>[39] <strong>Google Container Engine (GKE)</strong>. <a href="https://cloud.google.com/container-engine/" target="_blank" rel="external">https://cloud.google.com/container-engine/</a>, 2015.<br>[40] R. Grandl, G. Ananthanarayanan, S. Kandula, S. Rao, and A. Akella. <strong>Multi-resource packing for cluster schedulers</strong> In Proc. ACM SIGCOMM, Aug. 2014.<br>[41] <strong>Apache Hadoop Project</strong>. <a href="http://hadoop.apache.org/" target="_blank" rel="external">http://hadoop.apache.org/</a>, 2009.<br>[42] <strong>Hadoop MapReduce Next Generation – Capacity Scheduler</strong>. <a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html</a>, 2013.<br>[43] J. Hamilton. <strong>On designing and deploying internet-scale services</strong>. In Proc. Large Installation System Administration Conf. (LISA), pages 231–242, Dallas, TX, USA, Nov. 2007.<br>[44] P. Helland. <strong>Cosmos: big data and big challenges</strong>. <a href="http://research.microsoft.com/en-us/events/fs2011/helland_cosmos_big_data_and_big_challenges.pdf" target="_blank" rel="external">http://research.microsoft.com/en-us/events/fs2011/ helland_cosmos_big_data_and_big_challenges.pdf</a>, 2011.<br>[45] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. Joseph, R. Katz, S. Shenker, and I. Stoica. <strong>Mesos: a platform for fine-grained resource sharing in the data center</strong>. In Proc. USENIX Symp. on Networked Systems Design and Implementation (NSDI), 2011.<br>[46] <strong>IBM Platform Computing</strong>. <a href="http://www-03.ibm.com/systems/technicalcomputing/platformcomputing/products/clustermanager/index.html" target="_blank" rel="external">http://www-03.ibm.com/systems/technicalcomputing/platformcomputing/products/clustermanager/ index.html</a>.<br>[47] S. Iqbal, R. Gupta, and Y.-C. Fang. <strong>Planning considerations for job scheduling in HPC clusters</strong> Dell Power Solutions, Feb. 2005.<br>[48] M. Isaard. <strong>Autopilot: Automatic data center management</strong> ACM SIGOPS Operating Systems Review, 41(2), 2007.<br>[49] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar, and A. Goldberg. <strong>Quincy: fair scheduling for distributed computing clusters</strong> In Proc. ACM Symp. on Operating Systems Principles (SOSP), 2009.<br>[50] D. B. Jackson, Q. Snell, and M. J. <strong>Clement. Core algorithms of the Maui scheduler</strong> In Proc. Int’l Workshop on Job Scheduling Strategies for Parallel Processing, pages 87–102. Springer-Verlag, 2001.<br>[51] M. Kambadur, T. Moseley, R. Hank, and M. A. Kim. <strong>Measuring interference between live datacenter applications</strong> In Proc. Int’l Conf. for High Performance Computing, Networking, Storage and Analysis (SC), Salt Lake City, UT, Nov. 2012.<br>[52] S. Kavulya, J. Tan, R. Gandhi, and P. Narasimhan. <strong>An analysis of traces from a production MapReduce cluster</strong> In Proc. IEEE/ACM Int’l Symp. on Cluster, Cloud and Grid Computing (CCGrid), pages 94–103, 2010.<br>[53] <strong>Kubernetes</strong>. <a href="http://kubernetes.io" target="_blank" rel="external">http://kubernetes.io</a>, Aug. 2014.<br>[54] <strong>Kernel Based Virtual Machine</strong>. <a href="http://www.linux-kvm.org" target="_blank" rel="external">http://www.linux-kvm.org</a>.<br>[55] L. Lamport. <strong>The part-time parliament</strong> ACM Trans. on Computer Systems, 16(2):133–169, May 1998.<br>[56] J. Leverich and C. Kozyrakis. <strong>Reconciling high server utilization and sub-millisecond quality-of-service</strong> In Proc. European Conf. on Computer Systems (EuroSys), page 4, 2014.<br>[57] Z. Liu and S. Cho. <strong>Characterizing machines and workloads on a Google cluster</strong> In Proc. Int’l Workshop on Scheduling and Resource Management for Parallel and Distributed Systems (SRMPDS), Pittsburgh, PA, USA, Sept. 2012.<br>[58] <strong>Google LMCTFY project (let me contain that for you)</strong>. <a href="http://github.com/google/lmctfy" target="_blank" rel="external">http://github.com/google/lmctfy</a>, 2014.<br>[59] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. <strong>Pregel: a system for large-scale graph processing</strong> In Proc. ACM SIGMOD Conference, pages 135–146, Indianapolis, IA, USA, 2010.<br>[60] J. Mars, L. Tang, R. Hundt, K. Skadron, and M. L. Soffa. <strong>Bubble-Up: increasing utilization in modern warehouse scale computers via sensible co-locations</strong> In Proc. Int’l Symp. on Microarchitecture (Micro), Porto Alegre, Brazil, 2011.<br>[61] S. Melnik, A. Gubarev, J. J. Long, G. Romer, S. Shivakumar, M. Tolton, and T. Vassilakis. <strong>Dremel: interactive analysis of web-scale datasets</strong> In Proc. Int’l Conf. on Very Large Data Bases (VLDB), pages 330–339, Singapore, Sept. 2010.<br>[62] P. Menage. <strong>Linux control groups</strong>. <a href="http://www.kernel.org/doc/Documentation/cgroups/cgroups.txt" target="_blank" rel="external">http://www.kernel.org/doc/Documentation/cgroups/cgroups.txt</a>, 2007–2014.<br>[63] A. K. Mishra, J. L. Hellerstein, W. Cirne, and C. R. Das. <strong>Towards characterizing cloud backend workloads: insights from Google compute clusters</strong> ACM SIGMETRICS Performance Evaluation Review, 37:34–41, Mar. 2010.<br>[64] A. Narayanan. <strong>Tupperware: containerized deployment at Facebook</strong>. <a href="http://www.slideshare.net/dotCloud/tupperware-containerized-deployment-at-facebook" target="_blank" rel="external">http://www.slideshare.net/dotCloud/tupperware-containerized-deployment-at-facebook</a>, June 2014.<br>[65] K. Ousterhout, P. Wendell, M. Zaharia, and I. Stoica. <strong>Sparrow: distributed, low latency scheduling</strong> In Proc. ACM Symp. on Operating Systems Principles (SOSP), pages 69–84, Farminton, PA, USA, 2013.<br>[66] D. C. Parkes, A. D. Procaccia, and N. Shah. <strong>Beyond Dominant Resource Fairness: extensions, limitations, and indivisibilities</strong> In Proc. Electronic Commerce, pages 808–825, Valencia, Spain, 2012.<br>[67] <strong>Protocol buffers</strong>. <a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="external">https://developers.google.com/protocol-buffers/</a>, and <a href="https://github.com/google/protobuf/" target="_blank" rel="external">https://github.com/google/protobuf/</a>, 2014.<br>[68] C. Reiss, A. Tumanov, G. Ganger, R. Katz, and M. Kozuch. <strong>Heterogeneity and dynamicity of clouds at scale: Google trace analysis</strong> In Proc. ACM Symp. on Cloud Computing (SoCC), San Jose, CA, USA, Oct. 2012.<br>[69] M. Schwarzkopf, A. Konwinski, M. Abd-El-Malek, and J. Wilkes. <strong>Omega: flexible, scalable schedulers for large compute clusters</strong> In Proc. European Conf. on Computer Systems (EuroSys), Prague, Czech Republic, 2013.<br>[70] B. Sharma, V. Chudnovsky, J. L. Hellerstein, R. Rifaat, and C. R. Das. <strong>Modeling and synthesizing task placement constraints in Google compute clusters</strong> In Proc. ACM Symp. on Cloud Computing (SoCC), pages 3:1–3:14, Cascais, Portugal, Oct. 2011.<br>[71] E. Shmueli and D. G. Feitelson. <strong>On simulation and design of parallel-systems schedulers: are we doing the right thing?</strong> IEEE Trans. on Parallel and Distributed Systems, 20(7):983–996, July 2009.<br>[72] A. Singh, M. Korupolu, and D. Mohapatra. <strong>Server-storage virtualization: integration and load balancing in data centers</strong> In Proc. Int’l Conf. for High Performance Computing, Networking, Storage and Analysis (SC), pages 53:1–53:12, Austin, TX, USA, 2008.<br>[73] <strong>Apache Spark Project</strong>. <a href="http://spark.apache.org/" target="_blank" rel="external">http://spark.apache.org/</a>, 2014.<br>[74] A. Tumanov, J. Cipar, M. A. Kozuch, and G. R. Ganger. <strong>Alsched: algebraic scheduling of mixed workloads in heterogeneous clouds</strong> In Proc. ACM Symp. on Cloud Computing (SoCC), San Jose, CA, USA, Oct. 2012.<br>[75] P. Turner, B. Rao, and N. Rao. <strong>CPU bandwidth control for CFS</strong> In Proc. Linux Symposium, pages 245–254, July 2010.<br>[76] V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, B. Saha, C. Curino, O. O’Malley, S. Radia, B. Reed, and E. Baldeschwieler. <strong>Apache Hadoop YARN: Yet Another Resource Negotiator</strong> In Proc. ACM Symp. on Cloud Computing (SoCC), Santa Clara, CA, USA, 2013.<br>[77] <strong>VMware VCloud Suite</strong>. <a href="http://www.vmware.com/products/vcloud-suite/" target="_blank" rel="external">http://www.vmware.com/products/vcloud-suite/</a>.<br>[78] A. Verma, M. Korupolu, and J. Wilkes. <strong>Evaluating job packing in warehouse-scale computing</strong> In IEEE Cluster, pages 48–56, Madrid, Spain, Sept. 2014.<br>[79] W. Whitt. <strong>Open and closed models for networks of queues</strong> AT&amp;T Bell Labs Technical Journal, 63(9), Nov. 1984.<br>[80] J. Wilkes. <strong>More Google cluster data</strong>. <a href="http://googleresearch.blogspot.com/2011/11/more-google-cluster-data.html" target="_blank" rel="external">http://googleresearch.blogspot.com/2011/11/more-google-cluster-data.html</a>, Nov. 2011.<br>[81] Y. Zhai, X. Zhang, S. Eranian, L. Tang, and J. Mars. <strong>HaPPy: Hyperthread-aware power profiling dynamically</strong> In Proc. USENIX Annual Technical Conf. (USENIX ATC), pages 211–217, Philadelphia, PA, USA, June 2014. USENIX Association.<br>[82] Q. Zhang, J. Hellerstein, and R. Boutaba. <strong>Characterizing task usage shapes in Google’s compute clusters</strong> In Proc. Int’l Workshop on Large-Scale Distributed Systems and Middleware (LADIS), 2011.<br>[83] X. Zhang, E. Tune, R. Hagmann, R. Jnagal, V. Gokhale, and J. Wilkes. <strong>CPI2: CPU performance isolation for shared compute clusters</strong> In Proc. European Conf. on Computer Systems (EuroSys), Prague, Czech Republic, 2013.<br>[84] Z. Zhang, C. Li, Y. Tao, R. Yang, H. Tang, and J. Xu. <strong>Fuxi: a fault-tolerant resource management and job scheduling system at internet scale</strong> In Proc. Int’l Conf. on Very Large Data Bases (VLDB), pages 1393–1404. VLDB Endowment Inc., Sept. 2014.<br>[85] Michael Litzkow, Miron Livny, and Matt Mutka. <strong>Condor - A Hunter of Idle Workstations</strong> In Proc. Int’l Conf. on Distributed Computing Systems (ICDCS) , pages 104-111, June 1988.<br>[86] Rajesh Raman, Miron Livny, and Marvin Solomon. <strong>Matchmaking: Distributed Resource Management for High Throughput Computing</strong> In Proc. Int’l Symp. on High Performance Distributed Computing (HPDC) , Chicago, IL, USA, July 1998.    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;发表于EuroSys 2015的 &lt;strong&gt;&lt;em&gt;Large-scale cluster management at Google with Borg&lt;/em&gt;&lt;/strong&gt; 详细介绍了Google的Borg资源管理器。已经有网友“难易（HardySimpson）” 翻译了此文，这里对其稍作修订。之前读过两三遍此文，每遍都感觉有新的体会，这次修订就是为了更仔细地读一遍。&lt;/p&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>【译文】Docker镜像格式规范，v1.2</title>
    <link href="https://ying-zhang.github.io/cloud/2017/docker-image-spec-v1.2/"/>
    <id>https://ying-zhang.github.io/cloud/2017/docker-image-spec-v1.2/</id>
    <published>2017-05-15T16:00:00.000Z</published>
    <updated>2017-10-30T03:41:07.105Z</updated>
    
    <content type="html"><![CDATA[<p>原文见 <a href="https://github.com/moby/moby/blob/master/image/spec/v1.2.md" target="_blank" rel="external">https://github.com/moby/moby/blob/master/image/spec/v1.2.md</a><br>Docker已经迁移到Moby项目了。</p><a id="more"></a><!-- TOC --><pre><code>- [title: 【译文】Docker镜像格式规范，v1.2](#title-%E3%80%90%E8%AF%91%E6%96%87%E3%80%91docker%E9%95%9C%E5%83%8F%E6%A0%BC%E5%BC%8F%E8%A7%84%E8%8C%83%EF%BC%8Cv12)</code></pre><ul><li><a href="#docker%E9%95%9C%E5%83%8F%E8%A7%84%E8%8C%83v120">Docker镜像规范v1.2.0</a><ul><li><a href="#%E6%9C%AF%E8%AF%AD">术语</a><ul><li><a href="#%E5%B1%82%EF%BC%88layer%EF%BC%89">层（Layer）</a></li><li><a href="#%E9%95%9C%E5%83%8F%E7%9A%84json%E6%8F%8F%E8%BF%B0%E6%96%87%E4%BB%B6">镜像的JSON描述文件</a></li><li><a href="#%E9%95%9C%E5%83%8F%E6%96%87%E4%BB%B6%E5%8F%98%E6%9B%B4%E9%9B%86%EF%BC%88changeset%EF%BC%89">镜像文件变更集（changeset）</a></li><li><a href="#%E5%B1%82%E7%9A%84diffid">层的DiffID</a></li><li><a href="#%E5%B1%82%E7%9A%84chainid">层的ChainID</a></li><li><a href="#%E9%95%9C%E5%83%8F%E7%9A%84imageid">镜像的ImageID</a></li><li><a href="#%E6%A0%87%E7%AD%BE%EF%BC%88tag%EF%BC%89">标签（Tag）</a></li><li><a href="#%E9%95%9C%E5%83%8F%E5%90%8D%EF%BC%88repository%EF%BC%89">镜像名（Repository）</a></li></ul></li><li><a href="#%E9%95%9C%E5%83%8F%E7%9A%84json%E6%8F%8F%E8%BF%B0%E6%96%87%E4%BB%B6%E7%A4%BA%E4%BE%8B">镜像的JSON描述文件示例</a></li><li><a href="#%E9%95%9C%E5%83%8F%E7%9A%84json%E6%8F%8F%E8%BF%B0%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E">镜像的JSON描述文件说明</a><ul><li><a href="#created">created</a></li><li><a href="#author">author</a></li><li><a href="#architecture">architecture</a></li><li><a href="#os">os</a></li><li><a href="#config">config</a><ul><li><a href="#user">User</a></li><li><a href="#memory">Memory</a></li><li><a href="#memoryswap">MemorySwap</a></li><li><a href="#cpushares">CpuShares</a></li><li><a href="#exposedports">ExposedPorts</a></li><li><a href="#env">Env</a></li><li><a href="#entrypoint">Entrypoint</a></li><li><a href="#cmd">Cmd</a></li><li><a href="#healthcheck">Healthcheck</a><ul><li><a href="#test">Test</a></li><li><a href="#volumes">Volumes</a></li><li><a href="#workingdir">WorkingDir</a></li><li><a href="#rootfs">rootfs</a></li></ul></li></ul></li><li><a href="#history">history</a></li></ul></li><li><a href="#%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F%E6%96%87%E4%BB%B6%E5%8F%98%E6%9B%B4%E9%9B%86">创建镜像文件变更集</a></li><li><a href="#%E9%95%9C%E5%83%8F%E7%9A%84%E7%BB%84%E5%90%88%E6%A0%BC%E5%BC%8F">镜像的组合格式</a></li></ul></li><li><a href="#todo">TODO</a></li><li><a href="#%E8%A1%A5%E5%85%85">补充</a><ul><li><a href="#alpine%E9%95%9C%E5%83%8F%E7%9A%84%E4%B8%BB%E6%9C%BA%E5%AD%98%E5%82%A8%E5%B8%83%E5%B1%80">alpine镜像的主机存储布局</a></li></ul></li></ul><!-- /TOC --><h1 id="Docker镜像规范v1-2-0"><a href="#Docker镜像规范v1-2-0" class="headerlink" title="Docker镜像规范v1.2.0"></a>Docker镜像规范v1.2.0</h1><p><strong>镜像（Image）</strong>是在基础文件集（root filesystem）之上依次变更的集合，及在容器运行的默认执行参数。本规范概述这些文件变更及执行参数的格式，创建和使用它们的方法。<br>此版本的镜像规范自Docker 1.12开始采用。</p><blockquote><p>译注：本规范中的 <strong>filesystem</strong> 并非通常意义的文件系统，实际上只是一组文件及文件夹的集合，故译为 <strong>文件集</strong>。</p></blockquote><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>本规范使用以下术语:</p><h3 id="层（Layer）"><a href="#层（Layer）" class="headerlink" title="层（Layer）"></a>层（Layer）</h3><p>镜像由 <strong>层</strong> 组成。 每一层都是若干文件的变更集合。层不包含环境变量或默认参数等元数据。这些元数据是镜像整体的属性，而不是特定层的。</p><h3 id="镜像的JSON描述文件"><a href="#镜像的JSON描述文件" class="headerlink" title="镜像的JSON描述文件"></a>镜像的JSON描述文件</h3><p>整个镜像有一个JSON描述文件，它包含镜像的基本信息，如创建的日期、作者、父镜像的ID、以及启动/运行的配置（如入口命令、默认参数、CPU / 内存份额、网络和数据卷等）。JSON文件还列出了组成镜像的每个层的加密散列及其命令历史。<br>该JSON文件是不可变的，更改它会导致重新计算整个镜像的ImageID（译注：见下面ImageID的计算方法小节），这意味着派生出一个新的镜像，而不是改变现有的镜像。</p><h3 id="镜像文件变更集（changeset）"><a href="#镜像文件变更集（changeset）" class="headerlink" title="镜像文件变更集（changeset）"></a>镜像文件变更集（changeset）</h3><p>每个层都是一组在其父层之上增加、修改或删除文件的归档。使用分层或联合文件系统（如AUFS），或通过从文件系统快照计算差异（Diff），可以将一系列的层（即文件变更集）合并成一个虚拟的单层文件集合（one cohesive filesystem）。</p><h3 id="层的DiffID"><a href="#层的DiffID" class="headerlink" title="层的DiffID"></a>层的DiffID</h3><p>将一个层的所有文件内容序列化后，计算出一个加密散列来作为该层的标识。具体是将层打包为一个<code>tar</code>包，然后计算其SHA256摘要，用十六进制编码表示长度为256比特的串（共64个字符），<br>如<code>sha256:a9561eb1b190625c9adb5a9513e72c4dedafc1cb2d4c5236c9a6957ec7dfd5a9</code>。<br>层的打包和解包必须是可重复的，以免更改层的ID，例如，应使用<code>tar-split</code>来保存tar包的header。注意，层的ID是基于未压缩的tar包计算的。</p><blockquote><p>译注：关于层的打包和解包的可重复性，<code>tar</code>程序将一组文件打包的 <strong>顺序</strong> 是与文件系统相关的，此处没有详细说明可重复性的实现方式，可参考Docker源码深入了解。<br>参考：<a href="https://unix.stackexchange.com/questions/120143/how-is-the-order-in-which-tar-works-on-files-determined" target="_blank" rel="external">tar打包的顺序</a>。</p></blockquote><h3 id="层的ChainID"><a href="#层的ChainID" class="headerlink" title="层的ChainID"></a>层的ChainID</h3><p>为方便起见，可以给一串有序的层计算出一个ID，称为 <strong>ChainID</strong>。<br>仅有一个层时，其ChainID与该层的DiffID相同。多个层时，其ChainID由下面的递归公式给出：</p><p>$$ChainID(layerN)=SHA256hex(ChainID(layer(N-1))+ “ \quad” +DiffID(layerN))$$ </p><h3 id="镜像的ImageID"><a href="#镜像的ImageID" class="headerlink" title="镜像的ImageID"></a>镜像的ImageID</h3><p>每个镜像的ID是其JSON描述文件的SHA256散列值，用十六进制编码表示，<br>如<code>sha256:a9561eb1b190625c9adb5a9513e72c4dedafc1cb2d4c5236c9a6957ec7dfd5a9</code>。<br>由于JSON文件包含镜像所有层的散列ID，据此计算出的ImageID，使得可以对镜像的即各层按内容寻址（Content Addressable，地址即各层的DiffID）。</p><h3 id="标签（Tag）"><a href="#标签（Tag）" class="headerlink" title="标签（Tag）"></a>标签（Tag）</h3><p>Tag是用户为ImageID指定的说明文字。Tag中的字符只能是大小写英文字母、数字、短线、下划线和点，即<code>[a-zA-Z0-9_.-]</code>，首个字符不能是<code>.</code>或<code>-</code>。Tag不能超过127个字符。</p><h3 id="镜像名（Repository）"><a href="#镜像名（Repository）" class="headerlink" title="镜像名（Repository）"></a>镜像名（Repository）</h3><p>这里的<code>Repository</code>是指镜像全名在冒号<code>:</code>之前的部分，冒号<code>:</code>之后的部分是镜像的标签（tag），用来区分镜像的版本。 如名为<code>my-app:3.1.4</code>的镜像，<code>my-app</code>就是镜像的 Repository 部分。<br>Repository又可以用斜杠<code>/</code>分隔开，<code>/</code>之前的部分是可选的DNS格式的主机名。主机名必须符合DNS规则，但 <strong>不得</strong> 包含下划线<code>_</code>字符，主机名可以有如<code>：8080</code>格式的端口号。<br>镜像名可以包含小写字符，数字和分隔符。 分隔符是句点<code>.</code>，一个或两个下划线<code>_</code>，或一个或多个短横线<code>-</code>，镜像名 <strong>不允许</strong> 以分隔符开头或结尾。</p><blockquote><p>译注：</p><ul><li>这里的 Repository 容易与git的 <strong>代码仓库</strong> 概念混淆。</li><li>DNS名和主机名的格式稍有不同，一般来说，主机名不允许使用下划线<code>_</code>，参考<a href="https://tools.ietf.org/html/rfc1123" target="_blank" rel="external">RFC 1123</a></li></ul></blockquote><h2 id="镜像的JSON描述文件示例"><a href="#镜像的JSON描述文件示例" class="headerlink" title="镜像的JSON描述文件示例"></a>镜像的JSON描述文件示例</h2><p>下面是一个镜像的JSON描述文件示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;architecture&quot;: &quot;amd64&quot;,</div><div class="line">  &quot;author&quot;: &quot;Alyssa P. Hacker &amp;ltalyspdev@example.com&amp;gt&quot;,</div><div class="line">  &quot;config&quot;: &#123;</div><div class="line">    &quot;Cmd&quot;: [</div><div class="line">      &quot;--foreground&quot;,</div><div class="line">      &quot;--config&quot;,</div><div class="line">      &quot;/etc/my-app.d/default.cfg&quot;</div><div class="line">    ],</div><div class="line">    &quot;CpuShares&quot;: 8,</div><div class="line">    &quot;Entrypoint&quot;: [</div><div class="line">      &quot;/bin/my-app-binary&quot;</div><div class="line">    ],</div><div class="line">    &quot;Env&quot;: [</div><div class="line">      &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,</div><div class="line">      &quot;FOO=docker_is_a_really&quot;,</div><div class="line">      &quot;BAR=great_tool_you_know&quot;</div><div class="line">    ],</div><div class="line">    &quot;ExposedPorts&quot;: &#123;</div><div class="line">      &quot;8080/tcp&quot;: &#123;&#125;</div><div class="line">    &#125;,</div><div class="line">    &quot;Memory&quot;: 2048,</div><div class="line">    &quot;MemorySwap&quot;: 4096,</div><div class="line">    &quot;User&quot;: &quot;alice&quot;,</div><div class="line">    &quot;Volumes&quot;: &#123;</div><div class="line">      &quot;/var/job-result-data&quot;: &#123;&#125;,</div><div class="line">      &quot;/var/log/my-app-logs&quot;: &#123;&#125;</div><div class="line">    &#125;,</div><div class="line">    &quot;WorkingDir&quot;: &quot;/home/alice&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;created&quot;: &quot;2015-10-31T22:22:56.015925234Z&quot;,</div><div class="line">  &quot;history&quot;: [</div><div class="line">    &#123;</div><div class="line">      &quot;created&quot;: &quot;2015-10-31T22:22:54.690851953Z&quot;,</div><div class="line">      &quot;created_by&quot;: &quot;/bin/sh -c #(nop) ADD file:a3bc1e842b69636f9df5256c49c5374fb4eef1e281fe3f282c65fb853ee171c5 in /&quot;</div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      &quot;created&quot;: &quot;2015-10-31T22:22:55.613815829Z&quot;,</div><div class="line">      &quot;created_by&quot;: &quot;/bin/sh -c #(nop) CMD [\&quot;sh\&quot;]&quot;,</div><div class="line">      &quot;empty_layer&quot;: true</div><div class="line">    &#125;</div><div class="line">  ],</div><div class="line">  &quot;os&quot;: &quot;linux&quot;,</div><div class="line">  &quot;rootfs&quot;: &#123;</div><div class="line">    &quot;diff_ids&quot;: [</div><div class="line">      &quot;sha256:c6f988f4874bb0add23a778f753c65efe992244e148a1d2ec2a8b664fb66bbd1&quot;,</div><div class="line">      &quot;sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef&quot;</div><div class="line">    ],</div><div class="line">    &quot;type&quot;: &quot;layers&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>注意，Docker生成的镜像JSON描述文件不包含为了格式化而插入的空格，这里是为了方便阅读。</p><h2 id="镜像的JSON描述文件说明"><a href="#镜像的JSON描述文件说明" class="headerlink" title="镜像的JSON描述文件说明"></a>镜像的JSON描述文件说明</h2><h3 id="created"><a href="#created" class="headerlink" title="created"></a>created</h3><p><code>string</code><br>镜像创建的日期和时间，<a href="https://zh.wikipedia.org/wiki/ISO_8601" target="_blank" rel="external">ISO-8601格式</a>。</p><h3 id="author"><a href="#author" class="headerlink" title="author"></a>author</h3><p><code>string</code><br>创建和负责维护改镜像的人员或组织名，或Email。</p><h3 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a>architecture</h3><p><code>string</code><br>镜像中可执行文件的CPU架构，可以是 </p><ul><li><code>386</code></li><li><code>amd64</code></li><li><code>arm</code><br>未来可能会支持更多的架构，有的容器引擎可能不支持某些架构。</li></ul><h3 id="os"><a href="#os" class="headerlink" title="os"></a>os</h3><p><code>string</code><br>镜像运行的操作系统名，可以是 </p><ul><li><code>darwin</code></li><li><code>freebsd</code></li><li><code>linux</code><br>未来可能会支持更多的架构，有的容器引擎可能不支持某些操作系统。</li></ul><h3 id="config"><a href="#config" class="headerlink" title="config"></a>config</h3><p><code>struct</code><br><code>config</code>结构是从镜像创建容器时，使用的基本执行参数。<code>config</code>可以是空值<code>null</code>，则创建容器时必须提供所有必要的执行参数。</p><p><code>config</code>结构的各字段说明</p><h4 id="User"><a href="#User" class="headerlink" title="User"></a>User</h4><p><code>string</code><br>容器中进程执行使用的用户名或UID。如果创建容器时没有在命令行给出，将使用此配置项的值。下面的格式都是有效的：</p><ul><li><code>user</code></li><li><code>uid</code></li><li><code>user:group</code></li><li><code>uid:gid</code></li><li><code>uid:group</code></li><li><code>user:gid</code></li></ul><p>如果没有给出组名 <code>group</code>/<code>gid</code>，默认的组使用容器中<code>/etc/passwd</code>文件对应的<code>user</code>/<code>uid</code>项。</p><h4 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h4><p><code>integer</code><br>内存限值（以 <strong>字节</strong> 为单位）。如果创建容器时没有在命令行给出，则使用此配置项的值。</p><h4 id="MemorySwap"><a href="#MemorySwap" class="headerlink" title="MemorySwap"></a>MemorySwap</h4><p><code>integer</code><br>总的内存使用量（内存 + swap），设置为<code>-1</code>则禁用 swap。如果创建容器时没有在命令行给出，则使用此配置项的值。</p><h4 id="CpuShares"><a href="#CpuShares" class="headerlink" title="CpuShares"></a>CpuShares</h4><p><code>integer</code><br>CPU份额（相对其它容器的权重）。如果创建容器时没有在命令行给出，则使用此配置项的值。</p><h4 id="ExposedPorts"><a href="#ExposedPorts" class="headerlink" title="ExposedPorts"></a>ExposedPorts</h4><p><code>struct</code><br>基于此镜像创建的容器公开的端口列表。此JSON结构的特殊之处在于它是由Go语言的<code>map[string]struct{}</code>结构直接序列化为JSON格式的，形式为每个键对应着 <strong>值为空对象{}</strong> 的JSON对象。如下面的例子所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;8080&quot;: &#123;&#125;,</div><div class="line">    &quot;53/udp&quot;: &#123;&#125;,</div><div class="line">    &quot;2356/tcp&quot;: &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>其中的键可以是下面的格式：</p><ul><li><code>&quot;port/tcp&quot;</code></li><li><code>&quot;port/udp&quot;</code></li><li><code>&quot;port&quot;</code><br>如果没有给出协议，默认使用<code>tcp</code>协议。这是创建容器使用的默认值，可以与命令行提供的端口列表合并。</li></ul><h4 id="Env"><a href="#Env" class="headerlink" title="Env"></a>Env</h4><p><code>array of strings</code><br>Env的每项都是 <code>VARNAME=&quot;var value&quot;</code> 的格式。这是创建容器使用的默认值，可以与命令行提供的环境变量列表合并。</p><h4 id="Entrypoint"><a href="#Entrypoint" class="headerlink" title="Entrypoint"></a>Entrypoint</h4><p><code>array of strings</code><br>容器启动时执行的命令参数列表。这是创建容器使用的默认值，可以被命令行提供的入口命令替换。</p><h4 id="Cmd"><a href="#Cmd" class="headerlink" title="Cmd"></a>Cmd</h4><p><code>array of strings</code><br>容器启动时执行的命令参数列表（附加在<code>Entrypoint</code>之后）。这是创建容器使用的默认值，可以被命令行提供的入口命令替换。如果没有给出<code>Entrypoint</code>，那么<code>Cmd</code>列表的第一项将被认为是可执行的程序名。</p><h4 id="Healthcheck"><a href="#Healthcheck" class="headerlink" title="Healthcheck"></a>Healthcheck</h4><p><code>struct</code><br>用以检查容器是否正常的测试命令，如下面的例子所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;Test&quot;: [</div><div class="line">      &quot;CMD-SHELL&quot;,</div><div class="line">      &quot;/usr/bin/check-health localhost&quot;</div><div class="line">  ],</div><div class="line">  &quot;Interval&quot;: 30000000000,</div><div class="line">  &quot;Timeout&quot;:  10000000000,</div><div class="line">  &quot;Retries&quot;:  3</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>此结构有如下字段，</p><h5 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h5><p><code>array of strings</code><br>用以检查容器是否正常的测试命令，可以是</p><ul><li><code>[]</code> : 继承父镜像的健康检查命令；</li><li><code>[&quot;NONE&quot;]</code> : 禁用健康检查；</li><li><code>[&quot;CMD&quot;, arg1, arg2, ...]</code> : 直接执行命令和参数；</li><li><code>[&quot;CMD-SHELL&quot;, command]</code> : 使用系统默认shell执行命令；</li></ul><p>如果容器状态正常，测试命令退出后应返回 <code>0</code>，否则返回 <code>1</code>。</p><ul><li>Interval <code>integer</code>：相邻两次尝试的间隔，单位为纳秒；</li><li>Timeout <code>integer</code>：认为异常的超时间隔，单位为纳秒；</li><li>Retries <code>integer</code>：认为异常的重试次数。</li></ul><p>任何缺失的值都会从基础镜像继承。这是创建容器使用的默认值，可以与命令行提供的值合并（替代？）。</p><h5 id="Volumes"><a href="#Volumes" class="headerlink" title="Volumes"></a>Volumes</h5><p><code>struct</code><br>创建容器时作为数据卷的一组目录。此JSON结构的特殊之处在于它是由Go语言的<code>map[string]struct{}</code>结构直接序列化为JSON格式的，形式为每个键对应着 <strong>值为空对象{}</strong> 的JSON对象。如下面的例子所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;/var/my-app-data/&quot;: &#123;&#125;,</div><div class="line">    &quot;/etc/some-config.d/&quot;: &#123;&#125;,</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h5 id="WorkingDir"><a href="#WorkingDir" class="headerlink" title="WorkingDir"></a>WorkingDir</h5><p><code>string</code><br>容器入口程序的工作目录，这是创建容器使用的默认值，可以被命令行提供的值替代。</p><h5 id="rootfs"><a href="#rootfs" class="headerlink" title="rootfs"></a>rootfs</h5><p><code>struct</code><br>rootfs结构是镜像各层的<code>DiffID</code>列表，此结构使镜像的描述文件的散列与各层的散列（及内容）相对应。rootfs有两个字段:</p><ul><li><code>type</code>，其值一般为 <code>layers</code>.</li><li><code>diff_ids</code> 各层散列（<code>DiffID</code>）的数组，顺序为从最底层到最顶层。</li></ul><p>下面是 rootfs 的一个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&quot;rootfs&quot;: &#123;</div><div class="line">  &quot;diff_ids&quot;: [</div><div class="line">    &quot;sha256:c6f988f4874bb0add23a778f753c65efe992244e148a1d2ec2a8b664fb66bbd1&quot;,</div><div class="line">    &quot;sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef&quot;,</div><div class="line">    &quot;sha256:13f53e08df5a220ab6d13c58b2bf83a59cbdc2e04d0a3f041ddf4b0ba4112d49&quot;</div><div class="line">  ],</div><div class="line">  &quot;type&quot;: &quot;layers&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="history"><a href="#history" class="headerlink" title="history"></a>history</h3><p><code>struct</code><br><code>history</code>结构是描述每层历史的一组对象，顺序为从最底层到最顶层。每个对象有以下字段。</p><ul><li><code>created</code>: 创建的日期和时间，<a href="https://zh.wikipedia.org/wiki/ISO_8601" target="_blank" rel="external">ISO-8601格式</a>；</li><li><code>author</code>: 创建的作者；</li><li><code>created_by</code>: 创建该层的命令；</li><li><code>comment</code>: 创建该层的注释；</li><li><code>empty_layer</code>: 标识此项历史记录是否会创建一个文件变更集。如果值为<code>true</code>，则此项历史不会对应一个实际的文件集（如<code>ENV</code>命令就对层的文件没有影响）。<br>下面是 history 结构的一个例子：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&quot;history&quot;: [</div><div class="line">  &#123;</div><div class="line">    &quot;created&quot;: &quot;2015-10-31T22:22:54.690851953Z&quot;,</div><div class="line">    &quot;created_by&quot;: &quot;/bin/sh -c #(nop) ADD file:a3bc1e842b69636f9df5256c49c5374fb4eef1e281fe3f282c65fb853ee171c5 in /&quot;</div><div class="line">  &#125;,</div><div class="line">  &#123;</div><div class="line">    &quot;created&quot;: &quot;2015-10-31T22:22:55.613815829Z&quot;,</div><div class="line">    &quot;created_by&quot;: &quot;/bin/sh -c #(nop) CMD [\&quot;sh\&quot;]&quot;,</div><div class="line">    &quot;empty_layer&quot;: true</div><div class="line">  &#125;</div><div class="line">]</div></pre></td></tr></table></figure></li></ul><p>镜像的JSON文件中任何额外的字段应被认为是特定于实现的，如果无法处理，应该将其忽略。</p><h2 id="创建镜像文件变更集"><a href="#创建镜像文件变更集" class="headerlink" title="创建镜像文件变更集"></a>创建镜像文件变更集</h2><p>创建镜像文件变更集的例子如下：<br>首先，镜像的基础文件是一个空的目录，使用了随机生成的目录名<code>c3167915dc9d</code>（层的DiffID是基于目录内的文件内容生成的）。</p><p>然后在其中创建文件和目录:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">c3167915dc9d/</div><div class="line">    etc/</div><div class="line">        my-app-config</div><div class="line">    bin/</div><div class="line">        my-app-binary</div><div class="line">        my-app-tools</div></pre></td></tr></table></figure></p><p>将目录<code>c3167915dc9d</code>提交为一个 <code>tar</code> 包（无压缩），其中包含如下的文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">etc/my-app-config</div><div class="line">bin/my-app-binary</div><div class="line">bin/my-app-tools</div></pre></td></tr></table></figure><p>如果要在此基础上更改文件，则创建一个新的目录，假如为<code>f60c56784b83</code>，将其初始化为父镜像的快照，即与目录<code>c3167915dc9d</code>的内容相同。</p><blockquote><p>注意：支持 Copy-on-Write 或联合机制的文件系统创建快照很高效。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">f60c56784b83/</div><div class="line">    etc/</div><div class="line">        my-app-config</div><div class="line">    bin/</div><div class="line">        my-app-binary</div><div class="line">        my-app-tools</div></pre></td></tr></table></figure><p>然后添加一个配置目录<code>/etc/my-app.d</code>，其中包含默认的配置文件。可执行程序<code>my-app-tools</code>也更新了，以便处理新的配置文件路径。<br>修改后的目录<code>f60c56784b83</code>如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">f60c56784b83/</div><div class="line">    etc/</div><div class="line">        my-app.d/</div><div class="line">            default.cfg</div><div class="line">    bin/</div><div class="line">        my-app-binary</div><div class="line">        my-app-tools</div></pre></td></tr></table></figure></p><p>其中移除了<code>/etc/my-app-config</code>，然后创建了新的目录和文件<code>/etc/my-app.d/default.cfg</code>。<code>/bin/my-app-tools</code>也替换成新的版本。在将此目录提交为变更集之前，首先需要与其父镜像的快照<code>f60c56784b83</code>比较，找出增加、修改及删除的文件和目录。本例中找到如下的变更集：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">增加：  /etc/my-app.d/default.cfg</div><div class="line">修改：  /bin/my-app-tools</div><div class="line">删除：  /etc/my-app-config</div></pre></td></tr></table></figure></p><p>创建一个 <strong>仅包含</strong> 此变更集的tar包：增加和修改的文件内容及目录被完整地包含在tar包中；而删除的项则对应为相同路径的空文件，其文件名或目录名增加<code>.wh.</code>前缀（表示已删除）。</p><blockquote><p>注意：无法直接创建以名称<code>.wh.</code>开头的文件或目录。</p></blockquote><p>目录<code>f60c56784b83</code>生成的<code>tar</code> 包中有如下的文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">/etc/my-app.d/default.cfg</div><div class="line">/bin/my-app-tools</div><div class="line">/etc/.wh.my-app-config</div></pre></td></tr></table></figure><p>任何镜像都是由若干类似的文件变更集的tar包组成的。</p><h2 id="镜像的组合格式"><a href="#镜像的组合格式" class="headerlink" title="镜像的组合格式"></a>镜像的组合格式</h2><p>包含镜像完整内容的单一tar包格式如下：</p><ul><li>镜像名：tag</li><li>镜像的 JSON 配置文件</li><li>各层的tar包</li></ul><p>如镜像<code>library/busybox</code>的组合tar包内容如下（使用<code>tree</code>命令输出）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── 47bcc53f74dc94b1920f0b34f6036096526296767650f223433fe65c35f149eb.json</div><div class="line">├── 5f29f704785248ddb9d06b90a11b5ea36c534865e9035e4022bb2e71d4ecbb9a</div><div class="line">│   ├── VERSION</div><div class="line">│   ├── json</div><div class="line">│   └── layer.tar</div><div class="line">├── a65da33792c5187473faa80fa3e1b975acba06712852d1dea860692ccddf3198</div><div class="line">│   ├── VERSION</div><div class="line">│   ├── json</div><div class="line">│   └── layer.tar</div><div class="line">├── manifest.json</div><div class="line">└── repositories</div></pre></td></tr></table></figure><p>镜像的每层都对应一个目录，其名称是64个十六进制的字符，是根据该层的文件内容确定性地生成的。</p><blockquote><p>注意：该目录名 <strong>不必</strong> 是层的<code>DiffID</code>或<code>ChainID</code>。</p></blockquote><p>每个目录包含3个文件：</p><ul><li><code>VERSION</code> - <code>json</code>文件模式的版本号；</li><li><code>json</code> - 旧的JSON格式镜像层元数据。v1.2版的镜像规范中，各层没有JSON元数据，但在v1版中是存在的。此文件是为了向后兼容v1版格式。</li><li><code>layer.tar</code> - 该层的tar包。</li></ul><p>注意：这个目录结构仅用于向后兼容。当前的实现使用<code>manifest.json</code>文件中列出的目录。</p><p><code>VERSION</code>文件只是JSON元数据模式的版本号：<code>1.0</code>。</p><p><code>repositories</code>也是一个JSON文件，包含镜像名和tag列表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;  </div><div class="line">    &quot;busybox&quot;:&#123;  </div><div class="line">        &quot;latest&quot;:&quot;5f29f704785248ddb9d06b90a11b5ea36c534865e9035e4022bb2e71d4ecbb9a&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>其中有镜像的<code>repository</code>和一组tag列表。每个tag关联着镜像的 ID。该文件仅用于向后兼容。当前的实现使用<code>manifest.json</code>文件。</p><p><code>manifest.json</code>文件是顶层镜像的JSON配置。<br>该文件包含以下元数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[</div><div class="line">  &#123;</div><div class="line">    &quot;Config&quot;: &quot;47bcc53f74dc94b1920f0b34f6036096526296767650f223433fe65c35f149eb.json&quot;,</div><div class="line">    &quot;RepoTags&quot;: [&quot;busybox:latest&quot;],</div><div class="line">    &quot;Layers&quot;: [</div><div class="line">      &quot;a65da33792c5187473faa80fa3e1b975acba06712852d1dea860692ccddf3198/layer.tar&quot;,</div><div class="line">      &quot;5f29f704785248ddb9d06b90a11b5ea36c534865e9035e4022bb2e71d4ecbb9a/layer.tar&quot;</div><div class="line">    ]</div><div class="line">  &#125;</div><div class="line">]</div></pre></td></tr></table></figure></p><p>上面这个JSON数组中，每项都对应着一个镜像。</p><ul><li><code>Config</code> 指向该镜像的JSON文件；</li><li><code>RepoTags</code> 是该镜像的名称；</li><li><code>Layers</code> 指向镜像各层的 tar 包；</li><li><code>Parent</code> 可选，指向其父镜像的 imageID，父镜像的ID必须在同一个 <code>manifest.json</code> 文件中存在。</li></ul><p>不要把 <code>manifest.json</code> 与用来push和pull镜像的分发清单（distribution manifest）相混淆。<br>一般来说，支持v1.2版本镜像规范的实现将使用<code>manifest.json</code>文件，早期的实现仍使用 <code>*/json</code>和<code>repositories</code>文件。</p><h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><p>其它相关文档：</p><ul><li><a href="https://github.com/opencontainers/image-spec" target="_blank" rel="external">Open Containers Initiative Image Spec</a></li><li><a href="https://github.com/docker/distribution/blob/master/docs/spec/manifest-v2-2.md" target="_blank" rel="external">Image Manifest Version 2, Schema 2</a></li><li><a href="https://github.com/docker/distribution/blob/master/docs/spec/api.md" target="_blank" rel="external">Docker Registry HTTP API V2</a></li><li><a href="https://github.com/apache/mesos/blob/master/docs/container-image.md" target="_blank" rel="external">Supporting Container Images in Mesos Containerizer</a></li></ul><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><blockquote><p>注意：不要把上面的 <strong>镜像格式</strong> 与镜像的 <strong>主机存储布局</strong> 搞混了。</p><ul><li>镜像格式是执行<code>docker save &lt;镜像名或ID&gt;</code>之后得到的对应镜像<code>tar</code>包的格式。</li><li>镜像在主机的存储布局，以及镜像push和pull都 <strong>不会</strong> 用到打包成一个文件的镜像，因为这样不利于多个层并行加速下载和利用本地缓存的层。</li></ul><p>镜像的各层存在顺序依赖，而镜像也有父子继承关系。<br>最初Docker只支持AUFS存储驱动，但AUFS没有合并到Linux内核，虽然Ubuntu内置了AUFS，但RHEL/CentOS则需要添加对应的内核模块。目前Docker在RHEL/CentOS默认使用OverlayFS作为存储驱动。OverlayFS只支持上下两层，所以其主机存储布局与AUFS不同，但镜像格式不受影响。</p></blockquote><h2 id="alpine镜像的主机存储布局"><a href="#alpine镜像的主机存储布局" class="headerlink" title="alpine镜像的主机存储布局"></a>alpine镜像的主机存储布局</h2><p>alpine镜像仅有一个层，比较简单。Ubuntu使用AUFS存储驱动的文件布局如下。layer.tar包 已经被解开了。</p><ul><li>./aufs是解开layer.tar后的文件内容；</li><li>./aufs/mnt是容器文件系统的挂载点；</li><li>./containers是创建的容器的读写层；</li><li>./image/aufs/distribution中两个文件夹相当于正反查找的指针；</li><li>./image/aufs/imagedb/content/sha256/[id] 接近镜像的JSON描述文件，但与上面的v1.2规范不完全一致；</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"># tree /var/lib/docker</div><div class="line">/var/lib/docker</div><div class="line">├── aufs</div><div class="line">│   ├── diff</div><div class="line">│   │   └── 0b9c9a223af5f795049b86fc4f3dace61a44ced8a08a3cc8ccad0699eecec951</div><div class="line">│   │       ├── bin</div><div class="line">│   │       │   ├── ash -&gt; /bin/busybox</div><div class="line">│   │       │  # ... alpine镜像中的文件列表（大部分是指向/bin/busybox的软链接）</div><div class="line">│   ├── layers</div><div class="line">│   │   └── 0b9c9a223af5f795049b86fc4f3dace61a44ced8a08a3cc8ccad0699eecec951</div><div class="line">│   └── mnt</div><div class="line">│       └── 0b9c9a223af5f795049b86fc4f3dace61a44ced8a08a3cc8ccad0699eecec951</div><div class="line">├── containers</div><div class="line">├── image</div><div class="line">│   └── aufs</div><div class="line">│       ├── distribution</div><div class="line">│       │   ├── diffid-by-digest</div><div class="line">│       │   │   └── sha256</div><div class="line">│       │   │       └── cfc728c1c5584d8e0ae69368fc9c34d54d72651355573ba42554c2469a0a6299</div><div class="line">│       │   └── v2metadata-by-diffid</div><div class="line">│       │       └── sha256</div><div class="line">│       │           └── e154057080f406372ebecadc0bfb5ff8a7982a0d13823bab1be5b86926c6f860</div><div class="line">│       ├── imagedb</div><div class="line">│       │   ├── content</div><div class="line">│       │   │   └── sha256</div><div class="line">│       │   │       └── 02674b9cb179d57c68b526733adf38b458bd31ba0abff0c2bf5ceca5bad72cd9</div><div class="line">│       │   └── metadata</div><div class="line">│       │       └── sha256</div><div class="line">│       ├── layerdb</div><div class="line">│       │   ├── sha256</div><div class="line">│       │   │   └── e154057080f406372ebecadc0bfb5ff8a7982a0d13823bab1be5b86926c6f860</div><div class="line">│       │   │       ├── cache-id </div><div class="line">│       │   │       ├── diff</div><div class="line">│       │   │       ├── size</div><div class="line">│       │   │       └── tar-split.json.gz # 如果是中间层，此处会有 parent 文件</div><div class="line">│       │   └── tmp</div><div class="line">│       └── repositories.json</div><div class="line">├── network</div><div class="line">│   └── files</div><div class="line">│       └── local-kv.db</div><div class="line">├── plugins</div><div class="line">│   ├── storage</div><div class="line">│   │   └── blobs</div><div class="line">│   │       └── tmp</div><div class="line">│   └── tmp</div><div class="line">├── swarm</div><div class="line">├── tmp</div><div class="line">├── trust</div><div class="line">└── volumes</div><div class="line">    └── metadata.db</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文见 &lt;a href=&quot;https://github.com/moby/moby/blob/master/image/spec/v1.2.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/moby/moby/blob/master/image/spec/v1.2.md&lt;/a&gt;&lt;br&gt;Docker已经迁移到Moby项目了。&lt;/p&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>CentOS 7 及 Ubuntu 16.04 设置 NAT 网络</title>
    <link href="https://ying-zhang.github.io/misc/2017/setup-nat-x-route/"/>
    <id>https://ying-zhang.github.io/misc/2017/setup-nat-x-route/</id>
    <published>2017-04-14T16:00:00.000Z</published>
    <updated>2017-10-30T03:42:06.185Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下设置一台双网卡的CentOS服务器提供 NAT 转发，作为内网网关，其它服务器的默认网关更改为此节点，添加路由，以及处理外部ssh登录变慢的问题。<br><a id="more"></a></p><!-- TOC --><ul><li><a href="#centos-7-%E8%AE%BE%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8nat">CentOS 7 设置和使用NAT</a><ul><li><a href="#nat%E7%BD%91%E5%85%B3%E7%9A%84%E8%AE%BE%E7%BD%AE">NAT网关的设置</a><ul><li><a href="#%E5%85%81%E8%AE%B8ip%E8%BD%AC%E5%8F%91">允许IP转发</a></li><li><a href="#%E8%AE%BE%E7%BD%AEiptables%E8%A7%84%E5%88%99">设置<code>iptables</code>规则</a></li></ul></li><li><a href="#%E8%AE%BE%E7%BD%AE%E5%85%B6%E5%AE%83-centos-7-%E6%9C%BA%E5%99%A8%E4%BD%BF%E7%94%A8nat%E7%BD%91%E7%BB%9C">设置其它 CentOS 7 机器使用NAT网络</a><ul><li><a href="#%E5%88%87%E6%8D%A2%E9%BB%98%E8%AE%A4%E7%BD%91%E5%85%B3">切换默认网关</a></li><li><a href="#%E4%B8%BAem1%E6%B7%BB%E5%8A%A0%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1">为<code>em1</code>添加静态路由</a></li></ul></li></ul></li><li><a href="#ubuntu-1604-%E8%AE%BE%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8nat">Ubuntu 16.04 设置和使用NAT</a><ul><li><a href="#%E8%AE%BE%E7%BD%AEnat">设置NAT</a><ul><li><a href="#%E5%85%81%E8%AE%B8ip%E8%BD%AC%E5%8F%91">允许IP转发</a></li><li><a href="#%E8%AE%BE%E7%BD%AEiptables%E8%A7%84%E5%88%99">设置iptables规则</a></li></ul></li><li><a href="#%E8%AE%BE%E7%BD%AE%E5%85%B6%E5%AE%83%E6%9C%BA%E5%99%A8%E4%BD%BF%E7%94%A8nat%E7%BD%91%E7%BB%9C">设置其它机器使用NAT网络</a></li></ul></li><li><a href="#%E5%85%B6%E5%AE%83%E9%97%AE%E9%A2%98">其它问题</a><ul><li><a href="#%E7%A6%81%E7%94%A8%E5%90%84%E6%9C%BA%E5%99%A8%E7%9A%84sshd-dns%E9%80%89%E9%A1%B9%EF%BC%8C%E8%A7%A3%E5%86%B3ssh%E7%99%BB%E5%BD%95%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98">禁用各机器的SSHD DNS选项，解决ssh登录很慢的问题</a></li><li><a href="#%E5%8F%96%E6%B6%88%E5%8E%9F%E6%9D%A5%E8%AE%BE%E7%BD%AE%E7%9A%84http%E4%BB%A3%E7%90%86">取消原来设置的http代理</a></li></ul></li><li><a href="#ps-%E4%BD%BF%E7%94%A8-ssh-%E9%9A%A7%E9%81%93">PS: 使用 SSH 隧道</a></li></ul><!-- /TOC --><p>小组有一个Dell的刀片服务器机柜，每台机器都是双网卡，分别连到了2个LAN。</p><ul><li>一个LAN是学校的公网IP（下文记为<code>2.2.2.0/24</code>网段）。装好CentOS 7系统后，<code>em1</code>网卡在公网LAN，可以在校内直接访问<code>em1</code>的IP，但由于学校网络管理限制，校外无法访问。所以说这个公网IP跟私网IP差不了多少。</li><li>另一个LAN是用于远程管理的机房内私网IP（下文记为<code>10.0.0.0/24</code>网段），<code>em2</code>网卡在这个私网LAN。服务器的iDRAC也在这个私网里。这个私网的网关是某个机器上的VPN服务提供的，IP是<code>10.0.0.1</code>。登录到这个VPN之后，就可以远程访问iDRAC，也可以通过私网IP ssh登录到机器。</li></ul><p>一直是远程管理，只去过一次机房，还不清楚服务器的具体组网拓扑。按理也可以修改一些设置，通过VPN的网关访问外网，不过没有VPN这台机器的管理权限，只好另外找机器设置NAT。</p><h1 id="CentOS-7-设置和使用NAT"><a href="#CentOS-7-设置和使用NAT" class="headerlink" title="CentOS 7 设置和使用NAT"></a>CentOS 7 设置和使用NAT</h1><p>虽然学校的网络管理限制了刀片集群的LAN使用公网IP连不了校外网络，而且不能登录个人的外网帐号，好在可以填表申请一个<code>147</code>的公网IP（记对应的机器是<code>n147</code>）直通校外网。<br>之前是通过在<code>n147</code>上提供Squid HTTP代理服务的方式使其它机器能够访问外网，但代理用起来实在是不方便，所以改成NAT方式，这样其它机器将<code>n147</code>的私网IP作为网关，使用私网IP就可以访问外网了。在NAT模式下，<code>n147</code>就像一个软件实现的家用路由器。</p><h2 id="NAT网关的设置"><a href="#NAT网关的设置" class="headerlink" title="NAT网关的设置"></a>NAT网关的设置</h2><p>作为网关的机器名是<code>n147</code>，公网IP是<code>2.2.2.147/24</code>，对应<code>em1</code>网卡；私网IP是<code>10.0.0.147/24</code>，对应<code>em2</code>网卡。</p><h3 id="允许IP转发"><a href="#允许IP转发" class="headerlink" title="允许IP转发"></a>允许IP转发</h3><p>其它机器从<code>10.0.0.0/24</code>网段发送的数据包会被<code>em2</code>网卡接收，如果目标IP是外网，需要交给<code>em1</code>网卡发送出去。需要开启IP转发选项，才能允许不同网卡间转发数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 查看是否启用IP转发，如果是0则没有开启，是1则已经开启</div><div class="line">cat /proc/sys/net/ipv4/ip_forward</div><div class="line"></div><div class="line"># 可以修改上述文件的值来改变设置，但重启后会恢复默认值。</div><div class="line"># 需要将设置写入系统配置文件 /etc/sysctl.conf 或 /etc/sysctl.d/ip_forward.conf</div><div class="line"># 内容为</div><div class="line">net.ipv4.ip_forward = 1</div></pre></td></tr></table></figure></p><h3 id="设置iptables规则"><a href="#设置iptables规则" class="headerlink" title="设置iptables规则"></a>设置<code>iptables</code>规则</h3><p>安装必要软件并启用服务：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">yum install -y net-tools iproute iptables iptables-services</div><div class="line">systemctl disable firewalld</div><div class="line">systemctl stop firewalld</div><div class="line">systemctl enable iptables</div></pre></td></tr></table></figure></p><blockquote><p><code>net-tools</code>这个程序包有 <code>ifconfig</code>，<code>netstat</code>，<code>route</code>等命令，而<code>iproute2</code>（rpm包名为<code>iproute</code>）包括<code>tc</code>，<code>ip</code>，<code>ss</code>等命令，是<code>net-tools</code>的改进。</p></blockquote><p>通过<code>iptables</code>命令行可以添加NAT规则，但重启后也是会恢复默认值，应当将修改后的规则设置用<code>iptables-save</code>命令输出，然后保存起来。<br>这里直接使用<code>iptables-save</code>生成的配置项，保存到配置文件 <code>/etc/sysconfig/iptables</code>，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># Generated by iptables-save v1.4.21 on Thu Apr 20 22:14:39 2017</div><div class="line">*nat</div><div class="line">:PREROUTING ACCEPT [0:0]</div><div class="line">:INPUT ACCEPT [0:0]</div><div class="line">:OUTPUT ACCEPT [0:0]</div><div class="line">:POSTROUTING ACCEPT [0:0]</div><div class="line">-A POSTROUTING -o em1 -j MASQUERADE  </div><div class="line">COMMIT</div><div class="line"></div><div class="line">*filter</div><div class="line">:INPUT ACCEPT [0:0]</div><div class="line">:FORWARD ACCEPT [0:0]</div><div class="line">:OUTPUT ACCEPT [0:0]</div><div class="line">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</div><div class="line">-A INPUT -p icmp -j ACCEPT</div><div class="line">-A INPUT -i lo -j ACCEPT</div><div class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT</div><div class="line">-A FORWARD -i em2 -j ACCEPT</div><div class="line">COMMIT</div></pre></td></tr></table></figure></p><p>其中涉及NAT转发的是<code>-A POSTROUTING -o em1 -j MASQUERADE</code> 和 <code>-A FORWARD -i em2 -j ACCEPT</code>。<br>此外还删除了原来禁止<code>ping</code>的规则：<br><code>-A INPUT -j REJECT --reject-with icmp-host-prohibited</code><br><code>-A FORWARD -j REJECT --reject-with icmp-host-prohibited</code></p><p>重启<code>iptables</code>服务<code>systemctl restart iptables</code>。</p><blockquote><p>可以修改<code>/etc/sysconfig/iptables-config</code> 配置文件，设置下面三个选项均为<code>yes</code>来自动保存<code>iptables</code>规则</p><ul><li><code>IPTABLES_MODULES_UNLOAD=&quot;yes&quot;</code></li><li><code>IPTABLES_SAVE_ON_STOP=&quot;yes&quot;</code></li><li><code>IPTABLES_SAVE_ON_RESTART=&quot;yes&quot;</code></li></ul><p>参考</p><ul><li><a href="http://salogs.com/news/2015/08/20/iptables-save/" target="_blank" rel="external">保存iptable规则并开机自动加载</a></li><li><a href="http://blog.csdn.net/hepeng597/article/details/8270138" target="_blank" rel="external">iptables用法初解</a></li></ul></blockquote><h2 id="设置其它-CentOS-7-机器使用NAT网络"><a href="#设置其它-CentOS-7-机器使用NAT网络" class="headerlink" title="设置其它 CentOS 7 机器使用NAT网络"></a>设置其它 CentOS 7 机器使用NAT网络</h2><h3 id="切换默认网关"><a href="#切换默认网关" class="headerlink" title="切换默认网关"></a>切换默认网关</h3><p>安装系统后，默认网关是<code>em1</code>所在LAN的<code>2.2.2.1</code>，需要将其更改为<code>em2</code>所在LAN的<code>10.0.0.147</code>，修改如下的网卡配置文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># /etc/sysconfig/network-scripts/ifcfg-em1</div><div class="line">DEFROUTE=no</div><div class="line"></div><div class="line"># /etc/sysconfig/network-scripts/ifcfg-em2</div><div class="line">DEFROUTE=yes</div><div class="line"># ...</div><div class="line">GATEWAY=10.0.0.147</div><div class="line">DNS1=233.5.5.5</div><div class="line">DNS2=223.6.6.6</div></pre></td></tr></table></figure></p><p>其中<code>em2</code>的网关即刚才配置的<code>n147</code>的私网IP，由于没有在<code>n147</code>上运行DNS服务，这里使用的是阿里的DNS。也可以安装配置<code>dnsmasq</code>等DNS服务器。</p><h3 id="为em1添加静态路由"><a href="#为em1添加静态路由" class="headerlink" title="为em1添加静态路由"></a>为<code>em1</code>添加静态路由</h3><p>修改默认网关后，查看路由表，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># route -n</div><div class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</div><div class="line">0.0.0.0         10.0.0.147      0.0.0.0         UG    100    0        0 em2</div><div class="line">10.0.0.0        0.0.0.0         255.255.0.0     U     100    0        0 em2</div><div class="line">2.2.2.0         0.0.0.0         255.255.255.0   U     100    0        0 em1</div><div class="line">172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0</div></pre></td></tr></table></figure></p><p>发现从<code>em1</code>收发的数据包也要经过默认网关<code>10.0.0.147</code>，而这个网关在机房的私网内，<strong>外部不能直接访问，导致外部无法公网IP访问服务器</strong>。为此，需要为<code>em1</code>添加静态路由，不走<code>10.0.0.147</code>，新建文件<code>/etc/sysconfig/network-scripts/route-em1</code>，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2.2.0.0/16    via 2.2.2.1</div><div class="line">172.0.0.0/8   via 2.2.2.1</div></pre></td></tr></table></figure></p><p>其中</p><ul><li><code>2.2.0.0/16</code>网段是学校 <strong>有线</strong> 网络的IP网段</li><li><code>172.0.0.0/8</code>网段是学校 <strong>无线</strong> 网络的IP网段。<blockquote><p>注意</p></blockquote></li><li><code>docker0</code>网桥默认也是B类私网的<code>172.x.0.0/16</code>网段，但其路由项更具体，所以<a href="http://answ.me/post/configure-docker-subnet/" target="_blank" rel="external">不会造成冲突</a>。</li><li><code>docker0</code>网桥实际也工作在NAT模式，也需要开启IP转发。</li></ul><p>重启网络，更新配置，<code>systemctl restart network</code>。</p><blockquote><p>参考：<a href="https://www.thomas-krenn.com/en/wiki/Two_Default_Gateways_on_One_System" target="_blank" rel="external">Two Default Gateways on One System</a></p></blockquote><h1 id="Ubuntu-16-04-设置和使用NAT"><a href="#Ubuntu-16-04-设置和使用NAT" class="headerlink" title="Ubuntu 16.04 设置和使用NAT"></a>Ubuntu 16.04 设置和使用NAT</h1><p>Ubuntu 16.04 参考上面 CentOS 7 设置和使用NAT的步骤即可————怎么可能！<br>因为用到的工具都是<code>net-tools</code>，<code>iproute</code>和<code>iptables</code>这3个包里的，配置文件的内容是一样的，但配置文件的路径和启动方式就不一样了。</p><h2 id="设置NAT"><a href="#设置NAT" class="headerlink" title="设置NAT"></a>设置NAT</h2><h3 id="允许IP转发-1"><a href="#允许IP转发-1" class="headerlink" title="允许IP转发"></a>允许IP转发</h3><p>直接修改系统配置文件 <code>/etc/sysctl.conf</code>，里面已经有支持的配置项，取消<code>net.ipv4.ip_forward = 1</code>的注释即可。</p><h3 id="设置iptables规则-1"><a href="#设置iptables规则-1" class="headerlink" title="设置iptables规则"></a>设置iptables规则</h3><p>还是使用的<code>iptables</code>，为了使修改后的规则能够保存，规则跟上面CentOS的基本一样。为什么说是基本呢？网卡的默认名字改了： CentOS 7下是<code>em1</code>，<code>em2</code>，Ubuntu 16.04下是<code>eno1</code>，<code>eno2</code>，所以要把规则中对应的网卡名改过来。</p><p>然后要找到Ubuntu 16.04下系统会自动读取的iptables规则文件路径，参考<a href="https://help.ubuntu.com/community/IptablesHowTo" target="_blank" rel="external">IptablesHowTo - Ubuntu wiki</a>，，<strong>随便把规则保存到哪，反正系统不会自动加载</strong>，需要自己添加一个启动脚本。<br>因为机器是多网卡，启动脚本不好关联某个网卡，所以放在与网卡启动关联的脚本目录里，脚本文件是 <code>/etc/network/if-pre-up.d/iptablesload</code>，内容如下，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line">iptables-restore &lt; /etc/iptables.rules</div><div class="line">exit 0</div></pre></td></tr></table></figure></p><p>可见<code>/etc/iptables.rules</code>是保存的<code>iptables</code>规则，这个位置是任意的，只要跟启动脚本中保持一致就行了。启动脚本的名字也是任意的，只要放在那个路径下就可以了。<strong>但是，你要自己来写这个脚本</strong>。</p><p>编辑好<code>/etc/iptables.rules</code> 和 <code>/etc/network/if-pre-up.d/iptablesload</code>之后，重启网络服务。</p><p>虽然都是用<code>systemd</code>来管理系统服务，但服务的名字又不一样了，CentOS的网络服务名称是<code>network.service</code>，而Ubuntu则是<code>networking.service</code>。<br>执行命令<code>sudo systemctl restart networking</code>，重启网络服务。不过，其实我们是用<code>iptables</code>来实现的NAT，而不是<code>networking.service</code>相关服务，不过由于上面的那个脚本<code>iptablesload</code>会在网卡启动之前被执行，所以顺便完成了我们的目标。</p><h2 id="设置其它机器使用NAT网络"><a href="#设置其它机器使用NAT网络" class="headerlink" title="设置其它机器使用NAT网络"></a>设置其它机器使用NAT网络</h2><p>同样，在其它Ubuntu机器上也要切换默认网关，为eno1添加静态路由。情况又不一样了，Ubuntu的所有网卡配置和静态路由都保存在<code>/etc/network/interface</code>这个文件里，这个倒是方便了—— <strong><a href="https://askubuntu.com/questions/168033/how-to-set-static-routes-in-ubuntu-server" target="_blank" rel="external">如果面对空空的配置文件，你能猜到各配置项是什么的时候</a></strong>。<br>好在安装系统的时候设置了网卡，给<code>eno2</code>生成了一些配置项可以参考。修改后的内容如下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">auto lo</div><div class="line">iface lo inet loopback</div><div class="line"></div><div class="line">auto eno1</div><div class="line">iface eno1 inet static</div><div class="line">  address 2.2.2.140</div><div class="line">  netmask 255.255.255.0</div><div class="line">  network 2.2.2.0</div><div class="line">  broadcast 2.2.2.255</div><div class="line">  # gateway 2.2.2.1</div><div class="line">  up ip route add 2.2.0.0/16   via 2.2.2.1 || true</div><div class="line">  up ip route add 2.2.2.0/24   via 2.2.2.1 || true</div><div class="line">  up ip route add 172.0.0.0/8  via 2.2.2.1 || true</div><div class="line"></div><div class="line"># The primary network interface</div><div class="line">auto eno2</div><div class="line">iface eno2 inet static</div><div class="line">  address 10.0.0.140</div><div class="line">  netmask 255.255.255.0</div><div class="line">  network 10.0.0.0</div><div class="line">  broadcast 10.0.0.255</div><div class="line">  gateway 10.0.0.147</div><div class="line">  # dns-* options are implemented by the resolvconf package, if installed</div><div class="line">  dns-nameservers 233.5.5.5</div></pre></td></tr></table></figure></p><blockquote><p>注意</p><ul><li>不是像CentOS那样明确地在各网卡的配置文件中说明是否该网卡作为默认路由网关，而是哪个网卡写了<code>gateway</code>就以它作为默认网关，如果2个网卡都写了，只有最后写的那个有效，所以最好明确地只保留一个<code>gateway</code>项。</li><li><code>up ip route add 2.2.0.0/16   via 2.2.2.1 || true</code>其实也就是一句启动网卡后顺便执行的脚本，如果你愿意，这里写一句<code>up echo helloworld</code>也没问题。后面的<code>|| true</code>是让脚本执行时忽略可能的错误，继续执行。这里使用的是<code>ip</code>命令，如果使用<code>route</code>命令，效果也是一样的，前提是已经安装了<code>net-tools</code>或<code>iproute</code>软件包。使用<code>netmask</code>和<code>CIDR</code>格式都可以。</li><li>哪个网卡的路由就写到对应的网卡下，这样执行<code>ifup eno1</code>这样的单独启动某个网卡的命令也会自动执行相应的命令添加该网卡的路由。</li></ul></blockquote><p>执行命令<code>sudo systemctl restart networking</code>，重启网络服务。<br>查看一下更新后的路由表，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># route -n</div><div class="line">Kernel IP routing table</div><div class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</div><div class="line">0.0.0.0         10.0.0.147      0.0.0.0         UG    0      0        0 eno2</div><div class="line">10.0.0.0        0.0.0.0         255.255.255.0   U     0      0        0 eno2</div><div class="line">2.2.0.0         2.2.2.1         255.255.0.0     UG    0      0        0 eno1</div><div class="line">2.2.2.0         0.0.0.0         255.255.255.0   U     0      0        0 eno1</div><div class="line">172.0.0.0       2.2.2.1         255.0.0.0       UG    0      0        0 eno1</div></pre></td></tr></table></figure></p><p>注意倒数第二行：<code>2.2.2.0    0.0.0.0    255.255.255.0   U    0    0    0    eno1</code>，这个是自动添加的。<br>它的网关并不是<code>2.2.2.1</code>，即便在<code>interface</code>中强制器网关为<code>2.2.2.1</code>，也会被改为<code>0.0.0.0</code>。因为<code>2.2.2.0/24</code>是本LAN的数据包，经<code>ARP</code>协议获得目标的<code>MAC</code>地址后，会被直接发到目标地址，而不会经网关<code>2.2.2.1</code>转发，虽然路由表中默认网关是<code>0.0.0.0</code>，也不会按路由表转发到<code>10.0.0.147</code>。</p><h1 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h1><p>下面的问题是在除NAT网关外的其它机器上设置的。</p><h2 id="禁用各机器的SSHD-DNS选项，解决ssh登录很慢的问题"><a href="#禁用各机器的SSHD-DNS选项，解决ssh登录很慢的问题" class="headerlink" title="禁用各机器的SSHD DNS选项，解决ssh登录很慢的问题"></a>禁用各机器的SSHD DNS选项，解决ssh登录很慢的问题</h2><p>修改默认网关，添加路由之后，虽然可以从外部通过公网IP <code>ping</code>通机房内的机器，但<code>ssh</code>登录过程需要等待很长时间。<br>参考<a href="http://blog.itpub.net/7345798/viewspace-1055461/" target="_blank" rel="external">ssh连接的时候很慢，ping的速度非常好</a>这篇文章，修改各机器上<code>sshd</code>的选项如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sed -i &quot;&apos;s/.*UseDNS.*/UseDNS no/g&apos;&quot; /etc/ssh/sshd_config</div><div class="line">systemctl restart sshd</div></pre></td></tr></table></figure></p><h2 id="取消原来设置的http代理"><a href="#取消原来设置的http代理" class="headerlink" title="取消原来设置的http代理"></a>取消原来设置的http代理</h2><p>注释掉配置文件中<code>http_proxy</code>和<code>https_proxy</code>环境变量的声明，并执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">unset http_proxy</div><div class="line">unset https_proxy</div></pre></td></tr></table></figure></p><h1 id="PS-使用-SSH-隧道"><a href="#PS-使用-SSH-隧道" class="headerlink" title="PS: 使用 SSH 隧道"></a>PS: 使用 SSH 隧道</h1><p>一般可以通过服务器的公网IP在校内登录到机器上，有时把机器的配置搞乱了，需要连接到内网，使用iDRAC来管理机器，这时就要连接VPN了。其实也可以不连接VPN，而利用其它正常机器的SSH创建隧道，实现连接内网的目的。<br>以Windows客户端为例，安装了Windows版的<code>git</code>后，会附带一些<code>mingw</code>命令程序（不是完整的<code>mingw</code>），首先将这些命令的可执行文件所在路径添加到<code>Path</code>环境变量，比如是<code>E:\App\git\usr\bin;E:\App\git\mingw64\bin;E:\App\git\bin</code>，然后就可以在Windows命令行使用这些命令了。其中就有<code>ssh.exe</code>。<br>在Windows命令行窗口执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh -NfD 1088 -i C:\Users\ying\.ssh\id_rsa root@n147</div></pre></td></tr></table></figure></p><p>这个命令执行成功后会退出，返回到命令提示符，但实际仍在后台监听本机的1088端口<code>127.0.0.1:1088</code>。<strong>如果需要停止SSH隧道，关闭该命令窗口即可</strong>。<br>因为我在Windows的<code>C:\Windows\System32\drivers\etc\hosts</code>文件中添加了<code>n147</code>的项，所以可以直接输入名字而不必是IP地址。</p><p>在IE或控制面板打开<code>Internet选项</code>，</p><ul><li>在<code>连接</code>选项卡单击<code>局域网设置</code>按钮，</li><li>在弹出的对话框勾选<code>为LAN使用代理服务器</code>，然后单击<code>高级</code>，</li><li>在弹出的对话框取消勾选<code>对所有协议使用相同的代理服务器</code>，然后在<code>套接字</code>对应的文本框填入<code>127.0.0.1</code>和上面命令中监听的端口号<code>1088</code>，</li><li>在<code>例外</code>文本框中删掉<code>10.*0;</code>的内容，然后一路确定关闭所有对话框。<br>这时应该可以用IE或Chrome访问机房内网的iDRAC。<strong>如果需要正常访问网页，需要取消勾选<code>为LAN使用代理服务器</code></strong>。</li></ul><p><img src="/img/sock5-proxy.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下设置一台双网卡的CentOS服务器提供 NAT 转发，作为内网网关，其它服务器的默认网关更改为此节点，添加路由，以及处理外部ssh登录变慢的问题。&lt;br&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>设置及使用HTTP代理</title>
    <link href="https://ying-zhang.github.io/misc/2017/setup-squid-proxy/"/>
    <id>https://ying-zhang.github.io/misc/2017/setup-squid-proxy/</id>
    <published>2017-04-13T16:00:00.000Z</published>
    <updated>2017-10-30T02:47:23.293Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下设置Squid作为HTTP代理，及docker、yum、apt等的代理配置。<br><a id="more"></a></p><!-- TOC --><ul><li><a href="#%E4%BD%BF%E7%94%A8squid%E6%8F%90%E4%BE%9Bhttp%E4%BB%A3%E7%90%86">使用Squid提供HTTP代理</a><ul><li><a href="#%E4%B8%BB%E6%9C%BA%E4%B8%8A%E5%AE%89%E8%A3%85%E5%92%8C%E8%AE%BE%E7%BD%AEsquid">主机上安装和设置Squid</a></li><li><a href="#%E4%BB%A5docker%E5%AE%B9%E5%99%A8%E7%9A%84%E6%96%B9%E5%BC%8F%E8%BF%90%E8%A1%8Csquid">以Docker容器的方式运行Squid</a></li></ul></li><li><a href="#%E4%BD%BF%E7%94%A8http%E4%BB%A3%E7%90%86">使用HTTP代理</a><ul><li><a href="#%E5%85%A8%E5%B1%80%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">全局的环境变量</a></li><li><a href="#docker">Docker</a></li><li><a href="#yum">yum</a></li><li><a href="#apt">apt</a></li></ul></li></ul><!-- /TOC --><h1 id="使用Squid提供HTTP代理"><a href="#使用Squid提供HTTP代理" class="headerlink" title="使用Squid提供HTTP代理"></a>使用Squid提供HTTP代理</h1><h2 id="主机上安装和设置Squid"><a href="#主机上安装和设置Squid" class="headerlink" title="主机上安装和设置Squid"></a>主机上安装和设置Squid</h2><p>作为网关的<code>n147</code>机器，公网IP是<code>2.2.2.147</code>。安装Squid，然后修改配置，启用服务。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">yum install -y squid # CentOS</div><div class="line">apt install -y squid # Ubuntu</div><div class="line">apk add squid     # Alpine</div><div class="line"></div><div class="line"># squid的配置文件在 /etc/squid/squid.conf，修改内容可参考下面的 Dockerfile</div><div class="line"></div><div class="line"># 修改配置后，初始化squid的工作目录</div><div class="line">squid -z</div><div class="line"></div><div class="line"># 启动服务</div><div class="line">systemctl enable squid</div><div class="line">systemctl start  squid</div></pre></td></tr></table></figure></p><h2 id="以Docker容器的方式运行Squid"><a href="#以Docker容器的方式运行Squid" class="headerlink" title="以Docker容器的方式运行Squid"></a>以Docker容器的方式运行Squid</h2><p>Dockerfile内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">FROM alpine:latest</div><div class="line"></div><div class="line">RUN apk update --no-cache; \</div><div class="line">    apk add squid --no-cache</div><div class="line"></div><div class="line"># 可以在squid.conf中限制允许访问此代理的IP范围，否则只有内网IP可以访问</div><div class="line">RUN sed  -i &quot;/RFC 4291/a acl ics src 2.2.2.0/24&quot; squid.conf; \</div><div class="line">    sed  -i &quot;/RFC 4291/a acl ics src 2.2.3.3/32&quot; squid.conf</div><div class="line"></div><div class="line"># 可以修改默认的端口号，如果修改了默认端口，需要修改下面的 EXPOSE 部分</div><div class="line">RUN sed -i &quot;/http_port/c http_port 8888&quot; squid.conf</div><div class="line"></div><div class="line"># 开启cache</div><div class="line">RUN sed -i &apos;/cache_dir/s/#//g&apos; /etc/squid/squid.conf</div><div class="line"></div><div class="line"># 或者直接使用修改过的配置文件</div><div class="line"># ADD squid.conf /etc/squid/squid.conf</div><div class="line"></div><div class="line"># squid -z用于初始化，创建cache目录，但直接在Dockerfile中</div><div class="line"># RUN squid -z</div><div class="line"># 却无法创建cache目录，导致squid无法启动</div><div class="line"># 故将初始化和启动命令写入脚本中</div><div class="line"></div><div class="line">RUN echo -e &apos;#!/bin/sh\n[ -d /var/cache/squid/00 ] || squid -z\nsquid -N&apos; &gt;/squid.sh; \</div><div class="line">    chmod +x /squid.sh</div><div class="line"></div><div class="line">EXPOSE 3128</div><div class="line">CMD [&quot;/squid.sh&quot;]</div></pre></td></tr></table></figure></p><p>构造镜像：<code>docker build ./ -t squid:latest</code><br>启动容器：<code>docker run -d -p 3128:3128 --name squid squid:latest</code></p><h1 id="使用HTTP代理"><a href="#使用HTTP代理" class="headerlink" title="使用HTTP代理"></a>使用HTTP代理</h1><p>内网其它不能直接访问外网的机器可以设置使用<code>n147</code>提供的代理服务。</p><h2 id="全局的环境变量"><a href="#全局的环境变量" class="headerlink" title="全局的环境变量"></a>全局的环境变量</h2><p>在<code>/etc/environment</code>（不需要<code>export</code>），<code>/etc/profile</code>或<code>/etc/profile.d/http_proxy.sh</code>导出<code>http_proxy</code>和<code>https_proxy</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export http_proxy=http://2.2.2.147:3128</div><div class="line">export https_proxy=http://2.2.2.147:3128</div></pre></td></tr></table></figure></p><p><code>squid</code>可以作为https代理，只要设置 <code>https_proxy=http://2.2.2.147:3128</code>， 即这个环境变量以<code>http://</code>开头。</p><h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><p>Docker需要<a href="https://docs.docker.com/engine/admin/systemd/" target="_blank" rel="external">单独设置代理</a>，新建文件<code>/etc/systemd/system/docker.service.d/http-proxy.conf</code>，内容如下（注意多项环境变量之间要有空格，还设置了对私有镜像仓库不使用代理）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[Service]</div><div class="line">Environment=&quot;HTTP_PROXY=http://2.2.2.147:3128&quot; &quot;HTTPS_PROXY=http://2.2.2.147:3128&quot;  &quot;NO_PROXY=localhost,10.0.0.147&quot;</div></pre></td></tr></table></figure></p><p>重启docker daemon： <code>systemctl restart docker</code>，执行<code>docker info</code>查看是否生效。</p><h2 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h2><p>yum 会使用全局代理设置，也可以单独设置代理，在<code>/etc/yum.conf</code>中增加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">proxy=http://2.2.2.147:3128</div></pre></td></tr></table></figure></p><h2 id="apt"><a href="#apt" class="headerlink" title="apt"></a>apt</h2><p>在文件<code>/etc/apt/apt.conf</code>中增加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Acquire::http::proxy  &quot;http://2.2.2.147:3128&quot;;</div><div class="line">Acquire::https::proxy &quot;http://2.2.2.147:3128&quot;;</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下设置Squid作为HTTP代理，及docker、yum、apt等的代理配置。&lt;br&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>在MPI集群执行Linpack测试</title>
    <link href="https://ying-zhang.github.io/misc/2017/install-linpack-xhpl/"/>
    <id>https://ying-zhang.github.io/misc/2017/install-linpack-xhpl/</id>
    <published>2017-04-11T16:00:00.000Z</published>
    <updated>2017-10-30T03:41:18.347Z</updated>
    
    <content type="html"><![CDATA[<p>使用虚拟机搭建一个3台VM的CentOS 7集群，安装mpi，blas和hpl，执行linpack测试集群的计算性能。<br>尝试编译netlib hpl 2.2时遇到一个错误，导致不能使用mpi。<br>后来发现可以直接使用Intel编译好的xhpl。由于不理解mpi和xhpl的选项，集群的性能比单机还明显低得多，还没找到原因;-(<br><a id="more"></a></p><!-- TOC --><ul><li><a href="#%E5%AE%89%E8%A3%85%E5%92%8C%E8%AE%BE%E7%BD%AEmpich%EF%BC%8Cblas%EF%BC%88atlas%EF%BC%89%EF%BC%8Chpl">安装和设置mpich，blas（atlas），hpl</a><ul><li><a href="#%E8%AE%BE%E7%BD%AEmpich">设置mpich</a></li><li><a href="#blas-linpacklapack-hpl">BLAS, LINPACK/LAPACK, HPL</a></li><li><a href="#%E5%8D%95%E6%9C%BA%E4%B8%8A%E6%89%A7%E8%A1%8C-hpl">单机上执行 HPL</a></li></ul></li><li><a href="#%E4%BD%BF%E7%94%A8intel-mkl-benchmarks">使用Intel MKL Benchmarks</a></li><li><a href="#todo-%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%B5%8B%E8%AF%95mpi-xhpl">Todo: 集群中测试mpi xhpl</a></li></ul><!-- /TOC --><h1 id="安装和设置mpich，blas（atlas），hpl"><a href="#安装和设置mpich，blas（atlas），hpl" class="headerlink" title="安装和设置mpich，blas（atlas），hpl"></a>安装和设置mpich，blas（atlas），hpl</h1><p>编译可只在一台VM上进行，然后将编译的结果拷贝到其它VM。</p><h2 id="设置mpich"><a href="#设置mpich" class="headerlink" title="设置mpich"></a>设置mpich</h2><p>参考 <a href="http://www.mpich.org/static/downloads/3.2/mpich-3.2-installguide.pdf" target="_blank" rel="external">MPI安装手册</a> 和 <a href="http://www.mpich.org/static/downloads/3.2/mpich-3.2-userguide.pdf" target="_blank" rel="external">MPI用户手册</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># 安装编译器</div><div class="line">yum install -y gcc gcc-gfortran gcc-c++ bzip2 wget</div><div class="line"></div><div class="line"># yum 可以安装mpich-3.2.x86_64，但只有lib，没有bin，所以这里从src编译</div><div class="line">wget http://www.mpich.org/static/downloads/3.2/mpich-3.2.tar.gz</div><div class="line">tar axf mpich-3.2.tar.gz</div><div class="line">cd ~/mpich-3.2</div><div class="line">./configure prefix=/opt/mpich</div><div class="line">make -j 8 &amp;&amp; make install  # make只会编译lib，make install才会编译lib和bin</div><div class="line"></div><div class="line">cp -r examples/ /opt/mpich/</div><div class="line"></div><div class="line"># 把编译结果打包</div><div class="line">cd ~</div><div class="line">tar zcf mpich-3.2-build.tar.gz /opt/mpich/</div><div class="line"># 可以将其保存到主机上</div><div class="line"></div><div class="line">echo &apos;export PATH=$PATH:/opt/mpich/bin&apos;                         &gt;&gt; /etc/profile</div><div class="line">echo &apos;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/mpich/lib&apos;   &gt;&gt; /etc/profile</div><div class="line">source /etc/profile</div><div class="line"></div><div class="line"># 在本机测试一下</div><div class="line">mpiexec -n 3 /opt/mpich/examples/cpi</div></pre></td></tr></table></figure></p><p>mpiexec会在单个主机创建N个进程（通过 -n 指定）执行后面的命令（程序），如果通过 -f host_list_file 指定集群节点列表，会把进程分布在这些节点上分布执行。<br>mpiexec执行的可以是普通的命令，这时只是重复地执行N次，但这些命令之间并没有什么联系。<br>如果执行的是一个使用了mpi库的程序，那么程序执行中会彼此通信，协调计算进度，从而充分利用集群的计算资源。<br>要在集群上运行mpi程序，需要所有的节点上都有这个程序的可执行文件，以及需要的数据，配置文件，环境变量等。当然可以将这些文件等拷贝到各节点，但一般会创建一个共享目录，集群中的节点都将共享目录挂载到相同的路径，并将mpi程序及相关文件放到共享目录下。</p><h2 id="BLAS-LINPACK-LAPACK-HPL"><a href="#BLAS-LINPACK-LAPACK-HPL" class="headerlink" title="BLAS, LINPACK/LAPACK, HPL"></a>BLAS, LINPACK/LAPACK, HPL</h2><p><a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" target="_blank" rel="external">BLAS（Basic Linear Algebra Subprograms）- wiki</a> 或 <a href="https://zh.wikipedia.org/wiki/BLAS" target="_blank" rel="external">BLAS基础线性代数程序集 - wiki</a> 是一个API标准，有多个开源实现，如Netlib BLAS（Fortran实现），Netlib ATLAS，Intel MKL和ACML等。这里使用ATLAS。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">wget https://downloads.sourceforge.net/project/math-atlas/Stable/3.10.3/atlas3.10.3.tar.bz2</div><div class="line">tar axf atlas3.10.3.tar.bz2   # 需要已安装bzip2</div><div class="line">cd ATLAS</div><div class="line">mkdir build; cd build</div><div class="line">../configure</div><div class="line">make -j 8 &amp;&amp; make install # 编译完成后安装到/usr/local/atlas，包括 lib 和 include</div><div class="line"># 同样将编译的结果也打包保存</div><div class="line">mkdir atlas</div><div class="line">mv bin/ lib/ include/ atlas/</div><div class="line">tar zcf atlas-build.tar.gz atlas/</div></pre></td></tr></table></figure></p><p><a href="https://en.wikipedia.org/wiki/LINPACK" target="_blank" rel="external">LINPACK</a>是一个线性代数数值计算库，其中用到了BLAS。不过目前多是使用它的后继<a href="https://en.wikipedia.org/wiki/LAPACK" target="_blank" rel="external">LAPACK</a>。<a href="http://www.netlib.org/benchmark/hpl/" target="_blank" rel="external">HPLinpack（Highly Parallel Computing benchmark，HP不是指惠普公司）</a>是一个使用LINPACK测试集群浮点计算性能的测试基准程序，测试的结果是多少GFPLOPS。</p><p>参考 <a href="http://blog.chinaunix.net/uid-20104120-id-4071017.html" target="_blank" rel="external">http://blog.chinaunix.net/uid-20104120-id-4071017.html</a> 。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">wget http://www.netlib.org/benchmark/hpl/hpl-2.2.tar.gz</div><div class="line">tar axf hpl-2.2.tar.gz</div><div class="line">cd hpl-2.2</div><div class="line">cp setup/Make.Linux_PII_CBLAS_gm Make.x86_64</div><div class="line"></div><div class="line"># 编辑 Make.x86_64，修改的内容如下。</div><div class="line"># 因为设置MPdir后有编译错误，所以没有设置。这样编译出来的是单机版的。</div><div class="line"># 注意各值结尾不要有 空格。</div><div class="line"># 虽然在 INCdir、BINdir和LIBdir删掉了$(ARCH)，但最终还创建了x86_64的空目录</div><div class="line">ARCH         = x86_64</div><div class="line"></div><div class="line">TOPdir       = $(HOME)/hpl-2.2</div><div class="line">INCdir       = $(TOPdir)/include</div><div class="line">BINdir       = $(TOPdir)/bin</div><div class="line">LIBdir       = $(TOPdir)/lib</div><div class="line"></div><div class="line">MPdir        =</div><div class="line">MPinc        =</div><div class="line">MPlib        =</div><div class="line"></div><div class="line">LAdir        = /usr/local/atlas  # atlas 执行了make install之后的安装目录</div><div class="line">LAinc        = -I$(LAdir)/include</div><div class="line">LAlib        = $(LAdir)/lib/libcblas.a $(LAdir)/lib/libatlas.a</div><div class="line"></div><div class="line"># 开始编译</div><div class="line">make arch=x86_64 -j 8</div><div class="line"></div><div class="line"># 也将编译的结果也打包保存</div><div class="line">mv hpl hpl.bk.d; mkdir hpl</div><div class="line">mv bin/ lib/ include/ hpl/</div><div class="line">rmdir hpl/bin/x86_64/ hpl/include/x86_64/ hpl/lib/x86_64/</div><div class="line">tar zcf hpl-build.tar.gz hpl/</div></pre></td></tr></table></figure></p><h2 id="单机上执行-HPL"><a href="#单机上执行-HPL" class="headerlink" title="单机上执行 HPL"></a>单机上执行 HPL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /root/hpl-2.2/hpl/bin/</div><div class="line">mpiexec -n 4 ./xhpl  # 得到的结果比较差，只有约0.4GFLOPS</div><div class="line"># 因为需要配置HPL.dat，选择合适的参数，另外编译中一些选项也会有影响</div></pre></td></tr></table></figure><h1 id="使用Intel-MKL-Benchmarks"><a href="#使用Intel-MKL-Benchmarks" class="headerlink" title="使用Intel MKL Benchmarks"></a>使用Intel MKL Benchmarks</h1><p>参考 <a href="https://software.intel.com/en-us/articles/intel-mkl-benchmarks-suite" target="_blank" rel="external">https://software.intel.com/en-us/articles/intel-mkl-benchmarks-suite</a> 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">wget http://registrationcenter-download.intel.com/akdlm/irc_nas/9752/l_mklb_p_2017.2.015.tgz</div><div class="line">tar axf l_mklb_p_2017.2.015.tgz</div><div class="line">cd l_mklb_p_2017.2.015/benchmarks_2017/linux/mkl/benchmarks/linpack</div><div class="line">./runme_xeon64</div></pre></td></tr></table></figure><p>测试的结果约 150GFLOPS。<br>还有一个Windows版的，在主机上测试也是接近的结果。</p><h1 id="Todo-集群中测试mpi-xhpl"><a href="#Todo-集群中测试mpi-xhpl" class="headerlink" title="Todo: 集群中测试mpi xhpl"></a>Todo: 集群中测试mpi xhpl</h1><p>直接运行 <code>l_mklb_p_2017.2.015/benchmarks_2017/linux/mkl/benchmarks/mp_linpack</code> 中的 <code>runme_intel64_static</code> 会报不识别 perhost 参数的错误。<br>执行 <code>mpiexec -f hosts -n 12 ./xhpl_intel64_static</code>（xhpl_intel64_static和HPL.dat拷贝到了/root，hosts是节点列表），在其它节点通过<code>top</code>监视，确实执行了xhpl_intel64_static，但输出只有当前机器的，而且只有约4GFLOPS（单机执行xhpl_intel64_static也是这么多，按理应接近4×集群节点数啊），不知道问题具体出在哪里，看来还是需要仔细看文档了。或许MPI，BLAS等全部使用Intel的版本？<br>参考</p><ul><li><a href="https://software.intel.com/zh-cn/articles/intel-mpi-library-documentation" target="_blank" rel="external">Intel® MPI Library - Documentation</a></li><li><a href="https://software.intel.com/en-us/node/528457" target="_blank" rel="external">Intel® Optimized MP LINPACK Benchmark for Clusters</a></li><li><a href="https://software.intel.com/en-us/articles/performance-tools-for-software-developers-hpl-application-note" target="_blank" rel="external">HPL application note</a></li><li><a href="http://khmel.org/?p=527" target="_blank" rel="external">HPC LINPACK benchmark</a></li><li><a href="http://blog.sciencenet.cn/blog-935970-892936.html" target="_blank" rel="external">如何做LINPACK测试及性能优化</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用虚拟机搭建一个3台VM的CentOS 7集群，安装mpi，blas和hpl，执行linpack测试集群的计算性能。&lt;br&gt;尝试编译netlib hpl 2.2时遇到一个错误，导致不能使用mpi。&lt;br&gt;后来发现可以直接使用Intel编译好的xhpl。由于不理解mpi和xhpl的选项，集群的性能比单机还明显低得多，还没找到原因;-(&lt;br&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>再说docker及云中的网络连接</title>
    <link href="https://ying-zhang.github.io/cloud/2017/vm-net-2/"/>
    <id>https://ying-zhang.github.io/cloud/2017/vm-net-2/</id>
    <published>2017-04-07T16:00:00.000Z</published>
    <updated>2017-10-30T03:41:43.318Z</updated>
    
    <content type="html"><![CDATA[<p>之前写过一篇<a href="https://ying-zhang.github.io/cloud/2016/vm-net/">关于虚拟机和docker网络的日志</a>，主要介绍的是VM的虚拟网络，顺带提了一下docker的桥接。<br>经过2016年的几次版本升级，docker的网络功能有了很大的改善，已经基本稳定可用了。<br>去年<a href="https://github.com/NAP-GHJ/NetTool/blob/master/blog/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C.md" target="_blank" rel="external">与一个大四做毕设的学弟折腾过一段时间docker网络</a>，这里参考浙大SEL实验室的《Docker容器与容器云 第2版》（下面简称《容器云》书） <strong>4.2节 Docker高级网络实战</strong> ，整理一下相关内容。</p><a id="more"></a><hr><!-- TOC --><pre><code>- [title: 再说docker及云中的网络连接](#title-%E5%86%8D%E8%AF%B4docker%E5%8F%8A%E4%BA%91%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5)</code></pre><ul><li><a href="#%E5%86%8D%E8%AF%B4docker%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F%EF%BC%8C%E8%B7%AF%E7%94%B1%E5%99%A8%EF%BC%8Cnat%EF%BC%8C%E4%BA%A4%E6%8D%A2%E6%9C%BA">再说Docker桥接模式，路由器，NAT，交换机</a></li><li><a href="#docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%9A%84%E7%BD%91%E7%BB%9C">Docker跨主机的网络</a></li><li><a href="#%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6">跨主机网络的实现机制</a></li><li><a href="#macvlan">macvlan</a></li><li><a href="#overlay%E7%BD%91%E7%BB%9C">overlay网络</a></li><li><a href="#%E5%AF%B9%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E7%9A%84%E9%9C%80%E6%B1%82">对容器网络的需求</a></li><li><a href="#%E5%85%B6%E5%AE%83">其它</a></li><li><a href="#%E8%84%91%E6%B4%9E">脑洞</a></li></ul><!-- /TOC --><h1 id="再说Docker桥接模式，路由器，NAT，交换机"><a href="#再说Docker桥接模式，路由器，NAT，交换机" class="headerlink" title="再说Docker桥接模式，路由器，NAT，交换机"></a>再说Docker桥接模式，路由器，NAT，交换机</h1><p><a href="https://ying-zhang.github.io/cloud/2016/vm-net/index.html#docker-bridge">上篇日志提到Docker的桥接模式（bridge）实际上是NAT方式</a>，也给了两个设置Docker“名副其实”的bridge模式的链接，这点在《容器云》的 “4.2.2 pipework 原理解析：1. 将Docker配置到本地网络环境中” 一节也提到了。<br>实际为NAT的docker bridge模式和类似VM的bridge模式都是用到了Linux bridge虚拟网桥。这个虚拟设备其实既是一个 <strong>虚拟路由器</strong>，也是一个 <strong>虚拟交换机</strong>，因为（家用）路由器的一侧就包括了一个交换机。<br>参考下图。<br><img src="/img/wifi-router.png" alt="家用无线路由器拓扑"></p><ul><li>如果把墙上的网口用网线接到路由器的 <strong>WAN口</strong>，把家里的PC，手机等接到路由器的 <strong>有线LAN口</strong> 或 <strong>wifi</strong>，这是路由器的正常工作模式，即 <strong>NAT模式</strong>，类似于 <strong>docker的bridge模式</strong>。</li><li>如果稍稍开一下脑洞，把网线接到 <strong>有线LAN口中的任意一个</strong>，留着WAN口什么也不接，这时PC，手机等设备还是能连到网络上的！这时只用了路由器的LAN一侧，只工作在 <strong>交换机</strong> 模式，这类似于 <strong>VM的bridge模式</strong>。其实交换机的功能就是把一个网络端口变成多个网络端口。</li></ul><h1 id="Docker跨主机的网络"><a href="#Docker跨主机的网络" class="headerlink" title="Docker跨主机的网络"></a>Docker跨主机的网络</h1><p>上面强调docker的bridge模式名不副实，其实是因为docker早期版本缺少跨主机的网络功能，造成了诸多不便。在“传统的”VM技术中，是使用bridge模式来实现不同物理主机上VM直接联网的，即将VM网络接入到主机网络环境中。docker虽然用了同样的称呼，但没有提供同样的功能，除了端口映射和host模式，没有办法方便地将不同物理主机上的容器互相连通，让docker的用户非常痛苦。<br>多个第三方工具，比如pipework、weave、socketplane、flannel，calico等都是为了实现docker的跨主机网络。最终，在docker 1.9，docker提供了内置的Overlay网络模式。</p><p>回顾一下docker支持的网络模式 <a href="https://docs.docker.com/engine/userguide/networking/" target="_blank" rel="external">https://docs.docker.com/engine/userguide/networking/</a> ：</p><ul><li>none：docker撒手不管，由用户或第三方工具提供网络功能，上节中pipework就是这样的工具；</li><li>host：直接使用主机的网络栈，即没有网络隔离；</li><li>bridge：NAT模式，容器可以使用主机的网络访问外部，但外部访问容器的应用需要做端口映射，即在<code>docker run</code>命令中提供<code>-p</code>参数</li><li>overlay, gwbridge：这是ver 1.9引入的覆盖网络，下面会单独介绍。</li></ul><blockquote><p>docker还有一种<code>container</code>模式，使用已有容器的网络。Kubernetes的Pod是依赖于这种网络模式的：一个Pod包括多个功能相关的容器，它们共用一个网络栈，是 <a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause" target="_blank" rel="external"><strong>史上最小的docker镜像</strong> <code>pause</code> </a>创建的。<br>新版本（我使用的17.05.0-ce版）的docker使用这种模式的命令参数是<code>docker run --net container:&lt;network-container-name&gt; &lt;image&gt; &lt;entrypoint&gt;</code></p></blockquote><h1 id="跨主机网络的实现机制"><a href="#跨主机网络的实现机制" class="headerlink" title="跨主机网络的实现机制"></a>跨主机网络的实现机制</h1><p>跨主机网络需要处理：</p><ul><li>容器和主机网络的拓扑：即哪个容器子网对应于哪台物理主机。拓扑信息的源头当然是容器启动命令，可以通过标准路由协议在各主机的网络组件守护进程之间交换信息，也可以将拓扑信息写入etcd这样的高可用集中存储。</li><li>数据包的转发：iptables，VXLAN等。</li><li>子网隔离：对跨主机的网络，除了实现不同主机上的容器之间能够联网互通，还要能 <strong>不通</strong>，属于不同子网的容器之间不能通讯。</li></ul><p>不同的实现机制，在功能和性能上有所区别：</p><ul><li>将容器置于主机网络中：类似上面提到的VM的bridge模式，及macvlan方式；这种方式需要 <strong>外部机制</strong> 来支持子网，一般是传统的 <strong>VLAN</strong>，即交换机端口设置不同的VLAN ID；VLAN的12位长度限制了子网的数量，对公有云平台是一个限制，但对企业内的私有云一般是够用了。</li><li>路由转发：如<code>calico</code>，仍然使用docker的bridge模式，但不再使用NAT，而是 <strong>路由转发</strong>；NAT的出现是解决私有子网（<code>10.0.0.0/8， 172.16.0.0/12， 192.168.0.0/24</code>），所以路由器对这几个IP网段特别处理了。如果把这3个网段当成普通的IP网段，容器网络和主机网络就跟普通的多层IP网络是一样的（当然主机可以直接通讯，多数情况不必经过物理的核心路由器）。这样 <strong>虚拟交换机</strong> 就不够用了， 需要 <strong>虚拟路由器</strong>，<code>iptables</code> 就是这样一个虚拟路由器（软件路由器，也被作为防火墙）。可以手动添加iptables的路由条目；也可以使用工具自动化这个过程，比如通过标准的路由协议，或开发非标准的方式。</li><li>overlay网络：docker内置了overlay支持，CoreOS的flannel也是早期比较常用的overlay网络工具，Kubernetes就是使用的flannel。overlay网络是将容器的数据包封装起来，当成普通的数据，到目的主机后再拆开，转发给对应的目的容器。</li></ul><blockquote><p>路由转发和overlay方式都有一个 <strong>限制</strong>：每个物理主机上的容器子网必须在 <strong>不同</strong> 的网段。因为这2种方式都会用到docker网桥（docker0），网桥会聪明地通过子网掩码识别哪些数据包是它负责的本地LAN，只有不是同一子网的数据包才会从这个网桥发出去。<br>不过这个限制可以被绕过去。比如从calico或flannel的设置看起来，不同主机上的容器子网是在同一个IP网段的（比如<code>10.0.0.0/16</code>），但实际上每个主机上的容器子网是更细的网段，比如主机A上是<code>10.0.1.0/24</code>，主机B上是<code>10.0.2.0/24</code>，甚至使用了<code>10.0.0.1/32</code>这样的“子网”。</p><p>早期docker在每个主机上只能设置一个bridge，同一主机上的容器彼此无法隔离。目前版本的docker可以设置多个网络。</p></blockquote><h1 id="macvlan"><a href="#macvlan" class="headerlink" title="macvlan"></a>macvlan</h1><p>Linux bridge是一个软件实现的虚拟网桥，而macvlan则利用了网卡硬件的支持。<br>目前的docker内置了macvlan支持。参考<a href="https://docs.docker.com/engine/userguide/networking/get-started-macvlan/" target="_blank" rel="external">docker的macvlan文档</a> 和 <a href="https://github.com/alfredhuang211/study-docker-doc/blob/master/docker%E8%B7%A8%E4%B8%BB%E6%9C%BAmacvlan%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE.md" target="_blank" rel="external">数人云的一篇文档 - docker跨主机macvlan网络配置</a>，按下面的例子可以设置macvlan：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 主机IP为10.1.1.10/24，网卡名称ens33，首先开启网卡的混杂模式</div><div class="line">ip link set ens33 promisc on</div><div class="line"></div><div class="line"># 创建名为macvlan_net的docker网络</div><div class="line">docker network create -d macvlan --subnet=10.1.1.0/24 --gateway=10.1.1.2 -o parent=ens33 macvlan_net</div><div class="line"></div><div class="line"># 运行容器时指定或动态分配IP。注意，动态分配的IP可能与主机IP冲突</div><div class="line">docker run -ti --net macvlan_net --ip=10.1.1.101  centos:net /bin/bash</div><div class="line">docker run -ti --net macvlan_net centos:net /bin/bash</div></pre></td></tr></table></figure></p><p>上面使用的<code>centos:net</code>镜像是在centos:7基础上安装了<code>iproute</code>和<code>net-tools</code>软件包，以在镜像内提供<code>ip</code>，<code>ifconfig</code>等网络管理命令。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># Dockerfile</div><div class="line">FROM centos:7</div><div class="line">RUN yum install net-tools iproute -y</div><div class="line"></div><div class="line"># docker build . -t centos:net</div></pre></td></tr></table></figure></p><p>按上面的命令创建的2个容器彼此可以pin通，也可以ping通网关和其它主机，但 <strong>不能ping同本主机</strong>，这是macvlan本身的限制（摊手）。macvlan有4种工作模式，可以参考<a href="https://cizixs.github.io/2017/02/14/network-virtualization-macvlan" target="_blank" rel="external">linux 网络虚拟化： macvlan</a>、<a href="https://www.ibm.com/developerworks/cn/linux/1312_xiawc_linuxvirtnet/" target="_blank" rel="external">Linux 上虚拟网络与真实网络的映射</a>和<a href="http://backreference.org/2014/03/20/some-notes-on-macvlanmacvtap/" target="_blank" rel="external">Some notes on macvlan/macvtap</a>，分别为VEPA（不常用，需交换机支持hairpin模式，又称reflective relay反射中继），桥接，私有和Passthru（直通）。<br>使用macvlan网络的容器被置于主机网络中。子网隔离需要VLAN，但实际上同一主机上的容器都使用主机的网线，对应的是交换机的同一个端口，也就是同一个VLAN ID，所以需要macvlan作为一个虚拟交换机，也支持设置VLAN ID。这方面可以参考 <a href="http://t.cn/RXpdDrf" target="_blank" rel="external">基于macvlan的Docker容器网络系统的设计与实现 - 浙江大学硕士学位论文 - 万方</a> 和 <a href="/doc/Virtual_switching_technologies_and_Linux_bridge_ppt.pdf">Virtual switching technologies and Linux bridge - ppt</a>。</p><h1 id="overlay网络"><a href="#overlay网络" class="headerlink" title="overlay网络"></a>overlay网络</h1><p>flannel开始是自己实现的overlay机制，将IP包封装到UDP的数据段，即IP in IP，需要拦截并应答容器的ARP包；后来加入了VXLAN的转发方式。VXLAN是将完整的二层以太网包封装到UDP的数据段，即MAC in IP，提供了完整的虚拟二层网络，而且VXLAN是内核支持的，性能好得多。<br>早期版本flannel的配置可以参考<a href="http://dockone.io/article/618" target="_blank" rel="external">一篇文章带你了解Flannel</a>。<br>关于<a href="https://tools.ietf.org/html/rfc7348" target="_blank" rel="external">VXLAN</a>，从名字上看似乎是VLAN的扩展，但它跟VLAN的实现方式有很大的差别。<br>VLAN是二层以太网包的一个段。对VLAN的支持和VLAN ID是在交换机上设置的（针对交换机各端口，计算机上不需要特别设置）。虽然把VLAN从12位拓展到24位似乎比较简单直接，但需要 <strong>升级交换机</strong> 才能支持新的以太网数据包格式。所以VXLAN把扩展的24位VLAN ID放到了UDP的数据段。VXLAN数据包的处理是通过虚拟的VTEP网卡实现的，实际上是发生在主机上，对交换机而言是透明的。交换机要支持VXLAN还比较复杂些，看起来VXLAN是为主机上的虚拟交换机量身定制的。VXLAN的下层网络可以手动设置点对点的拓扑，也支持通过组播自动发现并组网，不过这样就比较复杂了。<br><img src="/img/vnet-vxlan.png" alt="">。</p><p>VXLAN设计的一个应用场景是在不同的三层网络（如多个数据中心）之上（即overlay）建立一个虚拟的二层网络，即所谓的“大二层”。大二层的需求是为了应对虚拟机的在线迁移。虚拟机一般使用桥接模式，与主机有相同的网络环境。当VM迁移到另一个主机后，VM的MAC和IP应保持不变，以便减少对VM内应用和其用户的影响（虽然VM的MAC和IP没有变化，但新主机对应于交换机的端口发生了变化，用户会经历一小段时间的网络中断，以等待交换机端口学习新的ARP映射）。如果不同主机在不同的VLAN甚至跨数据中心，那么迁移后VM将无法与原VLAN的节点通讯。所以在线迁移需要没有隔离的大二层网络，但这样会造成广播风暴。点对点的VXLAN overlay网络可以比较好的解决这个矛盾。<br>docker出现后，面临的跨主机网络与大二层需求不同但也有类似之处，docker内置的overlay网络就使用了VXLAN转发。</p><blockquote><p>VLAN作为传统的子网隔离机制，是工作在二层的。除了VLAN ID不同，每个VLAN的IP网段也不同，即三层通过IP子网来隔离。VLAN之间如果需要通讯，则需设置经路由转发。</p></blockquote><p>VXLAN是由VMware为主提出来的，微软提出了类似的<a href="https://tools.ietf.org/html/rfc7637" target="_blank" rel="external">NVGRE</a>。NVGRE的数据包格式与VXLAN相比向下兼容性不够好，需要升级设备才能应用。</p><blockquote><p>参考</p><ul><li><a href="http://support.huawei.com/huaweiconnect/enterprise/thread-334207.html" target="_blank" rel="external">【华为悦读汇】技术发烧友：认识VXLAN</a></li><li><a href="http://support.huawei.com/huaweiconnect/enterprise/thread-333013.html" target="_blank" rel="external">【华为悦读汇】技术发烧友：闲话大二层网络</a></li><li><a href="https://www.zhihu.com/question/24393680" target="_blank" rel="external">Overlay 网络技术，最想解决什么问题？</a></li><li><a href="http://t.cn/RXpdege" target="_blank" rel="external">基于容器云平台的网络资源管理与配置系统 - 浙江大学硕士论文 - 万方</a></li></ul></blockquote><h1 id="对容器网络的需求"><a href="#对容器网络的需求" class="headerlink" title="对容器网络的需求"></a>对容器网络的需求</h1><ul><li>提供类似传统网络的体验<ul><li>VPS（Virtual Private Server）× n + 虚拟网络 = VPC（Virtual Private Cloud）：不同租户的子网彼此隔离，租户可以指定或被分配IP网段，DHCP或静态指定IP，关联公网IP以便与互联网连通；</li><li>租户可以有多个子网，设置虚拟路由器；</li><li>安全组，防火墙，负载均衡，DNS；</li></ul></li><li>性能：高带宽，低延迟，扩展性。VXLAN和calico是目前性能比较好的2种技术。</li><li>容器与物理主机，虚拟机互联共存：这一点目前还没有比较好的实现。</li></ul><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p>关于Calico，可参考<a href="http://docs.projectcalico.org/v2.1/getting-started/docker/" target="_blank" rel="external">其官网</a>和<a href="http://blog.dataman-inc.com/shurenyun-docker-133/" target="_blank" rel="external">将Docker网络方案进行到底</a>。<br>关于不同网络方式的性能，可参考豆瓣上（是的，豆瓣）的这篇<a href="https://www.douban.com/note/530365327/" target="_blank" rel="external">Docker network on cloud 中文</a>或者 <a href="https://cmgs.me/life/docker-network-cloud" target="_blank" rel="external">https://cmgs.me/life/docker-network-cloud</a> 。这里盗个图。<br><img src="/img/vnet-pk.png" alt=""></p><h1 id="脑洞"><a href="#脑洞" class="headerlink" title="脑洞"></a>脑洞</h1><p>上面的一堆方案，有的自称为SDN，但实际与SDN还有些差距。SDN是针对网络设备的，而这些方案多是在服务器主机上实现虚拟网络设备（交换机，路由器），更接近NFV的目标。<br><strong>被上面一堆方案搞得头痛的，不妨看看这样一个有意思的想法：<a href="https://coreos.com/blog/jumpers-and-the-software-defined-localhost.html" target="_blank" rel="external">Jumpers and the Software Defined Localhost</a>：容器中只有一个loop（127.0.0.1）网卡！完全由外部来管理容器网络。</strong><br>隔离是肯定没问题，都不需要子网的概念了，但都只有一个<code>127.0.0.1</code>的IP，如何与其它容器通讯呢？jumpers使用的是端口，域名应该更好些，Docker 已经为每个容器内置了一个<a href="https://www.infoq.com/news/2016/08/docker-service-load-balancing" target="_blank" rel="external">DNS（127.0.0.11）</a>来帮助实现服务发现。</p><p><a href="http://dockone.io/article/2504" target="_blank" rel="external">DockOne微信分享（一三零）：探究PaaS网络模型设计</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前写过一篇&lt;a href=&quot;https://ying-zhang.github.io/cloud/2016/vm-net/&quot;&gt;关于虚拟机和docker网络的日志&lt;/a&gt;，主要介绍的是VM的虚拟网络，顺带提了一下docker的桥接。&lt;br&gt;经过2016年的几次版本升级，docker的网络功能有了很大的改善，已经基本稳定可用了。&lt;br&gt;去年&lt;a href=&quot;https://github.com/NAP-GHJ/NetTool/blob/master/blog/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;与一个大四做毕设的学弟折腾过一段时间docker网络&lt;/a&gt;，这里参考浙大SEL实验室的《Docker容器与容器云 第2版》（下面简称《容器云》书） &lt;strong&gt;4.2节 Docker高级网络实战&lt;/strong&gt; ，整理一下相关内容。&lt;/p&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Cobbler搭建PXE服务器</title>
    <link href="https://ying-zhang.github.io/misc/2017/cobbler-pxe/"/>
    <id>https://ying-zhang.github.io/misc/2017/cobbler-pxe/</id>
    <published>2017-03-21T16:00:00.000Z</published>
    <updated>2017-10-30T02:48:03.321Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zh.wikipedia.org/wiki/%E9%A2%84%E5%90%AF%E5%8A%A8%E6%89%A7%E8%A1%8C%E7%8E%AF%E5%A2%83" target="_blank" rel="external">PXE</a> （Preboot eXecution Environment，预执行环境）是通过 <strong>局域网</strong> 来启动计算机（和安装操作系统）的技术。<br>一般是通过刻录到光驱或U盘的Live CD这样的本地存储来安装系统的，要通过网络来安装系统，首先要知道安装文件存放的服务器（TFTP服务器，Trivial File Transfer Protocol，精简FTP），而系统启动时网卡的IP都还没有。所以PXE必须要有一个 <strong>DHCP（Dynamic Host Configuration Protocol，动态主机设置协议）</strong>，不但负责为机器分配 IP地址，还会告知安装文件所在的服务器的 IP地址。<br>Cobbler简化了安装配置DHCP、TFTP、关联Kickstart应答文件等搭建PXE服务器的过程。</p><a id="more"></a><hr><!-- TOC --><ul><li><a href="#%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83">测试环境</a></li><li><a href="#cobbler%E8%AE%BE%E7%BD%AE%E6%AD%A5%E9%AA%A4">Cobbler设置步骤</a><ul><li><a href="#%E7%A6%81%E7%94%A8selinux">禁用SELinux</a></li><li><a href="#%E7%A6%81%E7%94%A8%E9%98%B2%E7%81%AB%E5%A2%99">禁用防火墙</a></li><li><a href="#%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6">安装软件</a></li><li><a href="#%E8%AE%BE%E7%BD%AE%E4%B8%BA%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8%E7%9A%84%E6%9C%8D%E5%8A%A1">设置为开机启动的服务</a></li><li><a href="#%E7%94%9F%E6%88%90%E5%8A%A0%E5%AF%86%E7%9A%84root%E5%AF%86%E7%A0%81">生成加密的root密码</a></li></ul></li><li><a href="#%E4%BF%AE%E6%94%B9cobbler%E7%9A%84%E9%85%8D%E7%BD%AE">修改cobbler的配置</a><ul><li><a href="#%E4%BF%AE%E6%94%B9xinetd-tftp%E7%9A%84%E9%85%8D%E7%BD%AE">修改xinetd tftp的配置</a></li><li><a href="#%E4%BF%AE%E6%94%B9dhcp%E9%85%8D%E7%BD%AE">修改dhcp配置</a></li><li><a href="#%E4%BD%BF%E7%94%A8-dnsmasq-%E6%8F%90%E4%BE%9B-dhcp-%E6%9C%8D%E5%8A%A1">使用 dnsmasq 提供 dhcp 服务</a></li><li><a href="#%E5%90%AF%E5%8A%A8%E7%9B%B8%E5%85%B3%E6%9C%8D%E5%8A%A1">启动相关服务</a></li><li><a href="#%E6%89%A7%E8%A1%8C-cobbler-check">执行 <code>cobbler check</code></a></li><li><a href="#%E5%AF%BC%E5%85%A5centos%E7%B3%BB%E7%BB%9F%E7%9A%84iso%E5%AE%89%E8%A3%85%E9%95%9C%E5%83%8F">导入CentOS系统的iso安装镜像</a></li><li><a href="#%E4%BF%AE%E6%94%B9kickstarts%E6%96%87%E4%BB%B6">修改Kickstarts文件</a></li><li><a href="#%E6%9B%B4%E6%96%B0%E8%AE%BE%E7%BD%AE%EF%BC%8C%E6%9C%80%E5%90%8E%E7%9A%84%E6%A3%80%E6%9F%A5">更新设置，最后的检查</a></li></ul></li><li><a href="#%E5%9C%A8%E5%85%B6%E5%AE%83%E6%9C%BA%E5%99%A8%E4%BD%BF%E7%94%A8pxe%E5%AE%89%E8%A3%85%E7%B3%BB%E7%BB%9F">在其它机器使用PXE安装系统</a></li><li><a href="#%E5%9C%A8docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%BF%90%E8%A1%8Ccobbler">在docker容器中运行cobbler</a><ul><li><a href="#docker%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8systemd">docker容器使用systemd</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E5%AE%B9%E5%99%A8%E7%9A%84%E5%87%A0%E4%B8%AA%E5%9D%91">使用容器的几个坑</a></li></ul></li><li><a href="#%E7%9C%9F%E6%9C%BA%E4%B8%8A%E9%83%A8%E7%BD%B2%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98">真机上部署遇到的问题</a><ul><li><a href="#%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E7%BD%91%E5%8D%A1">选择合适的网卡</a></li><li><a href="#dell-idrac">Dell iDRAC</a></li></ul></li><li><a href="#%E5%8F%82%E8%80%83">参考</a></li></ul><!-- /TOC --><h1 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h1><p>虚拟机使用NAT网络，IP段10.1.1.0，子网掩码：255.255.255.0，DNS/DHCP/Gateway：10.1.1.2。</p><p>新建一个虚拟机，安装CentOS 7，使用的镜像是 <strong><a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1611.iso" target="_blank" rel="external">CentOS-7-x86_64-Minimal-1611.iso</a></strong> 。因为是Minimal的，不必选择附加的软件包。<br>安装后登录系统，查看IP地址<code>ip a</code>，可以通过ssh登录到VM。<br>关闭VM，拍摄一个快照。</p><ul><li>IP ： 10.1.1.10</li><li>子网掩码：255.255.255.0</li><li>用户：root</li></ul><h1 id="Cobbler设置步骤"><a href="#Cobbler设置步骤" class="headerlink" title="Cobbler设置步骤"></a>Cobbler设置步骤</h1><blockquote><p>因为虚拟机已经加载了iso镜像，可以在Guest OS中直接挂载。对远程的服务器，可以配置的同时把iso镜像拷贝到远程机器上去，或者使用共享文件，节省一点拷贝文件的时间。</p></blockquote><h2 id="禁用SELinux"><a href="#禁用SELinux" class="headerlink" title="禁用SELinux"></a>禁用SELinux</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sed -i &apos;s/SELINUX\=enforcing/SELINUX\=disabled/g&apos; /etc/selinux/config</div><div class="line">setenforce 0</div></pre></td></tr></table></figure><h2 id="禁用防火墙"><a href="#禁用防火墙" class="headerlink" title="禁用防火墙"></a>禁用防火墙</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl disable firewalld</div><div class="line">systemctl stop firewalld</div></pre></td></tr></table></figure><h2 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum -y install epel-release</div><div class="line">yum -y install cobbler dhcp httpd xinetd pykickstart fence-agents</div></pre></td></tr></table></figure><h2 id="设置为开机启动的服务"><a href="#设置为开机启动的服务" class="headerlink" title="设置为开机启动的服务"></a>设置为开机启动的服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">systemctl enable cobblerd dhcpd httpd rsyncd tftp xinetd</div></pre></td></tr></table></figure><blockquote><p>PXE 需要从 dhcp服务器获取新的IP地址，以及TFTP，HTTP服务器的IP地址， TFTP服务器提供操作系统的安装文件，HTTP服务器提供Kickstart应答文件。<br>cobbler_web 是cobbler的设置界面，跟HTTP服务不是一回事，可不必安装。</p></blockquote><h2 id="生成加密的root密码"><a href="#生成加密的root密码" class="headerlink" title="生成加密的root密码"></a>生成加密的root密码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">openssl passwd -1 -salt &quot;centos&quot; &quot;centos&quot;</div><div class="line"># 输出，由于salt和密码都是指定的，这段加密的字符串也就是确定的了</div><div class="line">$1$centos$Uq6E6Wp5SDZYbs6MCmamP0</div></pre></td></tr></table></figure><h1 id="修改cobbler的配置"><a href="#修改cobbler的配置" class="headerlink" title="修改cobbler的配置"></a>修改cobbler的配置</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">vi /etc/cobbler/settings</div><div class="line"># 改动 的内容如下</div><div class="line">default_password_crypted: &quot;$1$centos$Uq6E6Wp5SDZYbs6MCmamP0&quot;</div><div class="line">manage_dhcp: 1</div><div class="line">next_server: 10.1.1.10</div><div class="line">server: 10.1.1.10</div></pre></td></tr></table></figure><blockquote><p><a href="/doc/cobbler-setting.txt">修改后的完整 settings 文件 (删除了注释)</a></p></blockquote><h2 id="修改xinetd-tftp的配置"><a href="#修改xinetd-tftp的配置" class="headerlink" title="修改xinetd tftp的配置"></a>修改xinetd tftp的配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">vi /etc/xinetd.d/tftp</div><div class="line"># 将 disable = yes 改为  disable = no</div></pre></td></tr></table></figure><h2 id="修改dhcp配置"><a href="#修改dhcp配置" class="headerlink" title="修改dhcp配置"></a>修改dhcp配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">vi /etc/cobbler/dhcp.template</div><div class="line"># 改动的内容包括 子网段，分配的IP区间，</div><div class="line"># 以及 默认网关（路由器），DNS（默认网关和DNS的设置不影响PXE装机过程，只是新装的机器启动后可能无法访问外网）</div><div class="line"># $next-server 是指向 /etc/cobbler/settings 中的对应值</div><div class="line">#</div><div class="line">subnet 10.1.1.0 netmask 255.255.255.0 &#123;</div><div class="line">     option routers             10.1.1.2;</div><div class="line">     option domain-name-servers 10.1.1.2;</div><div class="line">     option subnet-mask         255.255.255.0;</div><div class="line">     range dynamic-bootp        10.1.1.100 10.1.1.110;</div><div class="line">     default-lease-time         21700;</div><div class="line">     max-lease-time             43100;</div><div class="line">     next-server                $next_server;</div><div class="line"></div><div class="line">     class &quot;pxeclients&quot; &#123;</div><div class="line">          match if substring (option vendor-class-identifier, 0, 9) = &quot;PXEClient&quot;;</div><div class="line">          if option pxe-system-type = 00:02 &#123;</div><div class="line">              filename &quot;ia64/elilo.efi&quot;;</div><div class="line">          &#125; else if option pxe-system-type = 00:06 &#123;</div><div class="line">              filename &quot;grub/grub-x86.efi&quot;;</div><div class="line">          &#125; else if option pxe-system-type = 00:07 &#123;</div><div class="line">              filename &quot;grub/grub-x86_64.efi&quot;;</div><div class="line">          &#125; else &#123;</div><div class="line">              filename &quot;pxelinux.0&quot;;</div><div class="line">          &#125;</div><div class="line">     &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>如果遇到DHCP服务启动失败，可能是 dhcpd 先于 cobblerd 启动，导致 cobbler 来不及设置 dhcpd ，dhcpd.conf 配置文件还是空的。<br>遇到这种情况，可以将 <code>/etc/cobbler/settings</code> 中的 <code>manage_dhcp: 1</code> 改为 <code>0</code>， 然后手动管理 dhcp：修改 <code>/etc/dhcp/dhcpd.conf</code>，格式如下。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">subnet 10.1.1.0 netmask 255.255.255.0 &#123;</div><div class="line">        option routers 10.1.1.2;</div><div class="line">        option domain-name-servers 10.1.1.2;</div><div class="line">        option subnet-mask 255.255.255.0;</div><div class="line">        default-lease-time 21600;</div><div class="line">        max-lease-time 43200;</div><div class="line">        range 10.1.1.100 10.1.1.110;</div><div class="line">        next-server 10.1.1.10;</div><div class="line">        filename &quot;pxelinux.0&quot;;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>注意要把 <code>$next_server</code> 设置为 <strong>具体的 IP地址</strong>，这个配置项是 PXE 的关键。<br><!--这样的一个好处是可以设置 **多个** `subnet {...}` 段，以针对不同的网络环境，dhcpd会自动将 subnet段 与系统网卡的配置匹配，忽略不匹配的设置。很可惜 cobbler 的 next_server 只能指定一个 IP， 换了机器还要修改设置。--><br>如果dhcp服务器（也就是cobbler的服务器）有多个网卡，上面dhcp的配置项与哪个网卡的IP段匹配，就在这个网卡所在的局域网上提供 dhcp 服务。如果没有找到任何匹配的网卡， dhcpd 会报错退出。<br>如果所在的局域网已经有其它的dhcp服务器，那么会存在竞争，可以考虑使用 dnsmasq。<br><code>/etc/cobbler/settings</code> 中 <code>next_server</code> 和 <code>server</code> 指定的IP地址要和DHCP的网段在 <strong>同一个局域网段</strong> 才能正常工作。</p><h2 id="使用-dnsmasq-提供-dhcp-服务"><a href="#使用-dnsmasq-提供-dhcp-服务" class="headerlink" title="使用 dnsmasq 提供 dhcp 服务"></a>使用 dnsmasq 提供 dhcp 服务</h2><p>参考<a href="http://cobbler.github.io/manuals/2.8.0/3/4/1_-_Managing_DHCP.html" target="_blank" rel="external">Managing DHCP - cobbler manual</a>，如果 dhcpd 无法正确配置，可以使用 dnsmasq。首先需要安装： <code>yum install -y dnsmasq</code>。<br>如果让cobbler来配置dnsmasq，需要设置<code>/etc/cobbler/settings</code> 中为 <code>manage_dhcp: 1</code>，<br>然后修改 <code>/etc/cobbler/modules.conf</code>，将<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[dhcp]</div><div class="line">module = manage_isc   # isc 即 dhcpd</div><div class="line"># 改为</div><div class="line">[dhcp]</div><div class="line">module = manage_dnsmasq</div></pre></td></tr></table></figure></p><p>然后修改 <code>/etc/cobbler/dnsmasq.template</code>，dnsmasq的配置比较简单，只要修改IP区间即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dhcp-range=10.1.1.100,10.1.1.110</div></pre></td></tr></table></figure></p><blockquote><p>注意 dhcpd 与 dnsmasq 的区间格式不同，配置文件的格式错误会导致服务无法启动。<br>如果只是临时提供 dhcp 服务，可设置比较小的区间，<strong>特别要避免与已有的重要服务器发生 IP地址 冲突</strong>！</p></blockquote><h2 id="启动相关服务"><a href="#启动相关服务" class="headerlink" title="启动相关服务"></a>启动相关服务</h2><blockquote><p>注意： 重启服务器后，需要确认下面的几项服务是否正常启动。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">systemctl start cobblerd dhcpd httpd rsyncd tftp xinetd</div></pre></td></tr></table></figure><h2 id="执行-cobbler-check"><a href="#执行-cobbler-check" class="headerlink" title="执行 cobbler check"></a>执行 <code>cobbler check</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 输出如下，可见只有一项问题，因为安装的是CentOS系统，可以忽略这一项。</div><div class="line">The following are potential configuration items that you may want to fix:</div><div class="line">1 : debmirror package is not installed, it will be required to manage debian deployments and repositories</div><div class="line">Restart cobblerd and then run &apos;cobbler sync&apos; to apply changes.</div></pre></td></tr></table></figure><ul><li>如果提示需下载额外的boot loader，可执行 <code>cobbler get-loaders</code> ，这 <strong>不是必须的</strong>，因为已经自带了常用系统的loader。如果确实要下载，且要使用代理的话，<code>/etc/cobbler/settings</code> 中可以在 <code>proxy_url_ext</code> 设置代理地址。</li><li>如果输出中说 httpd无法访问 或 SELinux 没有关闭，执行<code>systemctl status httpd</code> 查看，还可以通过访问 <a href="http://10.1.1.10" target="_blank" rel="external">http://10.1.1.10</a> （next_server的IP地址）来确认.可能是 <code>/etc/cobbler/settings</code> 中 <code>next_server</code> 或 <code>server</code> 的 IP地址 设置错误，改正IP地址后尝试重启 httpd。</li></ul><p>再次执行 <code>cobbler check</code>，并检查相关的服务是否正常启动，然后继续执行下面的步骤。</p><h2 id="导入CentOS系统的iso安装镜像"><a href="#导入CentOS系统的iso安装镜像" class="headerlink" title="导入CentOS系统的iso安装镜像"></a>导入CentOS系统的iso安装镜像</h2><p>虚拟机中已经为Guest OS加载了iso文件到<code>/dev/cdrom</code>，需要再挂载到<code>/mnt</code>：<code>mount -t auto -o loop,ro /dev/cdrom /mnt</code>；<br>也可以直接挂载iso文件：<code>mount -t iso9660 -o loop,ro /your/path/to/CentOS-7-x86_64-Minimal-1611.iso /mnt</code>。</p><p>导入安装镜像：<code>cobbler import --name=centos --arch=x86_64 --path=/mnt</code>。<br>等几分钟才能执行完import，因为这一步把安装光盘拷贝到了 <code>/var/www/cobbler/ks_mirror/</code></p><p>查看导入的项目：<code>cobbler profile list</code><br>输出为 centos-x86_64，进一步查看，<code>cobbler profile report --name=centos-x86_64</code>。</p><h2 id="修改Kickstarts文件"><a href="#修改Kickstarts文件" class="headerlink" title="修改Kickstarts文件"></a>修改Kickstarts文件</h2><p>从上面的命令输出可知使用的Kickstart文件在 <code>/var/lib/cobbler/kickstarts/sample_end.ks</code><br>修改下面2项</p><ul><li><code>firewall --enable</code>            改为 <code>firewall --disable</code></li><li><code>timezone  America/New_York</code>   改为 <code>timezone  Asia/Shanghai</code></li></ul><h2 id="更新设置，最后的检查"><a href="#更新设置，最后的检查" class="headerlink" title="更新设置，最后的检查"></a>更新设置，最后的检查</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cobbler sync</div><div class="line">systemctl restart cobblerd</div><div class="line">cobbler check</div></pre></td></tr></table></figure><h1 id="在其它机器使用PXE安装系统"><a href="#在其它机器使用PXE安装系统" class="headerlink" title="在其它机器使用PXE安装系统"></a>在其它机器使用PXE安装系统</h1><p>新建一个虚拟机，不要直接启动，而是通过菜单选择“虚拟机 -&gt; 电源 -&gt; 打开电源时进入固件”，在虚拟机的BIOS中将网络启动设置为第一项，然后按 <code>F10</code> 键保存并重启虚拟机。<br><img src="/img/pxe-boot-config.png" alt=""></p><p>稍等一下，DHCP配置完成后会显示启动项如下：<br><img src="/img/pxe-boot-menu.png" alt=""><br>选择第二项 centos-x86_64，回车，就会开始自动安装。</p><p>稍等一会儿，安装完成后自动重启。可以按上面的步骤，在BIOS中修改默认启动项为本机硬盘。<br>从本机硬盘启动后，以root用户登录，密码就是之前使用openssl设置的密码，查看一下分配的IP地址，可以用ssh登录后继续系统管理操作。</p><blockquote><p>从Cobbler官网的手册来看，它支持profile和system命令，应该是支持多网卡和多种操作系统的Profile等复杂需求的；<br>此外还可以在<a href="/doc/cobbler-setting.txt">settings</a>中设置加入LDAP domain，代理，安装软件包，配置用户ssh key等等功能，以及构建定制的系统iso镜像，<br>在Kickstart文件中也可以完成硬盘分区，安装软件包，配置用户等等功能。<br>水平有限，就不深入这些功能了;-(</p></blockquote><hr><h1 id="在docker容器中运行cobbler"><a href="#在docker容器中运行cobbler" class="headerlink" title="在docker容器中运行cobbler"></a>在docker容器中运行cobbler</h1><h2 id="docker容器使用systemd"><a href="#docker容器使用systemd" class="headerlink" title="docker容器使用systemd"></a>docker容器使用systemd</h2><p>通过 <code>docker pull centos:7</code> 直接pull下来的镜像 <strong>不能使用</strong> <code>systemd</code>， 因为这与 docker 的 <strong>单容器单进程</strong> 哲学不相容~~<br>我们需要自己build一个支持systemd的镜像，参考 <a href="https://hub.docker.com/_/centos/" target="_blank" rel="external">Docker Hub 的 CentOS镜像</a> 或<a href="https://store.docker.com/images/d5052416-4069-4619-8597-ba61df35ba6f" target="_blank" rel="external">Docker Store 的  CentOS</a>的页面中 <strong>Systemd integration</strong> 一节提供的 Dockerfile 即可。</p><p>再此镜像之上再build一个cobbler的镜像，这里偷懒，cobbler的Dockerfile只是安装安装必要的软件（<strong>增加 which 和 curl</strong>），启用相关服务项，其它设置进入容器的shell手工修改。</p><p>假设这个镜像的 tag 为 cobbler:default，启动一个容器，使用</p><ul><li>host 网络，<code>--network host</code></li><li>特权模式，<code>--privileged</code></li><li>挂载cgroup的fs，以使用systemd，<code>-v /sys/fs/cgroup:/sys/fs/cgroup:ro</code></li><li>挂载<code>/mnt</code>，这是已经挂载到主机的CentOS安装文件iso镜像，<code>-v /mnt:/mnt:ro</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker run -d --privileged --network host -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /mnt:/mnt:ro \ </div><div class="line">--name cobbler cobbler:latest /usr/sbin/init</div></pre></td></tr></table></figure><p>因为容器的<code>Entrypoint</code>是<code>/usr/sbin/init</code>，而且是<code>-d</code>，即detached，启动后不会进入shell。<br>可以通过<code>docker exec</code>执行容器的shell，需要增加 <code>-ti</code>选项为shell分配一个终端，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker exec -ti cobbler bash</div></pre></td></tr></table></figure></p><p>退出容器（不会停止容器运行）的快捷键是 Ctrl + P, Q，或在容器的shell中执行<code>exit</code>。</p><p>按上述步骤启动容器，在容器的shell中执行上一节的操作，修改配置，导入iso镜像，检查各项服务是否正常启动。</p><h2 id="使用容器的几个坑"><a href="#使用容器的几个坑" class="headerlink" title="使用容器的几个坑"></a>使用容器的几个坑</h2><blockquote><p>Docker官方的CentOS镜像太精简了，</p><ul><li>没有 <code>curl</code> 及 <code>wget</code>，还少了 <code>which</code>， 也需要装上；</li><li><code>/etc/httpd/logs</code>是一个链接，但应该是一个目录，结果导致 httpd 无法启动。把原来的链接删掉，新建一个<code>/etc/httpd/logs/</code>目录即可；</li><li>没有 <code>/var/log/cobbler/tasks</code> 目录，导致cobbler sync 失败，手动创建该目录。 </li></ul></blockquote><h1 id="真机上部署遇到的问题"><a href="#真机上部署遇到的问题" class="headerlink" title="真机上部署遇到的问题"></a>真机上部署遇到的问题</h1><h2 id="选择合适的网卡"><a href="#选择合适的网卡" class="headerlink" title="选择合适的网卡"></a>选择合适的网卡</h2><p>服务器有2个网卡，BIOS默认只有一个网卡能通过PXE启动机器。一般的机器都是<code>em1</code>，但有的机器需要修改BIOS选择<code>em2</code>网卡才行。</p><h2 id="Dell-iDRAC"><a href="#Dell-iDRAC" class="headerlink" title="Dell iDRAC"></a>Dell iDRAC</h2><p>服务器是Dell的，有2种型号，分别通过iDRAC6 和 iDRAC8 web界面远程管理。它们都可以提供服务器的画面显示，鼠标和键盘控制，并且可以将本地的iso镜像/光驱挂载为远程服务器的虚拟光驱。<br>在使用PXE之前，先要装好一台机器。<br>装机之前，先把iDRAC的固件升级了一下，通过机器的服务标签可以搜索到对应的固件。<br>iDRAC6 的画面显示是通过 jnlp控件 显示的，要安装好 jre，并在控制面板的Java选项（或直接执行<code>javacpl.exe</code>）中添加 <code>安全例外项</code>。jnlp控件都是一次性的，每个会话都要重新下载，还要点击n个安全提示对话框。最好使用IE，使用Chrome出现过安装系统快结束时页面错误，功亏一篑。</p><blockquote><p>iDRAC6 的 jviewer jnlp控件对应的jar包证书比较旧了，如果使用较新的jre，会因为证书过期而无法执行jnlp控件，所以需要安装 java 7 版本的 jre。</p></blockquote><p>通过jnlp安装CentOS不管什么版本都是图形化的安装界面，太耗资源了，先要黑屏等着等传过去一堆文件之后才能显示出安装界面来，在安装界面虽然鼠标指针可以移动，但单击没有反应，键盘也没有反应;-(<br>安装Ubuntu Server版基于的文本安装界面，很快就可以显示出来，可以只用键盘操作。</p><p>旧版iDRAC8 的jnlp同样是有显示但无法操作，好在升级后可以使用 html5 的新界面，而且支持多个会话。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="http://cobbler.github.io/manuals/quickstart/" target="_blank" rel="external">Cobbler Quick Start</a></li><li><a href="http://readshlinux.blog.51cto.com/9322509/1812402" target="_blank" rel="external">Centos7.2安装Cobbler 并安装系统</a></li><li><a href="http://www.linuxtechi.com/install-and-configure-cobbler-on-centos-7/" target="_blank" rel="external">How to Install and Configure Cobbler on CentOS 7.x</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E9%A2%84%E5%90%AF%E5%8A%A8%E6%89%A7%E8%A1%8C%E7%8E%AF%E5%A2%83&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;PXE&lt;/a&gt; （Preboot eXecution Environment，预执行环境）是通过 &lt;strong&gt;局域网&lt;/strong&gt; 来启动计算机（和安装操作系统）的技术。&lt;br&gt;一般是通过刻录到光驱或U盘的Live CD这样的本地存储来安装系统的，要通过网络来安装系统，首先要知道安装文件存放的服务器（TFTP服务器，Trivial File Transfer Protocol，精简FTP），而系统启动时网卡的IP都还没有。所以PXE必须要有一个 &lt;strong&gt;DHCP（Dynamic Host Configuration Protocol，动态主机设置协议）&lt;/strong&gt;，不但负责为机器分配 IP地址，还会告知安装文件所在的服务器的 IP地址。&lt;br&gt;Cobbler简化了安装配置DHCP、TFTP、关联Kickstart应答文件等搭建PXE服务器的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>【译文】Spanner, TrueTime 和CAP理论</title>
    <link href="https://ying-zhang.github.io/cloud/2017/spanner-truetime-cap/"/>
    <id>https://ying-zhang.github.io/cloud/2017/spanner-truetime-cap/</id>
    <published>2017-03-05T16:00:00.000Z</published>
    <updated>2017-10-30T03:41:56.842Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://en.wikipedia.org/wiki/Eric_Brewer_%28scientist%29" target="_blank" rel="external">Eric Brewer，VP, Infrastructure, Google</a><br>2017-02-14<br>英文原文：<a href="https://research.google.com/pubs/pub45855.html" target="_blank" rel="external">Spanner, TrueTime and the CAP Theorem</a> ,<a href="https://research.google.com/pubs/archive/45855.pdf" target="_blank" rel="external">英文全文 PDF</a><br><a href="/doc/Spanner-TrueTime-CAP.pdf">译文全文PDF</a></p><a id="more"></a><!-- TOC --><pre><code>- [title: 【译文】Spanner, TrueTime 和CAP理论](#title-%E3%80%90%E8%AF%91%E6%96%87%E3%80%91spanner-truetime-%E5%92%8Ccap%E7%90%86%E8%AE%BA)</code></pre><ul><li><a href="#%E8%AF%91%E8%80%85%E6%B3%A8">译者注</a><ul><li><a href="#cap%E5%AE%9A%E7%90%86%E5%92%8C%E4%B8%80%E8%87%B4%E6%80%A7%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0">CAP定理和一致性相关文章</a></li><li><a href="#%E5%85%B3%E4%BA%8E%E7%89%A9%E7%90%86%E6%97%B6%E9%97%B4">关于物理时间</a></li><li><a href="#cloud-spanner%E7%9A%84%E7%9B%B8%E5%85%B3%E6%8A%A5%E9%81%93">Cloud Spanner的相关报道</a></li><li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B">分布式系统课程</a></li></ul></li><li><a href="#%E8%AF%91%E6%96%87">译文</a><ul><li><a href="#spanner%E5%A3%B0%E7%A7%B0%E5%90%8C%E6%97%B6%E8%BE%BE%E5%88%B0%E4%BA%86ca">Spanner声称同时达到了CA</a></li><li><a href="#%E5%8F%AF%E7%94%A8%E6%80%A7%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE">可用性的统计数据</a></li><li><a href="#%E8%BF%99%E5%B0%B1%E6%98%AF%E7%BD%91%E7%BB%9C">这就是网络</a></li><li><a href="#%E7%BD%91%E7%BB%9C%E5%88%86%E5%8C%BA%E6%97%B6%E4%BC%9A%E5%8F%91%E7%94%9F%E4%BB%80%E4%B9%88">网络分区时会发生什么</a></li><li><a href="#%E5%85%B3%E4%BA%8Etruetime">关于TrueTime</a></li><li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li><li><a href="#%E8%87%B4%E8%B0%A2">致谢</a></li><li><a href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">参考文献</a></li></ul></li></ul><!-- /TOC --><h1 id="译者注"><a href="#译者注" class="headerlink" title="译者注"></a>译者注</h1><p><a href="https://zh.wikipedia.org/wiki/CAP%E5%AE%9A%E7%90%86" target="_blank" rel="external">CAP定理</a>是分布式系统中一个“著名”的结论，它又被称为布鲁尔定理（Brewer’s theorem，看看上面的本文作者是谁？）。CAP定理说的是一种不可能性，可能让人联想到另一个类似的不可能定理 “<a href="https://zh.wikipedia.org/wiki/%E5%93%A5%E5%BE%B7%E5%B0%94%E4%B8%8D%E5%AE%8C%E5%A4%87%E5%AE%9A%E7%90%86" target="_blank" rel="external">哥德尔不完备性定理</a>”。CAP定理可能是互联网公司在面试时用来区分科班和半路出家程序员的必考题了。<br>实际上 <strong>这个定理的重要性被高估了</strong>。注意到 <strong>就是本文的作者（Eric Brewer）提出了CAP定理</strong>，而他还在2012年发表了一篇文章 “<a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="external">CAP理论十二年回顾：”规则”变了</a>” 的文章，再加上这篇 <strong>借Spanner来讨论CAP定理</strong> 的文章，实际上是委婉地承认了这一点。</p><p>CAP定理说这3个字母最多只能同时实现2个字母的组合，但这3个字母并非同一层次的概念：A（可用性）和C（一致性）是数据副本的属性，但P（容忍网络分区）反映的是物理世界网络的状态。<br><strong>网络总是可能会断的</strong>。为实现A，需要客户端到服务器之间的网络正常，为实现C，需要服务器之间的网络正常。如果网络一切正常，那么AC能同时实现，CAP定理 <strong>在正常情况下并没有什么作用</strong>。如果网络中断，AC或者有1个无法实现，或者2个都无法实现，好吧，等着挨老板的骂吧（一个更好的办法是 <strong>准备预案</strong>，可能是 <strong>技术上的</strong>，也可能是 <strong>公关</strong> 预案）。 <strong>AC</strong> 不可能同时实现，是物理世界本身的限制。 下面的文章中认为Google的网络可靠性是足够高的，所以他们认为同时实现了CAP。<br>CAP只是一个定性的理论，讨论的情况或者是0， 或者是100%。首先，工程上的事没有什么能100%，比如系统可用性，可以说99.99%，99.999%，但没有一个工程师拍胸脯说能达到100%；其次，这3个字母都是可以更细致地量化的（C的量化比较复杂）：</p><ul><li>对于P，如果一台机器与集群失联了，一般不会认为发生了网络分区，分布式集群的管理系统应该能够容忍这个故障，继续正常运行；那么2台机器呢，n台机器呢？恰好 1：1的分裂为2个分区只是无数可能中的一种，其概率是 0 （$ 1/\infty $）。到何种程度才认为处于网络分区状态呢？</li><li>对于A，就是常说的“n个9”了，通过可用时间可以准确地测量。如果一个系统不可用会怎样？ <strong>12306.cn</strong> 网站每天23:00~06:00停止购票服务，好像也没有造成多大的不便嘛。</li><li>对于C，一致性的量化比较复杂，从Linearisability，Atomicity/Strong Consistency，到Eventually Consistency，有多种不同程度的一致性。对于最终一致性（Eventually Consistency），而言，多久算是 “最终” 也是需要量化的。关于量化这一点，在“<a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="external">CAP理论十二年回顾：”规则”变了</a>” 这篇文章中也有讨论。</li></ul><h2 id="CAP定理和一致性相关文章"><a href="#CAP定理和一致性相关文章" class="headerlink" title="CAP定理和一致性相关文章"></a>CAP定理和一致性相关文章</h2><p>Blog</p><ul><li><a href="http://www.yunweipai.com/archives/8432.html" target="_blank" rel="external">不懂点CAP理论，你好意思说你是做分布式的吗？</a></li><li><a href="https://codahale.com/you-cant-sacrifice-partition-tolerance/" target="_blank" rel="external">You Can’t Sacrifice Partition Tolerance</a></li><li><a href="http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html" target="_blank" rel="external">Please stop calling databases CP or AP</a></li><li><a href="https://github.com/aphyr/partitions-post" target="_blank" rel="external">关于网络分区的一篇blog</a></li></ul><p>Paper</p><ul><li><a href="http://queue.acm.org/detail.cfm?id=2462076" target="_blank" rel="external">Eventual Consistency Today - Limitations, Extensions,and Beyond - CACM1305</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/replicated-data-consistency-explained-through-baseball/" target="_blank" rel="external">Replicated Data Consistency Explained Through Baseball - CACM1312</a>，译文<a href="/doc/CACM1312_Replicated_Data_Consistency_Explained_Through_Baseball.pdf">以棒球赛为例解释复制数据的一致性问题</a></li><li><a href="www.bailis.org/papers/pbs-vldbj2014.pd">Quantifying Eventual Consistency with PBS - CACM1408</a>，译文<a href="/doc/CACM1408_Quantifying_Eventual_Consistency_with_PBS.pdf">利用PBS 量化最终一致性</a>；作者<a href="http://www.bailis.org/" target="_blank" rel="external">Peter Bailis</a>也是一位大牛。</li><li><a href="http://cs.nju.edu.cn/yuhuang/huangyufiles/papers/2017-pa2a-tc.pdf" target="_blank" rel="external">Probabilistically-Atomic 2-Atomicity: Enabling Almost Strong Consistency in Distributed Storage Systems</a>， <a href="http://ieeexplore.ieee.org/document/7547362/" target="_blank" rel="external">IEEE Xplore上的版本</a></li><li><a href="https://www.ksp.kit.edu/9783731501862" target="_blank" rel="external">PhD14 - Benchmarking Eventually Consistent Distributed Storage System - David Bermbach</a></li><li><a href="http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6155638" target="_blank" rel="external">IEEE Computer - ( Vol. 45 Issue 2 ) The CAP Theorem’s Growing Impact</a></li><li><a href="https://arxiv.org/abs/1509.05393" target="_blank" rel="external">A Critique of the CAP Theorem</a><blockquote><p>The CAP Theorem is a frequently cited impossibility result in distributed systems, especially among NoSQL distributed databases. In this paper we survey some of the confusion about the meaning of CAP, including inconsistencies and ambiguities in its definitions, and we highlight some problems in its formalization. CAP is often interpreted as proof that eventually consistent databases have better availability properties than strongly consistent databases; although there is some truth in this, we show that more careful reasoning is required. These problems cast doubt on the utility of CAP as a tool for reasoning about trade-offs in practical systems. As alternative to CAP, we propose a “delay-sensitivity” framework, which analyzes the sensitivity of operation latency to network delay, and which may help practitioners reason about the trade-offs between consistency guarantees and tolerance of network faults.</p></blockquote></li></ul><h2 id="关于物理时间"><a href="#关于物理时间" class="headerlink" title="关于物理时间"></a>关于物理时间</h2><ul><li><a href="http://dl.acm.org/citation.cfm?id=2347750" target="_blank" rel="external">Toward higher precision - PTP协议 - CACM-2012-10</a></li><li><a href="https://spectracom.com/sites/default/files/document-files/Time%20for%20Datacenters%20to%20Consider%20Time%20as%20a%20Service_WP12-101_A.pdf" target="_blank" rel="external">Time for Data Centers to Consider Time as a Service</a></li><li><a href="https://en.wikipedia.org/wiki/Global_Positioning_System" target="_blank" rel="external">GPS - Global Positioning System - wiki</a></li><li><p><a href="http://www.beidou.gov.cn/attach/beidou/China&#39;s%20BeiDou%20Navigation%20Satellite%20System%28Chinese%29.pdf" target="_blank" rel="external">中国北斗卫星导航系统白皮书</a></p><blockquote><p>正在运行的北斗二号系统发播B1I和B2I公开服务信号，免费向亚太地区提供公开服务。服务区为南北纬55度、东经55度到180度区域，定位精度优于10米，测速精度优于0.2米/秒，<strong>授时精度优于50纳秒</strong>。</p></blockquote></li><li><p><a href="http://www.ucolick.org/~sla/leapsecs/timescales.html" target="_blank" rel="external">Time Scales</a></p></li><li><a href="http://gjss.ndrc.gov.cn/gzdtx/201309/t20130926_683874.html" target="_blank" rel="external">长短波授时系统</a></li></ul><p>如果在万能的淘宝上搜“时钟同步服务器，GPS，北斗”，就会看到很多产品，价格不比一台服务器贵。比如下面的链接（免费的广告啊~~）<br>高精度的时间同步是一个系统工程。即便是局域网的范围，不是简单买一台NTP服务器，然后接到网络里就算完工了。当然，还是要看精度要求有多高，NTP的精度还是比较低的。<br>通过网络同步时间的问题在于计算机网络（以太网）尽力而为和多层协议带来的 <strong>固有的通信延迟不确定性</strong>。延迟大并不可怕，只要是稳定的延迟，就可以预先扣除，但延迟的不确定性就很难处理了。可能的方案是使用专用，没有任何协议的物理线路，就跟电力线路一样，延迟几乎没有波动。</p><ul><li><a href="http://www.syn029.com/h-index.html" target="_blank" rel="external">西安同步电子科技有限公司</a>，<a href="https://shop102437782.taobao.com/index.htm" target="_blank" rel="external">他们的淘宝店</a></li><li><a href="http://www.jingtech.cn/col.jsp?id=130" target="_blank" rel="external">西安景驰电子科技有限公司</a>，<a href="https://zhuanlan.zhihu.com/p/21596625" target="_blank" rel="external">TS3200 系列GPS/BD 同步时钟 - 知乎专栏</a></li><li><a href="http://www.bonzn.cn/col.jsp?id=105" target="_blank" rel="external">广州邦正电力科技有限公司</a>，</li></ul><p>为什么2家都是西安的？不一定是巧合，很可能是因为中国科学院国家授时中心就在西安临潼。<br>只说GPS/北斗芯片的话，其实只有几十块钱，不但智能手机必备，现在比较流行的共享单车都装了这些定位芯片。当然，从一个GPS芯片到一个时钟同步服务器产品还是要有很多外围设备的，作为外行，就不深入讨论了。</p><h2 id="Cloud-Spanner的相关报道"><a href="#Cloud-Spanner的相关报道" class="headerlink" title="Cloud Spanner的相关报道"></a>Cloud Spanner的相关报道</h2><p><a href="http://www.infoq.com/cn/news/2017/02/Google-Cloud-Spanner-hit-CAP" target="_blank" rel="external">Infoq - 谷歌新发布的分布式数据库服务，是要打破CAP定理了吗？</a><br><a href="http://www.infoq.com/cn/news/2017/03/google-cloud-spanner-beta" target="_blank" rel="external">Infoq - 谷歌对外发布了云Spanner Beta版</a></p><h2 id="分布式系统课程"><a href="#分布式系统课程" class="headerlink" title="分布式系统课程"></a>分布式系统课程</h2><ul><li><a href="https://courses.cs.washington.edu/courses/cse552/13au/calendar/lecturelist.html" target="_blank" rel="external">Washington Univ. CSE552: Distributed and Parallel Systems - Fall 2013</a></li><li><a href="http://nil.csail.mit.edu/6.824/2016/schedule.html" target="_blank" rel="external">MIT 6.824: Distributed Systems - Spring 2016</a></li><li><a href="http://www.bigoh.net/wiki/index.php/Dis-alg" target="_blank" rel="external">分布式算法入门</a></li><li><a href="https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/" target="_blank" rel="external">Notes on Distributed Systems for Young Bloods</a></li></ul><hr><h1 id="译文"><a href="#译文" class="headerlink" title="译文"></a>译文</h1><p>Spanner是Google的高可用的全球SQL数据库[CDE + 12：Spanner]。它管理着大规模的复制的数据。大规模既指数据量方面，又有事务量方面。它为写入其中的每项数据分配全局一致的实时时间戳，客户端可以在整个数据库上执行全局一致的读操作，而无需使用锁（译注：因为Spanner使用的是物理时间，而且是全球分布的，所以这里的全局既可以理解为逻辑上的整体，也可以理解为全球性的）。<br>CAP定理[Bre12]说，下面三个期望的属性中，你最多只能同时达到两个：</p><ul><li>C：一致性（Consistency），本文中我们可以认为这是指顺序一致性（Serializability）；</li><li>A：读取和更新的100％可用性（Availability）；</li><li>P：对网络分区（Partitions）的容忍。</li></ul><p>舍弃其中一个字母，就剩下三种系统：CA，CP和AP。请注意，并非自然就会有这三个属性中的两个，有许多系统只有其中的一个属性，甚至一个也没有。<br>对于“广域”上的分布式系统，通常认为网络分区是不可避免的，尽管不一定常见[BK14]。一旦你认为网络分区是不可避免的，任何分布式系统必须准备好放弃一致性（剩下AP）或可用性（剩下CP），这不是人们想做的选择。事实上，CAP定理的出发点是让设计者认真对待这种权衡。但是有两个重要的警告：首先，你只需要在实际发生网络分区期间放弃去某些东西，即时那时也有有许多缓解措施（参见文章“CAP理论12年回顾”[Bre12]）。其次，CAP定理关注的是100%可用性，而本文是关于现实的高可用性涉及的权衡（译注：高但不是100%）。</p><h2 id="Spanner声称同时达到了CA"><a href="#Spanner声称同时达到了CA" class="headerlink" title="Spanner声称同时达到了CA"></a>Spanner声称同时达到了CA</h2><p>尽管是一个全球分布式系统，Spanner却声称具有一致性和高可用性，这意味着没有网络分区，因此很多人表示怀疑 。这是否意味着Spanner是CAP定义的CA系统？简短的答案是技术上“不是”，但效果上“是”，用户可以并确实认为是CA系统。<br>纯粹主义的答案是“否”，因为网络分区总是可能发生，事实上在Google也确实发生过。在网络分区时，Spanner选择C而放弃了A。因此从技术上来说，它是一个CP系统。我们下面探讨网络分区的影响。<br>考虑到始终提供一致性（C），Spanner声称为CA的真正问题是，它的核心用户是否认可它的可用性（A）。如果实际可用性足够高，用户可以忽略运行中断，则Spanner是可以声称达到了“有效CA”的。这并不意味着100％的可用性（Spanner目前和将来也不会提供），而是如5个或更多个“9”（即1/〖10〗^5或更少的失效）。反过来，真正的试金石是，那些希望自己本身的服务高可用的用户，他们是否会编写处理运行中断异常的代码：如果他们没有编写这些代码，那么他们已经假设Spanner的高可用性了。基于大量的Spanner内部用户，我们知道他们认为Spanner是高可用的。<br>第二点是存在许多其它运行中断的原因，除了“生死与共”的Spanner之外，其它原因也会让用户的服务失效。我们实际上关心差异化可用性，即用户是否确实已经发现Spanner已停掉了。差异化可用性比Spanner的实际可用性还要高。也就是说，你必须真的听到大树倒下的声音才算是出了麻烦（译注：也就是说，Spanner的短暂不可用不一定会立即造成用户的系统不可用）。<br>第三个问题是运行中断是否由于网络分区造成的。如果Spanner运行中断的主要原因不是网络分区，那么声称CA就更充分了。例如，任何数据库在所有副本都脱机的情况下都不能提供可用性，这与网络分区无关。这种多副本情况下的运行中断应该是非常罕见的，但如果网络分区的概率显著的更小，那么就可以有效地忽略网络分区对可用性的影响。对于Spanner，这意味着可用性中断的发生，实际并非是由于网络分区，而是一些其它的多种故障（因为单一故障不会造成可用性中断）。</p><h2 id="可用性的统计数据"><a href="#可用性的统计数据" class="headerlink" title="可用性的统计数据"></a>可用性的统计数据</h2><p>在深入Spanner之前，值得先讨论一下Chubby的演进。Chubby是另一个提供CA的广域系统。在Chubby的论文[Bur06]中提到700天中发生了9次30 s或更长时间的运行中断，其中6次与网络相关（如[BK14]中讨论的）。这对应的可用性最多也不超过5个9，如果我们更贴近实际，假设每次中断平均有10 min，那么就只有4个9；如果每次中断有几个小时的话，就只有3个9了。<br>对于锁和一致性读/写操作，随着多项网络、架构和运维的改进，<strong>目前的广域分布的Chubby集群能提供99.99958％的平均可用性</strong>（即仅有30 s多一点的运行中断）。从2009年开始，由于可用性“超额”，Chubby的站点可靠性工程师（SRE）开始人为地强制定期中断服务（译注：即应急演习），以确保我们能发现对Chubby的依赖和其故障可能造成的影响。<br>在内部，Spanner提供与Chubby水平相当的可靠性，少于5.9 s。云版本与内部版本有相同的基础，但添加了一些新的部分，所以它的可靠性在短期内可能稍低一些。<br><img src="/img/spanner-availability.png" alt="Spanner可用性数据"><br>上面的饼图显示了内部Spanner意外事件的原因。事件是意外的，但并非所有都严重到中断服务。一些事件可以轻易地处理掉。图中的数值是事件发生的频率而不是造成的后果。</p><ul><li>大量的事件（<strong>用户事件</strong>）是由于用户错误，例如超载或配置错误，并且大多只影响该用户，然而其它类别则可能影响区域中的所有用户。</li><li><strong>集群事件</strong> 反映除网络外的底层基础架构问题，如服务器和电源的问题。Spanner通过使用其它副本自动地处理这些事件，但有时需要SRE参与修复不完整的副本。</li><li><strong>运维事件</strong> 是由SRE引起的事故，例如配置错误。</li><li><strong>Bug事件</strong> 意味着软件bug触发的问题，这可能导致或大或小不同范围的运行中断。两个最大的中断都是由同时影响了某个数据库的所有副本的软件bug造成的。</li><li><strong>其它事件</strong> 是各种大多只发生一次的问题。</li></ul><p><strong>网络事件</strong> （低于8％）是网络分区和网络配置问题。还没有发生过较大集群的网络分区事件，也没有发生过一个分区的少数一方超过Spanner的Quorum的情况。我们确实看到个别数据中心或区域与其它网络断开。我们还有一些错误配置，短时调低了带宽，还有一些与硬件故障相关的暂时的延迟。曾经有一个事件，其中某个方向的网络中断，导致一个奇怪的分区，必须通过关闭一些节点才能解决。到目前为止，网络事件没有造成过大规模的运行中断。<br>总而言之，要声称“有效CA”，系统必须处于这种相对概率状态：<br>1）至少它在实践中必须具有非常高的可用性，以便用户可以忽略异常；<br>2）由网络分区造成的运行中断应只占很小一部分。<br>Spanner同时满足两者。</p><h2 id="这就是网络"><a href="#这就是网络" class="headerlink" title="这就是网络"></a>这就是网络</h2><p>许多人认为，Spanner通过使用<code>TrueTime</code> 可以绕过CAP。<code>TrueTime</code> 是一个提供全局同步时钟的服务。<code>TrueTime</code> 是非比寻常的，但为实现CA，<code>TrueTime</code> 的作用并不显著。后面的小节会介绍<code>TrueTime</code> 。某种程度说，Spanner的特别之处是，Google私有的广域网，加上多年的运维改进，显著降低了网络分区的发生，从而使高可用性成为可能。<br>首先，Google运行自己的私有全球网络。Spanner并非在公共的互联网上运行 —— 实际上，Spanner的每个数据包只流过Google控制的路由器和链路（不包括到远程客户端的任何边缘链路）。此外，每个数据中心通常至少有三个独立的光纤将其连接到私有的全球网络。因此确保任何两个数据中心之间有多条网络通路 。类似的，数据中心内的设备和链路也是冗余的。因此，通常的灾难性事件，如光纤被切断，不会导致网络分区或运行中断。<br>因此，网络分区的真正风险不是网络被切断，而是某些大范围的配置或软件升级同时破坏了多个链路。这是一个真正的风险，并且Google持续地努力防止和缓解这一风险。一般的策略是限制任何更新的影响范围（“爆炸半径”），以便我们不可避免地推送一个错误的变更后，它只破坏一部分链路或副本。我们在修复问题之前不会尝试任何其它变更。<br>虽然网络分区大大减少了，但光速是有限的。广域上的一致性操作的往返时间（RTT）下限仍比较大，洲际约有几十毫秒或更长。（光速约0.5 ft/ns，若洲际为1 000 mile的距离，约合5 000 000 ft，则最少需要10 ms）。Google将一个“区域”的范围限制在RTT 2 ms之内，以在延迟和容灾之间达到平衡。Spanner通过尽可能的事务批量化来缓解延迟，但这并不能降低单个事务的延迟。对于读操作，延迟通常较低，这是由于全局时间戳和使用本地副本的能力（如下节所述）。<br>具有较弱一致性的模型可能具有较低的更新延迟。然而，如果距离不够远，就会存在一个持久性较低的窗口。因为在数据被复制到另一个站点之前，如果本地站点遭受了灾害，所有的数据都可能被彻底破坏掉（译注：弱一致性模型会推迟数据的跨站点复制操作以降低延迟）。</p><h2 id="网络分区时会发生什么"><a href="#网络分区时会发生什么" class="headerlink" title="网络分区时会发生什么"></a>网络分区时会发生什么</h2><p>为了理解分区，我们需要更多地了解一下Spanner的工作原理。和大多数ACID数据库一样，Spanner使用两阶段提交（2PC）和严格的两阶段锁，以确保隔离性和强一致性。2PC被称为“反可用性”协议[Hel16]，因为事务期间所有成员必须正常工作。为缓解这一问题，Spanner的每个成员实际是一个Paxos组（译注：多个节点组成逻辑上的单个节点），即便Paxos组中某个节点宕机了，每个2PC“成员”也是高可用的。每个分组也是数据放置和复制的基本单元。<br>前面提到，一般来说当发生网络分区时，Spanner会选择C而非A。在实践中，这是考虑到：</p><ul><li>使用Paxos组来达成关于某个更新的共识；如果Paxos Leader由于网络分区而不能维持Quorum，则更新被暂停，并且系统不可用（由CAP的定义）。最终，如果大多数成员可用的话，新的Leader就可能选举出来；</li><li>对跨组事务使用2PC还意味着组内成员的网络分区可以阻止提交。</li></ul><p>在实践中最可能的结果是，网络分区的一侧满足Quorum，并将继续运行，也许需要重新选举Leader。因此，服务继续可用，但是另一侧分区的成员数较少，不满足Quorum，它们的用户无法访问该服务。这个例子说明了差异化可用性的重要性：那些无法访问服务的用户可能会有其它更严重的问题，例如失去连接，也可能已经宕机了。这意味着构建在Spanner之上的多区域服务，即使在网络分区时也能相对良好地运行。存在比较小的可能性，Spanner的某一部分会完全不可用。<br>只要所有事务相关的组都有Quorum选举的Leader，并位于分区的同一侧，Spanner中的事务就会正常运行。这意味着一些事务正常提交，有些事务则会超时，但它们总是满足一致性的。Spanner的一个特性是，任何正常返回的读操作都是一致的，即使事务稍后终止了（由于超时在内的任何原因）。<br>除了常规事务之外，Spanner还支持快照读，即读取过去特定时刻的数据值。Spanner维护多个时间版本的值，每个版本都有一个时间戳，因此可以为快照读操作返回正确的版本。特别地，每个副本都知道生成快照的时间，并且任何副本能以本节点直接回复该时间点之前的读操作（除非它太旧了，以至于已经被垃圾收集）（译注：一般需要从多个节点读取数据并验证多数节点间是否一致）。类似地，很容易同时跨多个组异步读取。快照读完全不需要锁。事实上，只读事务被实现为在当前时刻（在任何最新的副本上）的快照读。<br>因此，快照读对网络分区而言更加健壮。特别的，快照读能在以下情况下正常工作：</p><ol><li>对于发起读操作的一侧网络分区，每个组至少存在一个副本</li><li>对于这些副本，读时间戳是过去的。</li></ol><p>如果Leader由于网络分区而暂停（这可能一直持续到网络分区结束），这时第2种情况可能就不成立了。因为这一侧的网络分区上可能无法选出新的Leader（译注：见下节引用的解释）。在网络分区期间，时间戳在分区开始之前的读操作很可能在分区的两侧都能成功，因为任何可达的副本有要读取的数据就足够了。</p><h2 id="关于TrueTime"><a href="#关于TrueTime" class="headerlink" title="关于TrueTime"></a>关于TrueTime</h2><p>通常，同步时钟可以用于避免分布式系统中的通信。Barbara Liskov提供了一个不错的概述和多个示例[Lis91] 。对于我们的目的，<code>TrueTime</code> 是一个误差有界但非0的全局同步时钟：它返回的是一个时间区间，能保证执行调用的真实时刻落在这个区间内。因此，如果两个区间不重叠，我们能明确地将调用按真实时间排序。但如果区间存在重叠，我们就无法给出这两个调用的顺序了（译注：即并发的）。<br>Spanner的一个微妙的之处是它用锁来实现顺序一致性（Serializability），但它用<code>TrueTime</code> 来实现外部一致性（External Consistency，接近于线性一致性，Linearizability）。Spanner的外部一致性不变量（Invariant）是：对任何两个事务 $T_1$ 和 $T_2$ （即使在地球的两端），<strong> 如果 $T_2$ 在 $T_1$ 提交之后才开始提交，则 $T_2$ 的时间戳大于 $T_1$ 的时间戳。</strong><br>引自Liskov [Lis91，第7节]：</p><blockquote><p>同步时钟可以用来降低违反外部一致性的概率。假设主节点（Primary）拥有租约（Lease，即一段时间的所有权），需要考虑整个副本组。副节点（Backup）发送到主节点的每条消息相当于是对主节点的一个租约。如果主节点持有一个来自次多数（Sub-majority ）副节点的未到期租约，则主节点能以单节点进行读操作。<br>…<br>该系统中的不变量是：每当主节点执行读取时，它持有来自多数副节点的一个有效租约。如果时钟不同步，这个不变量将不再成立。</p></blockquote><p>Spanner使用<code>TrueTime</code> 作为时钟，以确保不变量成立。具体地，在提交期间，Leader可能必须等待，直到它确定提交时间在过去（基于误差界限）。实践中，这种“提交等待”的时间并不太长，而且与（内部）事务通信并行地进行。一般来说，外部一致性需要单调增加的时间戳，“等待不确定性结束”也是一种常见的模式。<br>Spanner旨在通过对当选的Leader使用可顺延的租约，来延长Leader的在位时间（通常为10s）。如Liskov所讨论的，每次Quorum达成共识时（译注：可能是关于其它事项），租约就会被顺延，因为参与者刚刚验证了Leader是有效的。当Leader失效时，有两个选项：1）等待租约过期，然后选举新的Leader；或2）重启旧的Leader，这可能更快些。对于一些故障，我们可以发出一个“最后一息”的UDP数据包来释放租约，这是一个优化，以使租约尽快到期。由于计划外故障在Google的数据中心中很少见，所以长期的租约是合理的。租约还确保时间在Leader之间都是单调增长的，并且在没有Leader的情况下，使副节点能够在租约有效期内继续提供读取服务。<br>然而，<code>TrueTime</code> 的真正价值在于它在一致性快照方面的能力。回顾一下，多版本并发控制系统（Multi-Version Concurrency-Control systems，MVCC）[Ree78]有悠久的历史，它将旧版本分开保存，从而允许读取过时的版本，而不考虑当前的事务活动。这是一个非常有用和被低估的特性：具体到Spanner上，快照是一致的（在抓取快照时），因此如果你的系统中的某个不变量成立，它在快照中也会成立。即使你不知道是什么不变量！基本上，快照是在持续不断的多个事务之间抓取的，并且反映截至此时的所有内容，当然不会超过这些内容。如果没有事务一致的快照，则很难从过去的时刻重新开始，因为这种快照的内容可能反映的是未完成的事务，这种事务可能违反一些不变量或完整性约束。正是缺乏一致性，导致有时难以从备份恢复系统。特别是有可能出现数据损坏，需要手动修复 。<br>例如，考虑使用MapReduce对数据库执行大规模的分析查询。Bigtable存储着旧版本的数据，时间在数据分片上是“锯齿状”的，这使得结果不可预测，有时不一致（特别是对于较近的数据）。在Spanner上，同一个MapReduce可以选择精确的时间戳，并获得可重复和一致的结果。<br><code>TrueTime</code> 还使得跨多个独立系统抓取快照成为可能，只要它们使用（单调增加的）<code>TrueTime</code> 时间戳提交，对抓取快照的时间达成一致，并存储多个时间版本的数据（通常在日志中）。这不仅限于Spanner：你可以实现自己的事务系统，然后确保在两个系统（或甚至k个系统）上一致的快照。一般来说，在这些系统上需要一个2PC（同时持有锁）以就抓取快照的时间达成一致，并确认成功，但系统不需要就其它事项达成一致，甚至这些系统可能会有很大的差异。<br>你还可以使用时间戳做为工作流传递的令牌。例如，如果对系统进行更新，则可以将更新的时间戳传递到工作流的下一个阶段，以便可以确定系统时间是否在该事件之后。在网络分区的情况下，这可能是不成立的，在这种情况下，如果想要一致性，下一个阶段应该等待（或如果想要可用性就继续下去）。没有时间令牌，很难知道你是否需要等待。使用时间戳不是解决这个问题的唯一方法，但这种方法是优雅且健壮的，能够保证最终一致性（Eventual Consistency）。当不同的阶段没有约定规则且管理员不同时，这是特别有用的——因为双方可以在没有通信的情况下对时间达成一致 。<br>快照是关于过去的，但你也可以对未来达成一致。Spanner的一项特性是，为实现模式变更，可以就未来的某个时刻达成一致。这允许暂存对新模式的变更，以便能够同时提供新旧两个版本。一旦就绪，就可以选择一个时刻，在所有副本上以原子的方式切换到新的模式上（也可以选择暂存之前的时刻，但那时你可能还没有准备好）。至少理论上，你可以执行一些未来的操作，如计划删除或可预见的变更。<br><code>TrueTime</code> 本身可能受到网络分区的影响。时间的来源是GPS接收器和原子钟的组合，两者都可以通过它们自身的微小漂移来保持精确的时间（译注：微小漂移是保持时间同步的调节手段，并不是说这两种时钟源不稳定）。由于每个数据中心都有冗余的“Time Master”，因此网络分区的两侧很可能继续获取准确的时间。然而，各个节点需要与Time Master的网络连接，否则它们自己的时钟将偏移。因此，在网络分区期间，它们与Time Master的偏差会逐渐地增长，取决于本地时钟漂移的速率限值。基于<code>TrueTime</code> 的操作，例如Paxos Leader选举或事务提交，必须等待一段时间，但操作仍能够完成（假设2PC及Quorum通信正常）。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>Spanner合理地声称是一个“有效CA”系统。尽管运行在广域上，但它总是一致的，并达到了大于5个 9的可用性。与Chubby一样，同时达到CA在实践中是可能的，前提是像Google那样能控制整个网络，但这在广域上是罕见的。此外，还需要大量冗余的网络链路、处理相关故障的架构规划、以及非常细致的运维，特别是对于升级。如果还是不幸发生了运行中断，Spanner选择一致性而不是可用性。<br>Spanner使用两阶段提交来实现顺序一致性，它使用<code>TrueTime</code> 实现外部一致性、无锁的一致性读取、以及一致性快照。 </p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>特别感谢Spanner 和<code>TrueTime</code> 的专家：Andrew Fikes，Wilson Hsieh，和Peter Hochschild。 另外还要感谢 Brian Cooper，Kurt Rosenfeld，Chris Taylor，Susan Shepard，Sunil Mushran，Steve Middlekauff，Cliff Frey，Cian Cullinan，Robert Kubis，Deepti Srivastava，Sean Quinlan，Mike Burrows 和 Sebastian Kanthak。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[BK14] P. Bailis and K. Kingsbury. <a href="http://cacm.acm.org/magazines/2014/9/177925-the-network-is-reliable/abstract" target="_blank" rel="external">The Network is Reliable</a>, Communications of the ACM. Vol. 57 No. 9,<br>Pages 48-55. September 2014. Also: [<a href="https://aphyr.com/posts/288-the-network-is-reliable" target="_blank" rel="external">https://aphyr.com/posts/288-the-network-is-reliable</a>]<br>[Bre12] E. Brewer. <a href="https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="external">CAP Twelve Years Later: How the “Rules” Have Changed</a>, IEEE Computer, Vol. 45, Issue 2, February 2012. pp. 23–29. <a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="external">CAP理论十二年回顾：”规则”变了</a>（译注：这是本文作者之前的一篇文章）<br>[Bur06] M. Burrows. <a href="https://research.google.com/archive/chubby-osdi06.pdf" target="_blank" rel="external">The Chubby lock service for loosely-coupled distributed systems</a>. Proceedings of OSDI `06: Fourth Symposium on Operating System Design and Implementation, Seattle, WA, November 2006.<br>[CDE+12] J. Corbett, J. Dean, et. al.  Spanner: Google’s Globally-Distributed Database. Proceedings of OSDI ‘12: Tenth Symposium on Operating System Design and Implementation, Hollywood, CA, October, 2012. <a href="http://dblab.xmu.edu.cn/post/google-spanner/" target="_blank" rel="external">厦门大学计算机系 林子雨 老师的译文</a><br>[Hel16] P. Helland. <a href="http://queue.acm.org/detail.cfm?id=2953944" target="_blank" rel="external">Standing on Giant Distributed Shoulders: Farsighted Physicists of Yore were Danged  Smart!</a> ACM Queue, Vol. 14, Issue 2, March-April 2016.<br>[Lis91] B. Liskov. <a href="http://dl.acm.org/citation.cfm?id=112601" target="_blank" rel="external">Practical Uses of Synchronized Clocks in Distributed Systems</a>. ACM Principles of Distributed Computing (PODC). Montreal, Canada, August 1991.<br>[MHL+92] C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh and P. Schwartz. <a href="http://dl.acm.org/citation.cfm?id=128770" target="_blank" rel="external">ARIES: A Transaction Recovery  Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging</a>. ACM Transactions on Database Systems, Vol. 17, No. 1, March 1992, pp. 94-162.<br>[Ree78] D. Reed. <a href="http://publications.csail.mit.edu/lcs/specpub.php?id=773" target="_blank" rel="external">Naming and Synchronization in a Decentralized Computer System</a>, PhD Dissertation, MIT Laboratory for Computer Science, Technical Report MIT-LCS-TR-205. October 1978 [See Section 6.3 for list of versions with timestamps]</p><hr><p><img src="/img/spanner-books.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://en.wikipedia.org/wiki/Eric_Brewer_%28scientist%29&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Eric Brewer，VP, Infrastructure, Google&lt;/a&gt;&lt;br&gt;2017-02-14&lt;br&gt;英文原文：&lt;a href=&quot;https://research.google.com/pubs/pub45855.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Spanner, TrueTime and the CAP Theorem&lt;/a&gt; ,&lt;a href=&quot;https://research.google.com/pubs/archive/45855.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;英文全文 PDF&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;/doc/Spanner-TrueTime-CAP.pdf&quot;&gt;译文全文PDF&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>CCF目录单页版</title>
    <link href="https://ying-zhang.github.io/misc/2017/ccf-all-in-one/"/>
    <id>https://ying-zhang.github.io/misc/2017/ccf-all-in-one/</id>
    <published>2017-02-24T16:00:00.000Z</published>
    <updated>2017-10-30T02:48:06.699Z</updated>
    
    <content type="html"><![CDATA[<p>CCF目录有<a href="http://history.ccf.org.cn/sites/ccf/paiming.jsp" target="_blank" rel="external">网页版</a>和<a href="http://history.ccf.org.cn/sites/paiming/2015ccfmulu.pdf" target="_blank" rel="external">2015版的PDF</a>。<br>前一段时间整理论文列表，感觉分成多页的目录用起来不太方便。于是就粘贴复制，把它们合并到一起。<br>Markdown排版比较麻烦，于是把表格单独放在一个html文件里了，链接是 <strong><a href="/doc/ccf-all-in-one-2017-02-25.html">CCF目录完整列表2017-02-25</a></strong> 。<br>此外还有 <strong><a href="/doc/ccf_all_in_one_2017-02-25.xlsx">Excel格式</a></strong>。</p><p>CCF网站最近改版了，原来的链接发生了变化(2017-02-25访问，上面的链接已经更新过了)。新版的链接是<a href="http://webtest.ccf.org.cn/xspj/gyml/" target="_blank" rel="external">CCF目录</a> ，HTML版不太完整，而且没有提供PDF文件。</p><a id="more"></a><!-- TOC --><ul><li><a href="#%E8%AF%B4%E6%98%8E">说明</a></li><li><a href="#ccf%E7%9B%AE%E5%BD%95%E4%B8%AD%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98">CCF目录中存在的问题</a><ul><li><a href="#%E5%90%8C%E4%B8%80%E5%88%8A%E7%89%A9%E5%90%8C%E6%97%B6%E8%A2%AB%E5%88%97%E5%85%A5%E4%B8%8D%E5%90%8C%E6%96%B9%E5%90%91">同一刊物同时被列入不同方向</a></li><li><a href="#%E5%90%8C%E4%B8%80%E6%96%B9%E5%90%91%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E4%BC%9A%E8%AE%AE">同一方向中的重复会议</a></li></ul></li></ul><!-- /TOC --><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>表格中的类别简写分别是:</p><ul><li>AI：人工智能</li><li>系统：计算机体系结构/并行与分布计算/存储系统</li><li>软工：软件工程/系统软件/程序设计语言</li><li>数据库：数据库/数据检索/内容检索</li><li>安全：网络与信息安全</li><li>多媒体：计算机图形学与多媒体</li><li>网络：计算机网络</li><li>理论：计算机科学理论</li><li>其它：交叉/综合/新兴</li><li>交互：人机交互与普适计算</li></ul><p>表格中是按拼音排序的，排序字段依次是：方向，类型（会议/刊物），级别（A/B/C）。</p><h1 id="CCF目录中存在的问题"><a href="#CCF目录中存在的问题" class="headerlink" title="CCF目录中存在的问题"></a>CCF目录中存在的问题</h1><h2 id="同一刊物同时被列入不同方向"><a href="#同一刊物同时被列入不同方向" class="headerlink" title="同一刊物同时被列入不同方向"></a>同一刊物同时被列入不同方向</h2><p>多个研究方向经常出现交叉，所以这种情况也是可以接受的。</p><ul><li>B类刊物 <code>DKE: Data and Knowledge Engineering</code> 分别被列入了 AI 和 数据库 方向</li><li>C类刊物 <code>IJIS: International Journal of Intelligent Systems</code> 分别被列入了 AI 和 数据库 方向</li><li>C类刊物 <code>IPL: Information Processing Letters</code> 分别被列入了 理论 和 数据库 方向</li><li>B类刊物 <code>TOMCCAP: ACM Transactions on Multimedia Computing, Communications and Applications</code> 分别被列入了 网络 和 多媒体 方向</li></ul><h2 id="同一方向中的重复会议"><a href="#同一方向中的重复会议" class="headerlink" title="同一方向中的重复会议"></a>同一方向中的重复会议</h2><ul><li>安全方向的 C类会议 <code>ASIACCS</code>和 A类会议 <code>CCS</code>给出的链接都是 [<a href="http://dblp.org/db/conf/ccs/" target="_blank" rel="external">http://dblp.org/db/conf/ccs/</a>] ，这个不是错误，因为dblp上确实是在同一页面显示了 <code>ASIACCS</code> 和 <code>CCS</code>，不过列表中 <code>ASIACCS</code>的全称不太恰当</li><li>交互方向的 C类会议 <code>DIS: ACM Conference on Designing Interactive  Systems</code> 在第2条和第11条重复出现了两次</li></ul><p><img src="/img/ccf_sum.png" alt="刊物分方向汇总"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CCF目录有&lt;a href=&quot;http://history.ccf.org.cn/sites/ccf/paiming.jsp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;网页版&lt;/a&gt;和&lt;a href=&quot;http://history.ccf.org.cn/sites/paiming/2015ccfmulu.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;2015版的PDF&lt;/a&gt;。&lt;br&gt;前一段时间整理论文列表，感觉分成多页的目录用起来不太方便。于是就粘贴复制，把它们合并到一起。&lt;br&gt;Markdown排版比较麻烦，于是把表格单独放在一个html文件里了，链接是 &lt;strong&gt;&lt;a href=&quot;/doc/ccf-all-in-one-2017-02-25.html&quot;&gt;CCF目录完整列表2017-02-25&lt;/a&gt;&lt;/strong&gt; 。&lt;br&gt;此外还有 &lt;strong&gt;&lt;a href=&quot;/doc/ccf_all_in_one_2017-02-25.xlsx&quot;&gt;Excel格式&lt;/a&gt;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;CCF网站最近改版了，原来的链接发生了变化(2017-02-25访问，上面的链接已经更新过了)。新版的链接是&lt;a href=&quot;http://webtest.ccf.org.cn/xspj/gyml/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;CCF目录&lt;/a&gt; ，HTML版不太完整，而且没有提供PDF文件。&lt;/p&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>如何收集和整理论文（面向CS专业）</title>
    <link href="https://ying-zhang.github.io/misc/2016/we-love-paper/"/>
    <id>https://ying-zhang.github.io/misc/2016/we-love-paper/</id>
    <published>2016-09-27T16:00:00.000Z</published>
    <updated>2018-01-08T14:05:45.068Z</updated>
    
    <content type="html"><![CDATA[<p>论文（Paper）是每个研究生读研路上挥之不去的“阴云”。<br>无论是否已经有了一个好的课题或想法，都首先要收集某个研究方向一定数量的论文，来了解相关的工作和最新进展（State of the art &amp; practice）。<br>本文介绍了如何检索、收集计算机科学（CS）专业的论文，还介绍了相关的机构，学术会议和论文数据库。<br>文末有 <a href="#hosts"><strong>Bonus</strong></a> 哦;-)</p><a id="more"></a><hr><h1 id="tl-dr"><a href="#tl-dr" class="headerlink" title="tl;dr"></a>tl;dr</h1><ul><li>从<a href="https://ying-zhang.github.io/misc/2017/ccf-all-in-one/">CCF推荐目录</a>中自己感兴趣的方向的 <strong>A类会议及期刊</strong> 中找论文即可。</li><li>云计算，程序分析方向的<a href="#tldr">会议和期刊链接列表</a></li><li><a href="#hosts"><strong>Bonus</strong></a> 修改Hosts</li></ul><h1 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h1><p>按理说，开篇应该先要强调一下Paper对于科研的重要性的，直接把前辈的经验拿来吧：</p><ul><li>周志华老师的一篇关于<a href="/doc/research_and_paper_zhou_zhihua_2007_ppt.pdf">做研究与写论文的ppt</a></li><li>凌晓峰和杨强的<a href="http://item.jd.com/11127141.html" target="_blank" rel="external">《学术研究 - 你的成功之道》</a>，这本书的英文原版是<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6813064" target="_blank" rel="external">Crafting Your Research Future - A Guide to Successful Master’s and Ph.D. Degrees in Science &amp; Engineering</a></li></ul><p>首先要从前辈的经验中端正对Paper的态度：Paper不是科研的原因，而是结果（的一部分）。结果肯定是必要的，自然也就少不了Paper或者总结报告；<br>再者要借鉴前辈的学习方法和技巧，所谓“工欲善其事，必先利其器”，除了科研课题本身，养成一套高效的科研方法和习惯也是重要的。<br>重要的是要 <strong>有意识地探索和总结适合自己的科研方法</strong>，既要低头苦干，又要抬头看路，还要回头总结。</p><h2 id="论文发表的过程"><a href="#论文发表的过程" class="headerlink" title="论文发表的过程"></a>论文发表的过程</h2><pre>                                  / 期刊，特辑/专刊 → 多轮审稿 → 上线 \Idea -> 编程、实验、写Paper、投稿 <                                    ->出版，检索                                  \ 会议           → 审稿 → 赴会报告 /</pre><p>简单介绍一下发表论文的过程：</p><ul><li>首先投稿的Paper作者，一般是高校的的研究生，也有教师，比如<a href="https://amplab.cs.berkeley.edu/" target="_blank" rel="external">UC Berkeley 的AMPLab</a>；还有一些公司的研究院，比如<a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">微软</a>，<a href="https://research.google.com/pubs/papers.html" target="_blank" rel="external">谷歌</a>。显然，论文的出身对质量有很大影响。</li><li>期刊是传统的研究成果发表方式，一般期刊是季度，少有月度出版的，每期称为一个Issue，一年的各期集结为一个Volume。一般可以随时投稿给期刊，没有截稿日期（Deadline，ddl）的压力。投稿后一般要经过同行评审（Peer Review），针对审稿人的建议做大修，小修（Major，Minor Revision）等两三轮修改才能被接收，期间跨度一年多是常有的情况。不过如果有的期刊安排了专刊（Special Issue）计划，会公布一个截稿日期，审稿的进度会稍快些。<br>期刊分为 <strong>Transaction, Journal 和 Magazine</strong> 。这三者的学术严肃性依次降低，这可以从它们的封面上直观地看出来。严格来说Magazine不算是学术期刊了，上面很少发表新的原创性的内容，而是对当前进展的简介和综述，也会转发一些已经发表过的重要的Paper。 <strong>不过对于新手来说，先浏览一下Magazine，建立一个基本的概念还是很有必要的，特别是 <a href="http://dl.acm.org/citation.cfm?id=J79" target="_blank" rel="external">Communications of the ACM（CACM, ACM通讯）</a> 值得关注</strong>。</li><li>对快速发展的CS专业来说，期刊的节奏有点慢了，而且期刊版面有限，每期只能发布几篇到十几篇不等（有的期刊则动辄一卷几千页，所以，文章质量嘛……），所以上面发表的一般是一些理论性比较强的论文，或者综述性的论文；很多新成果转而发表在学术会议上来，这跟传统学科不太一样。<br>很多会议每年举行一次，时间上也是比较固定的月份，会提前在会议网站上发布下一年的call for paper（cfp，征稿启事）和deadline（截稿日期）。投稿后，一般经过两三个月的审稿就会通知作者是否录用（Notifications）。被接收的Paper会要求按格式和Reviewers的意见稍修改后提交正式的最终版，称为Camera Ready。最后需要作者赴会做Presentation。<br>会议录用的所有Paper会结集出版，称为 <strong>Proceedings</strong> 。有的会议还会推荐一些优秀的Paper到合作的期刊，扩展后作为期刊论文出版。<br>会议分为 <strong>Symposium , Conference 和 Workshop</strong>。这三者的学术严肃性依次降低，大部分会议都称为 <strong>Conference</strong>。一般来说 <strong>Workshop</strong> 是随某个 <strong>Conference</strong> 一同举办，可能没有固定的主题，Paper质量与主会议有所差别。</li></ul><p>通过这个过程，我们还可以知道如何 <strong>尽快</strong> 找到一篇感兴趣的文章：</p><ul><li>对于期刊，一般投稿是很多的，编辑部会把已经接收但还没有排到出版期号的文章先放到网上在线出版，称为Early Access，即在线预出版；</li><li>对于会议，在确定了接收的文章后，会在会议网站的Program/Accepted Papers/Schedule等类似链接下给出列表，同时会Email通知作者准备提交Camera Ready版。这时有的作者就会把Camera Ready版放到自己的主页上。之后会议组委将论文集结提供给所有作者，还会将论文集发布到ACM或IEEE（这两个机构直接参与了很多会议的组织）的论文数据库中。不同的会议组委效率不同，有的在开会前就上线出版了，有的在会议结束后还要等一段时间。</li></ul><h1 id="CS论文数据库"><a href="#CS论文数据库" class="headerlink" title="CS论文数据库"></a>CS论文数据库</h1><h2 id="ACM-IEEE-Computer等"><a href="#ACM-IEEE-Computer等" class="headerlink" title="ACM, IEEE Computer等"></a>ACM, IEEE Computer等</h2><p>一般会议和期刊都有自己的网站，但很少能在上面获取到论文全文。又因为来源分散，直接从它们那里检索Paper很不方便。有几个大型的论文数据库，它们与期刊或者会议主办方合作，或者自己组织会议或编辑出版期刊，比如下面的表格及<a href="#lib">图书馆页面截图</a>。</p><blockquote><p>注意， DL和机构的网站有很多介绍性的内容是重复的，下载论文全文要去DL的页面。</p></blockquote><table><thead><tr><th>机构</th><th>Digital Library （DL）</th><th>机构首页</th></tr></thead><tbody><tr><td>Association for Computing Machinery, ACM</td><td>ACM Digital Library  <a href="https://dl.acm.org/" target="_blank" rel="external">https://dl.acm.org/</a></td><td><a href="https://www.acm.org/" target="_blank" rel="external">https://www.acm.org/</a></td></tr><tr><td>IEEE Computer Society</td><td>IEEE Xplore DL <a href="http://ieeexplore.ieee.org/" target="_blank" rel="external">http://ieeexplore.ieee.org/</a></td><td><a href="https://www.computer.org/" target="_blank" rel="external">https://www.computer.org/</a></td></tr><tr><td>Elsevier ScienceDirect</td><td><a href="http://www.sciencedirect.com/" target="_blank" rel="external">http://www.sciencedirect.com/</a></td><td><a href="https://www.elsevier.com/" target="_blank" rel="external">https://www.elsevier.com/</a></td></tr><tr><td>Springer</td><td>Springer Link <a href="http://link.springer.com/" target="_blank" rel="external">http://link.springer.com/</a></td><td><a href="http://www.springer.com/" target="_blank" rel="external">http://www.springer.com/</a></td></tr><tr><td>Wiley</td><td>Wiley Online Lib <a href="http://onlinelibrary.wiley.com/" target="_blank" rel="external">http://onlinelibrary.wiley.com/</a></td><td><a href="http://www.wiley.com/" target="_blank" rel="external">http://www.wiley.com/</a></td></tr></tbody></table><p>ACM 和 IEEE Computer Society（计算机学会，IEEE还有电气、电子、通信等其它多个学会） 的网址后缀是 <strong>.org</strong>，这两个是CS领域最重要的学术组织，很多的CS学术会议都是由它们组织的。<br>Elsevier，Springer，Wiley的网址后缀则是 <strong>.com</strong> ，这些是学术出版商，内容以期刊为主，涵盖了CS及其它多个学科。<br>上面这几个数据库是 <strong>主要的论文全文来源</strong>。它们各自收录的会议和期刊基本没有重叠，从它们的数据库下载的Paper也都有各自的排版样式。</p><blockquote><p>ACM作为最“正统”的计算机学术组织，它的DL除了收录ACM组织的会议和期刊全文之外，还会索引其它几家数据库的 <strong>元数据</strong>，但没有全文，不过可以通过DOI链接跳转到这几家数据库的全文页面。<br>IEEE出版的一些论文在 computer.org （实际是<a href="https://www.computer.org/csdl/" target="_blank" rel="external">CSDL</a>）和 Xplore DL 都可能搜到，但这两个数据库是 <strong>分别</strong> 收费的，能在Xplore DL下载的不一定能在Computer.org下载。</p></blockquote><h3 id="ACM-SIGs"><a href="#ACM-SIGs" class="headerlink" title="ACM SIGs"></a>ACM SIGs</h3><p>ACM之下针对CS多个子方向的“分舵”，称为Special Interest Group，SIG，目前有三十多个<a href="http://www.acm.org/sigs/" target="_blank" rel="external">ACM SIGs</a>（或参考DL的这个链接<a href="http://dl.acm.org/sigs.cfm" target="_blank" rel="external">SIGs ACM DL</a>），比如</p><ul><li>体系结构方向的<a href="http://www.sigarch.org/" target="_blank" rel="external">SIGARCH</a>、<a href="http://www.sighpc.org/" target="_blank" rel="external">SIGHPC</a>、<a href="http://www.sigmetrics.org/" target="_blank" rel="external">SIGMETRICS</a>、<a href="http://www.sigmicro.org/" target="_blank" rel="external">SIGMICRO</a>、<a href="http://www.sigmobile.org/" target="_blank" rel="external">SIGMOBILE</a>，</li><li>网络方向的<a href="http://www.sigcomm.org/" target="_blank" rel="external">SIGCOMM</a>，</li><li>数据库方向的<a href="http://www.sigmod.org/" target="_blank" rel="external">SIGMOD</a>，</li><li>系统方向的<a href="http://www.sigops.org/" target="_blank" rel="external">SIGOPS</a>，</li><li>软件工程方向的<a href="http://www.sigplan.org/" target="_blank" rel="external">SIGPLAN</a>、<a href="http://www.sigsoft.org/" target="_blank" rel="external">SIGSOFT</a></li></ul><p>这些SIGs除了组织一系列的学术会议，还会评选本方向的一些奖项，包括 <strong>最佳论文</strong>，<strong>优秀博士论文</strong> 等（在DL中一般没有标明哪篇是Best Paper）。此外，</p><ul><li>有网站维护了一个<a href="http://jeffhuang.com/best_paper_awards.html" target="_blank" rel="external">部分会议的最佳论文列表</a>，</li><li>还有下面要介绍的USENIX的<a href="https://www.usenix.org/conferences/best-papers" target="_blank" rel="external">各会议最佳论文</a>。</li></ul><p>有的SIG会选择一些高质量的文章，以Review，Newsletter 或 Notes 的形式重新发表，引用的时候最好引用最初的来源。</p><h2 id="USENIX"><a href="#USENIX" class="headerlink" title="USENIX"></a><a href="https://www.usenix.org/" target="_blank" rel="external">USENIX</a></h2><p>要是学校的图书馆不差钱，把所有有价值的论文数据库都买买买来，那么下面截图中的电子资源列表应该很全了吧？然而还是少了一个重要的数据库：USENIX —— 它是免费的。<br>话说<strong><a href="https://www.usenix.org/" target="_blank" rel="external">USENIX</a></strong> 实在是个良心组织。USENIX最初称为Unix User Group。它组织了OSDI 、ATC、FAST、NSDI、LISA等会议，不但学术水平很高，贴近工业界，而且免费提供全文下载，还提供一些论文作者在会议上的slides及演讲视频。slides是对文章的提炼，读论文时可以参考。拿slides和视频来学习做Presentation，练习英语听力和口语也不错。</p><h2 id="arXiv"><a href="#arXiv" class="headerlink" title="arXiv"></a><a href="http://arxiv.org/" target="_blank" rel="external">arXiv</a></h2><p><a href="http://arxiv.org/" target="_blank" rel="external">arXiv</a>， 是archive（归档）的意思，是一个由康乃尔大学维护的免费的多学科论文<strong>预</strong>出版（preprint）数据库。所谓<strong>预</strong>出版，就是说论文还没有经过同行评审，文责自负，文章质量参差不齐，所以一般不会作为正式的学术成果。不过有的学科习惯上先把文章公开到arXiv上，然后再提交到会议上。</p><p><a name="lib"><img src="/img/lib.png" alt="图书馆电子资源"></a></p><h2 id="EI和SCI"><a href="#EI和SCI" class="headerlink" title="EI和SCI"></a>EI和SCI</h2><p>分别搜索上面的数据库还是有点麻烦，于是就有了一些聚合数据库，又称为索引。想必很多同学在读研之前早就听说EI和SCI，</p><ul><li>EI <strong>Engineering Index</strong> <a href="https://www.engineeringvillage.com/" target="_blank" rel="external">https://www.engineeringvillage.com/</a></li><li>SCI <strong>Science Citation Index</strong> <a href="http://apps.webofknowledge.com/" target="_blank" rel="external">http://apps.webofknowledge.com/</a></li></ul><p><strong>只看 URL 还以为是 山寨网站</strong>，它们的Web界面体验也不太友好，而且它们不止有CS一个学科，直接通过关键词搜索经常会给出不相关的内容。其实这两个数据库通常是在 <strong>已知文章标题的情况下</strong> 检索是不是被它们收录了，而 <strong>不是</strong> 用来收集文章的。</p><p>要确定某个会议论文集或者期刊<a href="http://www.philippe-fournier-viger.com/links.php" target="_blank" rel="external">是否被EI或SCI收录</a>，</p><ul><li>在<a href="https://www.elsevier.com/solutions/engineering-village/content" target="_blank" rel="external">EI收录列表</a> 页内搜索Compendex Source List，会找到一个Excel表格的链接，下载下来会发现这个表格是受保护的，但可以筛选标题（而且最后一个WorkSheet有中文翻译哦，满满的土洋结合，中国特色）。嘘~~ 也许你可以参考<a href="/doc/crack_xls_vb.txt">这个脚本解除保护</a>，还要建议把title列中每个单元格开头的<code>=</code>替换掉。这个Excel的title是排好序的，方便顺序浏览，比如有个杂志名叫<code>Computing</code>，真是起的好名字，如果直接搜索是肯定搜出一堆结果，所以，即便找到名字一样的期刊，最好也要再确认一下ISSN号。<strong>但是！</strong>，这个Excel表格并不完整，如果没有在表中搜索到，还是需要在EI的网站上搜索文章标题才能最终确认。对了，EI的数据库叫 <strong>Compendex</strong>。</li><li>在<code>webofknowledge</code>的网站查询之前，<strong>一定</strong> 要选择数据库为<code>检索 Web of Science 核心合集</code>，等自动刷新候，还要在页面下部展开“更多设置”，只选中<code>Science Citation Index Expanded (SCIEXPANDED) 1900年至今</code>这一项，然后才能查询出根正苗红的<code>SCI（E）</code>。请<strong>务必</strong>参考<a href="/doc/SCI_E_Web_of_Science.pdf">这个截图</a>。可以在<a href="http://ip-science.thomsonreuters.com/cgi-bin/jrnlst/jlsearch.cgi?PC=K" target="_blank" rel="external">SCI收录列表</a>直接输入期刊的名称来查询该期刊是否被SCI收录，但感觉这个查的也不全。还是要充分利用SCI的中国特色了，因为还有一个国内整理的SCI期刊列表：<a href="http://scit.nju.edu.cn/Item/1162.aspx" target="_blank" rel="external">中国科学技术信息研究所SCI（E）论文期刊分区列表（2016年）</a>，这是一个有13.8k多行的Excel表格，简洁粗暴。</li></ul><hr><p>上面的这些数据库可以免费检索标题和摘要，购买全文则价格不菲。如果学校的图书馆购买了这些数据库，一般会识别用户的IP地址，在学校网络范围内可以直接下载PDF全文。<br>校外就没有这么方便了，好在很多作者在Paper被录用后会在自己的主页上挂出PDF全文，从Google Scholar上可以搜索到这些PDF全文的链接，非常方便。<br>话说只要是能花钱买到的东西，去万能的 <strong>淘宝</strong> 肯定能找到，就看是买 <strong>VPN/代理，单篇文章，还是 整个数据库</strong> 了。</p><h2 id="dblp"><a href="#dblp" class="headerlink" title="dblp"></a>dblp</h2><p>dblp [<a href="http://dblp.org" target="_blank" rel="external">http://dblp.org</a>] ，或[<a href="http://dblp.uni-trier.de]，" target="_blank" rel="external">http://dblp.uni-trier.de]，</a> 是专注于CS学科的文献 <strong>元数据索引</strong> 数据库，优势是收集得相当完整，链接也很有规律，比如特定会议的 FSE 2016 <a href="http://dblp.org/db/conf/sigsoft/fse2016.html" target="_blank" rel="external">http://dblp.org/db/conf/sigsoft/fse2016.html</a> 或者某个作者的全部论文列（dblp对重名作者处理得很好），但只能搜索标题或作者等元数据，用来初步筛选论文非常方便，需要获取全文时还是要跳转到上面的几个数据库，数据更新也稍微滞后一点。<br>2015版CCF目录中的会议和期刊都是dblp的链接。</p><p>dblp 列出了关于CS论文的一些统计数据，比如（2016年10月查询）</p><ul><li><a href="http://dblp.dagstuhl.de/statistics/recordsindblp.html" target="_blank" rel="external">累计论文记录数量</a>，</li><li><a href="http://dblp.dagstuhl.de/statistics/publicationsperyear.html" target="_blank" rel="external">每年发表的论文数量</a>，</li><li><a href="http://dblp.dagstuhl.de/statistics/distributionofpublicationtype.html" target="_blank" rel="external">论文发表的类型</a>，其中会议论文占53%，</li><li>论文总数 3,587,354 ， 作者人数 1,825,286，会议数4,912，期刊数 1,491。</li></ul><p>另外，ACM DL也有一个<a href="http://dl.acm.org/contents_guide.cfm" target="_blank" rel="external">类似的统计</a>。<br><img src="/img/pubs.png" alt="每年发表的CS论文数量"></p><p>而且dblp整站的数据都可以下载为一个<a href="http://dblp.dagstuhl.de/xml/" target="_blank" rel="external">xml文件</a>，以供进一步挖掘。</p><h2 id="DOI"><a href="#DOI" class="headerlink" title="DOI"></a>DOI</h2><p>在查找或引用论文时经常会遇到DOI(Digital Object Identifier)，<a href="https://zh.wikipedia.org/wiki/DOI" target="_blank" rel="external">wikipedia介绍DOI</a>“是一套识别数字资源的机制，涵括的对象有视频、报告或书籍等等。它既有一套为资源命名的机制，也有一套将识别号解析为具体地址的协议”。</p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>从Google Scholar搜索全文时可能会跳转到下面这几个网站，因为它们会保存一些别人分享的全文。</p><ul><li>Semantic Scholar [<a href="https://www.semanticscholar.org" target="_blank" rel="external">https://www.semanticscholar.org</a>]</li><li>CiteSeerX [<a href="http://citeseerx.ist.psu.edu/" target="_blank" rel="external">http://citeseerx.ist.psu.edu/</a>]</li><li>ResearchGate [<a href="https://www.researchgate.net/" target="_blank" rel="external">https://www.researchgate.net/</a>] ，这是一个学术社交网络</li></ul><h1 id="CCF目录"><a href="#CCF目录" class="headerlink" title="CCF目录"></a>CCF目录</h1><p>EI和SCI只是两个论文数据库，但能够发表被EI和SCI收录的文章变成了能够毕业，能否获得奖学金，能否获得基金的指标。由于时代的限制，EI和SCI被赋予了不相称的地位和意义，而且短期看还是如此。<br>更 “<strong>不幸</strong>” 的是，对于CS的学生，还有一个<a href="http://history.ccf.org.cn/sites/ccf/paiming.jsp" target="_blank" rel="external">CCF目录</a>（<a href="http://history.ccf.org.cn/sites/paiming/2015ccfmulu.pdf" target="_blank" rel="external">2015版的PDF</a>）摆在面前。其实并非是不幸，而是十分幸运，因为CCF目录不像是一个紧箍咒，而更像是一个入门指南。</p><p>首先说<a href="http://www.ccf.org.cn/" target="_blank" rel="external">中国计算机学会 CCF</a>是国内的类似于ACM的计算机学术组织。也许某位同学的导师就是CCF会员。相比EI和SCI收录的成百上千的会议和期刊，CCF维护的目录显然<a href="https://ying-zhang.github.io/misc/2017/ccf-all-in-one/">精简得多</a>。<br>考虑到对EI和SCI指标要求的实际情况，目录选取的 <strong>大多</strong> 是被EI或SCI收录的，具体划分为10个子方向，并区分出A，B，C三个等级。<br>A，B类的会议和期刊的文章学术质量较高。这个质量不是简单通过所谓影响因子等机械的数据来评价的，而是综合了多种因素，国际上也获得比较广泛认可的。比如，翻一下本科的《现代操作系统》这本经典教材的参考文献，会发现其中引用的有不少SOSP，OSDI，ASPLOS,TCS等A类会议和期刊。</p><blockquote><p>如果只看标题和摘要，有的期刊/会议文章看起来非常值得一读，比如 <a href="http://dblp.org/db/journals/fgcs/" target="_blank" rel="external">FGCS    Future Generation Computer Systems</a>上的文章。但如果仔细读一下文章全文，经常是大失所望。好在CCF只给了FGCS C类的评级。<br>随着越来越多的研究生进入工业界，论文对码农来说也就不那么神秘了。看到别人写的文章末尾附上了一大堆英文论文，也不只是单纯地赞赏一番或浏览一下标题，还要看看这些文章发在哪些会议或者期刊上。其实经典的二八定律也符合学术界，对CS专业来说比例可能更甚。事实是大部分的文章不值得细读。这时就会发现CCF评级的价值了。仅从A类文章搜集资料就已经足够了，而且还能提高检索效率。</p></blockquote><p>上面提到ACM有三十多个SIGs，而CCF则只划分了10个子方向，不同的视角有不同的划分结果，这里有ACM的更详细的<a href="http://dl.acm.org/ccs/ccs.cfm" target="_blank" rel="external">分类系统CCS</a>，以及有重叠的<a href="https://www.acm.org/special-interest-groups/sigs-by-knowledge-area" target="_blank" rel="external">SIGs大类划分</a>，还有<a href="https://en.wikipedia.org/wiki/Outline_of_computer_science" target="_blank" rel="external">wikipedia上的一个划分</a>。</p><h1 id="Google-Scholar（谷歌学术）"><a href="#Google-Scholar（谷歌学术）" class="headerlink" title="Google Scholar（谷歌学术）"></a>Google Scholar（谷歌学术）</h1><p><a href="https://scholar.google.com/" target="_blank" rel="external">Google Scholar</a>非常强大又简单易用。虽然它不只收录CS专业的文献，但很容易搜索到准确的结果。我习惯先在谷歌学术上搜索，如果搜不到就改用 <strong>高级搜索</strong>，实在不行再去ACM DL、IEEE Xplore。<br><img src="/img/scholar_adv.png" alt="谷歌学术高级搜索"></p><h2 id="创建快讯"><a href="#创建快讯" class="headerlink" title="创建快讯"></a>创建快讯</h2><p>与Google网页搜索一样，可以在Google Scholar创建某个关键词或某篇文章的快讯（发送邮件通知最新的搜索结果）；<br>此外，注意搜索到的论文的作者是否有链接，打开即是Google Scholar创建的作者个人资料页，上面一般有作者的单位、联系方式，文章列表等，在 <strong>作者的个人资料页</strong> 可以创建关于他的新文章或新引用的快讯，及时获取动态。</p><blockquote><p>话说体验一下<a href="http://www.bing.com/academic" target="_blank" rel="external">必应学术</a>、<a href="http://xueshu.baidu.com/" target="_blank" rel="external">百度学术</a>和<a href="http://scholar.sogou.com/" target="_blank" rel="external">搜狗学术</a>也未尝不可。</p></blockquote><p><a name="tldr"></a></p><h1 id="tl-dr：链接列表"><a href="#tl-dr：链接列表" class="headerlink" title="tl,dr：链接列表"></a>tl,dr：链接列表</h1><table><thead><tr><th>缩写</th><th>CCF级别</th><th>链接</th><th>全称</th></tr></thead><tbody><tr><td>ASPLOS</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE178&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/asplos/" target="_blank" rel="external">DBLP</a></td><td>Architectural Support for Programming Languages and Operating Systems</td></tr><tr><td>HPCA</td><td><strong>A</strong></td><td><a href="http://ieeexplore.ieee.org/servlet/opac?punumber=1000335" target="_blank" rel="external">IEEE</a> , <a href="http://dblp.org/db/conf/hpca/" target="_blank" rel="external">DBLP</a></td><td>High-Performance Computer Architecture</td></tr><tr><td>ISCA</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE239&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/isca/" target="_blank" rel="external">DBLP</a></td><td>Int. Symposium on Computer Architecture</td></tr><tr><td>MICRO</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE203&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/micro/" target="_blank" rel="external">DBLP</a></td><td>Microarchitecture</td></tr><tr><td>PPoPP</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE241&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/ppopp/" target="_blank" rel="external">DBLP</a></td><td>Principles and Practice of Parallel Programming</td></tr><tr><td>SC</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE207&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/sc/" target="_blank" rel="external">DBLP</a></td><td>Int. Conf. for High Performance Computing, Networking, Storage, and Analysis</td></tr><tr><td>ATC</td><td><strong>A</strong></td><td><a href="https://www.usenix.org/conferences/byname/131" target="_blank" rel="external">USENIX</a> , <a href="http://dblp.org/db/conf/usenix/" target="_blank" rel="external">DBLP</a></td><td>USENIX Annul Technical Conf.</td></tr><tr><td>EuroSys</td><td><strong>B</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE101&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/eurosys/" target="_blank" rel="external">DBLP</a></td><td>European Conf. on Computer Systems</td></tr><tr><td>HPDC</td><td><strong>B</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE300&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/hpdc/" target="_blank" rel="external">DBLP</a></td><td>High-Performance Distributed Computing</td></tr><tr><td>LISA</td><td><strong>B</strong></td><td><a href="https://www.usenix.org/conferences/byname/5" target="_blank" rel="external">USENIX</a> , <a href="http://dblp.org/db/conf/lisa/" target="_blank" rel="external">DBLP</a></td><td>Large Installation system Administration Conf.</td></tr><tr><td>PODC</td><td><strong>B</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE221&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/podc/" target="_blank" rel="external">DBLP</a></td><td>Symposium on Principles of Distributed Computing</td></tr><tr><td>SigMetrics</td><td><strong>B</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE187&amp;tab=pubs" target="_blank" rel="external">ACM</a>，<a href="http://dl.acm.org/citation.cfm?id=J618" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/sigmetrics/" target="_blank" rel="external">DBLP</a></td><td>Int. Conf. on Measurement and Modeling of Computer Systems</td></tr><tr><td>TC</td><td><strong>A</strong></td><td><a href="https://www.computer.org/web/tc" target="_blank" rel="external">IEEE</a> , <a href="http://dblp.org/db/journals/tc/" target="_blank" rel="external">DBLP</a></td><td>Trans. on Computers</td></tr><tr><td>ToCS</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/pub.cfm?id=J774&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/journals/tocs/" target="_blank" rel="external">DBLP</a></td><td>Trans. on Computer Systems</td></tr><tr><td>TPDS</td><td><strong>A</strong></td><td><a href="https://www.computer.org/web/tpds" target="_blank" rel="external">IEEE</a> , <a href="http://dblp.org/db/journals/tpds/" target="_blank" rel="external">DBLP</a></td><td>Trans. on Parallel and Distributed Systems</td></tr></tbody></table><h2 id="云计算，大数据"><a href="#云计算，大数据" class="headerlink" title="云计算，大数据"></a>云计算，大数据</h2><table><thead><tr><th>缩写</th><th>CCF级别</th><th>链接</th><th>全称</th></tr></thead><tbody><tr><td>SIGMOD</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE227&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/sigmod/" target="_blank" rel="external">DBLP</a></td><td>Conf. on Management of Data</td></tr><tr><td>SoCC</td><td><strong>B</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE227&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/cloud/" target="_blank" rel="external">DBLP</a></td><td>Symposium on Cloud Computing</td></tr><tr><td>PODS</td><td><strong>B</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE227&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/pods/" target="_blank" rel="external">DBLP</a></td><td>SIGMOD Conf. on Principles of DB Systems</td></tr><tr><td>VLDB Endowment</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/pub.cfm?id=J1174&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/vldb/" target="_blank" rel="external">DBLP</a></td><td>Int. Conf. on Very Large Data Bases</td></tr><tr><td>VLDB Journal</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/pub.cfm?id=J869&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/journals/vldb/" target="_blank" rel="external">DBLP</a></td><td>Int. Journal on Very Large Data Bases</td></tr><tr><td>NSDI</td><td><strong>B</strong></td><td><a href="https://www.usenix.org/conferences/byname/178" target="_blank" rel="external">USENIX</a> , <a href="http://dblp.org/db/conf/nsdi/" target="_blank" rel="external">DBLP</a></td><td>Network System Design and Implementation</td></tr><tr><td>SigComm</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE258&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/sigcomm/" target="_blank" rel="external">DBLP</a></td><td>The Conf. of the ACM Special Interest Group on Data Communication</td></tr></tbody></table><p>体系结构中的 ASPLOS、ISCA、Micro、ATC、EuroSys、HPDC、LISA、SIGMETRICS、TPDS等，及软工的OSDI、SOSP等也都有不少云计算相关的文章。<br>IEEE 云计算系列（CLOUD等）也可以看看。 <a href="http://cloudcomputing.ieee.org/conferences" target="_blank" rel="external">IEEE</a> , <a href="http://dblp.org/db/conf/IEEEcloud/" target="_blank" rel="external">DBLP</a> </p><h2 id="软件工程（偏程序分析）"><a href="#软件工程（偏程序分析）" class="headerlink" title="软件工程（偏程序分析）"></a>软件工程（偏程序分析）</h2><ul><li>Github上的<a href="https://github.com/tue-mdse/conferenceMetrics" target="_blank" rel="external">软件工程方向会议的数据</a></li><li>UIUC的<a href="http://taoxie.cs.illinois.edu/" target="_blank" rel="external">谢涛老师</a>维护的<a href="http://taoxie.cs.illinois.edu/seconferences.htm" target="_blank" rel="external">软件工程方向的会议统计列表</a> </li></ul><table><thead><tr><th>缩写</th><th>CCF级别</th><th>链接</th><th>全称</th></tr></thead><tbody><tr><td>ASE</td><td><strong>A</strong></td><td><a href="http://dblp.org/db/conf/kbse/" target="_blank" rel="external">DBLP</a></td><td>Int. Conf. on Automated Software Engineering</td></tr><tr><td>FSE/ESEC</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE201&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/sigsoft/" target="_blank" rel="external">DBLP</a></td><td>Symposium on the Foundation of Software Engineering / European Softw. Eng. Conf.</td></tr><tr><td>ICSE</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE228&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/icse/" target="_blank" rel="external">DBLP</a></td><td>Int. Conf. on Software Engineering 另，FOSE会议：七年一届的展望</td></tr><tr><td>OOPSLA</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE181&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/oopsla/" target="_blank" rel="external">DBLP</a></td><td>Conf. on Object-Oriented Programming Systems, Languages, and Applications</td></tr><tr><td>OSDI</td><td><strong>A</strong></td><td><a href="https://www.usenix.org/conferences/byname/179" target="_blank" rel="external">USENIX</a> , <a href="http://dblp.org/db/conf/osdi/" target="_blank" rel="external">DBLP</a></td><td>USENIX Symposium on Operating Systems Design and Implementations，<strong>双数</strong> 年份召开</td></tr><tr><td>SOSP</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE208&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/sosp/" target="_blank" rel="external">DBLP</a></td><td>Symposium on Operating Systems Principles， <strong>单数</strong> 年份召开，另，<a href="http://sigops.org/sosp/sosp15/history/index.html" target="_blank" rel="external">SOSP 2015 History Day</a></td></tr><tr><td>PLDI</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE200&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/pldi/" target="_blank" rel="external">DBLP</a></td><td>SIGPLAN Symposium on Programming Language Design and Implementation</td></tr><tr><td>POPL</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE180&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/popl/" target="_blank" rel="external">DBLP</a></td><td>SIGPLAN&amp;SIGACT Symposium on Principles of Programming Languages</td></tr><tr><td>ECOOP</td><td><strong>B</strong></td><td><a href="http://www.ecoop.org/" target="_blank" rel="external">ECOOP</a> , <a href="http://dblp.org/db/conf/ecoop/" target="_blank" rel="external">DBLP</a></td><td>European Conf. on Object-Oriented Programming</td></tr><tr><td>HotOS</td><td><strong>B</strong></td><td><a href="https://www.usenix.org/conferences/byname/155" target="_blank" rel="external">USENIX</a> , <a href="http://dblp.org/db/conf/hotos/" target="_blank" rel="external">DBLP</a></td><td>USENIX Workshop on Hot Topics in Operating Systems</td></tr><tr><td>ICSME</td><td><strong>B</strong></td><td><a href="http://conferences.computer.org/icsm/" target="_blank" rel="external">IEEE</a> , <a href="http://dblp.org/db/conf/icsm/" target="_blank" rel="external">DBLP</a></td><td>Int. Conf. on Software Maintenance</td></tr><tr><td>ISSTA</td><td><strong>B</strong></td><td><a href="http://dl.acm.org/event.cfm?id=RE222&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/conf/issta/" target="_blank" rel="external">DBLP</a></td><td>Int. Symposium on Software Testing and Analysis</td></tr><tr><td>TOPLAS</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/pub.cfm?id=J783&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/journals/toplas/" target="_blank" rel="external">DBLP</a></td><td>Trans. on Programming Languages and Systems</td></tr><tr><td>TOSEM</td><td><strong>A</strong></td><td><a href="http://dl.acm.org/pub.cfm?id=J790&amp;tab=pubs" target="_blank" rel="external">ACM</a> , <a href="http://dblp.org/db/journals/tosem/" target="_blank" rel="external">DBLP</a></td><td>Trans. on Software Engineering Methodology</td></tr><tr><td>TSE</td><td><strong>A</strong></td><td><a href="https://www.computer.org/web/tse" target="_blank" rel="external">IEEE</a> , <a href="http://dblp.org/db/journals/tse/" target="_blank" rel="external">DBLP</a></td><td>Trans. on Software Engineering</td></tr><tr><td>SPE</td><td><strong>B</strong></td><td><a href="http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1097-024X" target="_blank" rel="external">Wiley</a>, <a href="http://dblp.org/db/journals/spe/" target="_blank" rel="external">DBLP</a></td><td>Software - Practice and Experience</td></tr></tbody></table><h2 id="ACM-DL列表"><a href="#ACM-DL列表" class="headerlink" title="ACM DL列表"></a>ACM DL列表</h2><ul><li><a href="http://dl.acm.org/contents_dl.cfm" target="_blank" rel="external">收录会议和期刊的完整列表</a></li><li><a href="http://dl.acm.org/events.cfm" target="_blank" rel="external">会议</a></li><li><a href="http://dl.acm.org/proceedings.cfm" target="_blank" rel="external">会议历次论文集</a></li><li><a href="http://dl.acm.org/pubs.cfm" target="_blank" rel="external">期刊和学报</a></li><li><a href="http://dl.acm.org/mags.cfm" target="_blank" rel="external">杂志</a></li><li><a href="http://dl.acm.org/conferences.cfm" target="_blank" rel="external">ACM Conferences - past 12 months</a></li><li><a href="http://dl.acm.org/UpcomingConfLocations.xml" target="_blank" rel="external">ACM Upcoming Conferences - RSS</a></li></ul><p>关于ACM的杂志，特别推荐</p><ul><li><a href="http://dl.acm.org/citation.cfm?id=J79" target="_blank" rel="external">Communications of the ACM, CACM</a>， <a href="http://dblp.org/db/journals/cacm/" target="_blank" rel="external">in dblp</a></li><li><a href="http://dl.acm.org/toco_arch.cfm?id=J79&amp;lang=chinese" target="_blank" rel="external">CACM中国版</a>，CCF曾经 选译 一部分CACM文章为中文并出版，但2016年后没有继续下去</li><li><a href="http://dl.acm.org/citation.cfm?id=J882" target="_blank" rel="external">Queue</a>也值得一看，不过它与CACM有很多重叠的文章</li></ul><p>期刊中，推荐<a href="http://dl.acm.org/citation.cfm?id=J204" target="_blank" rel="external">ACM Computing Surveys, CSUR</a>， <a href="http://dblp.org/db/journals/csur/" target="_blank" rel="external">in dblp</a></p><blockquote><p>看了ACM DL的列表页面，应该能体会到：1000项以内的列表还是不要分页的好。</p></blockquote><h2 id="IEEE-Computer列表"><a href="#IEEE-Computer列表" class="headerlink" title="IEEE Computer列表"></a>IEEE Computer列表</h2><ul><li><a href="https://www.computer.org/web/conferences/calendar/" target="_blank" rel="external">会议日历</a></li><li><a href="https://www.computer.org/web/publications/transactions" target="_blank" rel="external">期刊和学报</a></li><li><a href="https://www.computer.org/web/publications/magazines" target="_blank" rel="external">杂志</a></li></ul><h2 id="USENIX组织的会议列表"><a href="#USENIX组织的会议列表" class="headerlink" title="USENIX组织的会议列表"></a><a href="https://www.usenix.org/conferences/byname" target="_blank" rel="external">USENIX组织的会议列表</a></h2><p><a href="https://www.usenix.org/conferences/byname" target="_blank" rel="external">USENIX组织的会议列表</a>，其中包括ATC，FAST，LISA，MobiSys，NSDI，OSDI，VEE及HotCloud，HotOS等一系列 HotXXXX 的Workshop。</p><h2 id="国内三个学报"><a href="#国内三个学报" class="headerlink" title="国内三个学报"></a>国内三个学报</h2><ul><li><a href="http://www.jos.org.cn/ch/index.aspx" target="_blank" rel="external">软件学报</a>，<a href="http://rss.cnki.net/KNS/rss.aspx?journal=RJXB&amp;Virtual=KNS" target="_blank" rel="external">CNKI RSS</a></li><li><a href="http://cjc.ict.ac.cn/" target="_blank" rel="external">计算机学报</a>，<a href="http://rss.cnki.net/KNS/rss.aspx?journal=JSJX&amp;Virtual=KNS" target="_blank" rel="external">CNKI RSS</a></li><li><a href="http://crad.ict.ac.cn/CN/volumn/home.shtml" target="_blank" rel="external">计算机研究与发展</a> ，<a href="http://rss.cnki.net/KNS/rss.aspx?journal=JFYZ&amp;Virtual=KNS" target="_blank" rel="external">CNKI RSS</a></li></ul><h2 id="国内论文数据库"><a href="#国内论文数据库" class="headerlink" title="国内论文数据库"></a>国内论文数据库</h2><ul><li><a href="http://www.cnki.net/" target="_blank" rel="external">知网CNKI</a></li><li><a href="http://www.wanfangdata.com.cn/" target="_blank" rel="external">万方数据</a></li></ul><h2 id="其它链接"><a href="#其它链接" class="headerlink" title="其它链接"></a>其它链接</h2><ul><li><a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">微软研究院</a></li><li><a href="https://research.google.com/pubs/papers.html" target="_blank" rel="external">谷歌研究院</a></li><li><a href="https://blog.acolyer.org/" target="_blank" rel="external">The morning paper</a>, an interesting-influential-important paper from the world of CS every weekday morning</li><li><a href="http://sites.computer.org/debull/bull_issues.html" target="_blank" rel="external">IEEE Technical Committee on Data Engineering</a></li><li><a href="https://www.youtube.com" target="_blank" rel="external">YouTube</a>，</li><li><strong><a href="http://www1.cs.columbia.edu/~kaiser/relatedwork.htm" target="_blank" rel="external">Suggested Guidelines for Finding Materials to include in the “Related Work” Sections of Conference Papers</a></strong></li><li><a href="http://www.yocsef.org.cn/sites/yocweb/yocltzw.jsp?contentId=2658502145582" target="_blank" rel="external">YOCSEF专题论坛：从LNCS事件反思中国学术论文的发表</a>  </li></ul><h1 id="如何读论文"><a href="#如何读论文" class="headerlink" title="如何读论文"></a>如何读论文</h1><ul><li><a href="http://www.cs.columbia.edu/~hgs/netbib/efficientReading.pdf" target="_blank" rel="external">Efficient Reading of Papers in Science and Technology(pdf)</a></li><li><a href="http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf" target="_blank" rel="external">How to Read a Paper(pdf)</a></li><li><a href="https://www.cs.jhu.edu/~jason/advice/how-to-read-a-paper.html" target="_blank" rel="external">How to Read a Technical Paper</a></li><li><a href="http://item.jd.com/11127141.html" target="_blank" rel="external">《学术研究 - 你的成功之道》第3章</a></li></ul><h1 id="Todo-辅助工具"><a href="#Todo-辅助工具" class="headerlink" title="[Todo]辅助工具"></a>[Todo]辅助工具</h1><ul><li><a href="http://myhuiban.com" target="_blank" rel="external">会伴</a></li><li>Trans. on BigData的学术文献处理专刊 <a href="https://www.computer.org/csdl/trans/bd/2016/01/index.html" target="_blank" rel="external">Vol. 2 Issue 1</a>，<a href="https://www.computer.org/csdl/trans/bd/2016/02/index.html" target="_blank" rel="external">Vol. 2 Issue 2</a></li><li><a href="http://www.sciplore.org/" target="_blank" rel="external">Sciplore</a></li><li><a href="https://www.scopus.com/" target="_blank" rel="external">Scopus</a></li><li><a href="http://www.docear.org/" target="_blank" rel="external">Docear</a></li><li><a href="https://www.mendeley.com/" target="_blank" rel="external">Mendeley</a></li><li><a href="https://www.zotero.org/" target="_blank" rel="external">Zotero</a></li><li><a href="https://www.teambition.com/" target="_blank" rel="external">Teambition</a></li><li>Todo，如何整理文献，如何管理时间，<a href="https://www.zhihu.com/question/27956707" target="_blank" rel="external">科研小组里有哪些有效的组会形式 - 知乎</a></li></ul><p>如果所在的实验室没有什么积累，暂时没有好的idea/topic，不妨去浏览一下感兴趣的大方向的A类会议近三年或五年的文章列表，<strong>总会觉得</strong> 有几篇比其它文章更有意思，这就把范围缩小一些了；然后再从这几篇文章里提炼出关键词，去<a href="https://en.wikipedia.org/" target="_blank" rel="external">wikipedia</a>上搜索一下这个关键词，再从Related Work再扩展出去，体验一下这个小领域涉及的问题。找出来这个小领域里发文章比较多的作者和研究小组（实验室），去作者和小组的主页看看。这个前期工作其实花不了一周的时间，然后就收集一些相关的论文，粗览一遍，筛选出值得仔细研读的。我们不妨通过相关论文的数量来定义一个小的领域，武断地说100篇或200篇，这个具体的大小不是关键，但一个领域能发的文章必定是有限的，太多的话说明问题太复杂，还要细分，太少的话，如果不是幸运地发现了新的方向，就是问题太Trivial了。而这其中，又只有几篇或十几篇是开创性的，非常值得仔细研读的。<br>如果读完核心的几篇文章后还是没有新的想法，那就只好重复上面的过程，重新寻找另外感兴趣的领域了。</p><blockquote><p>PS，相比上面列出来一堆链接，其实这几句话才是这篇文章的重点啊 ;-)</p></blockquote><p>一些标题有<code>A systematic review on ...</code>综述文章，其中会介绍收集相关论文的过程，方法都类似，可以找一篇当作论文搜集方法的教程来看。</p><p>再列出知乎上的几个相关问题吧</p><ul><li><a href="https://www.zhihu.com/question/26901116" target="_blank" rel="external">如何总结和整理学术文献？</a></li><li><a href="https://www.zhihu.com/question/26857521" target="_blank" rel="external">如何高效管理文献？</a></li><li><a href="https://www.zhihu.com/question/22790506" target="_blank" rel="external">如何写好一篇高质量的IEEE/ACM Transaction级别的计算机科学论文?</a></li></ul><p><a name="hosts"></a></p><h1 id="Bonus-如何访问Google-Scholar"><a href="#Bonus-如何访问Google-Scholar" class="headerlink" title="[Bonus] 如何访问Google Scholar"></a>[Bonus] 如何访问Google Scholar</h1><h2 id="改hosts"><a href="#改hosts" class="headerlink" title="改hosts"></a><strong>改hosts</strong></h2><ul><li>IP v4， https://raw.githubusercontent.com/googlehosts/hosts/master/hosts-files/hosts  或短网址 https://git.io/v5rKk</li><li>IP v6， https://raw.githubusercontent.com/lennylxx/ipv6-hosts/master/host 或短网址 https://git.io/vMjCk</li></ul><h2 id="hosts文件的路径"><a href="#hosts文件的路径" class="headerlink" title="hosts文件的路径"></a><strong>hosts文件的路径</strong></h2><ul><li>Windows：<code>C:\Windows\System32\drivers\etc\hosts</code></li><li>Linux，Mac，Android（均需要root权限）：<code>/etc/hosts</code></li></ul><h1 id="PS-A-Survival-Guide-to-a-PhD"><a href="#PS-A-Survival-Guide-to-a-PhD" class="headerlink" title="PS: A Survival Guide to a PhD"></a>PS: <a href="http://karpathy.github.io/2016/09/07/phd/" target="_blank" rel="external">A Survival Guide to a PhD</a></h1><hr><p>飞鸟集</p><blockquote><p>第83<br>那想做好人的，在门外敲着门，那爱人的，看见门敞开着。</p><p>第142<br>让我设想，在群星之中，有一颗星是指导着我的生命通过不可知的黑暗的。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文（Paper）是每个研究生读研路上挥之不去的“阴云”。&lt;br&gt;无论是否已经有了一个好的课题或想法，都首先要收集某个研究方向一定数量的论文，来了解相关的工作和最新进展（State of the art &amp;amp; practice）。&lt;br&gt;本文介绍了如何检索、收集计算机科学（CS）专业的论文，还介绍了相关的机构，学术会议和论文数据库。&lt;br&gt;文末有 &lt;a href=&quot;#hosts&quot;&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/a&gt; 哦;-)&lt;/p&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>Windows从VHD启动实现极速快照和恢复</title>
    <link href="https://ying-zhang.github.io/cloud/2016/win-vhd-boot/"/>
    <id>https://ying-zhang.github.io/cloud/2016/win-vhd-boot/</id>
    <published>2016-09-21T16:00:00.000Z</published>
    <updated>2017-10-30T03:43:20.473Z</updated>
    
    <content type="html"><![CDATA[<p>把之前发过的<a href="/cloud/2016/vm-disk/index.html#win-vhd-boot">虚拟机及docker的存储，快照和镜像</a>相关部分单独拿出来。正文无内容。<br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;把之前发过的&lt;a href=&quot;/cloud/2016/vm-disk/index.html#win-vhd-boot&quot;&gt;虚拟机及docker的存储，快照和镜像&lt;/a&gt;相关部分单独拿出来。正文无内容。&lt;br&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>虚拟机及docker的存储，快照和镜像</title>
    <link href="https://ying-zhang.github.io/cloud/2016/vm-disk/"/>
    <id>https://ying-zhang.github.io/cloud/2016/vm-disk/</id>
    <published>2016-09-20T16:00:00.000Z</published>
    <updated>2017-10-30T03:41:50.175Z</updated>
    
    <content type="html"><![CDATA[<p>VM的虚拟硬盘让我们能够把整个操作系统和应用软件、配置、数据都 <strong>封装</strong> 在一个（或多个）文件里，这样就实现了VM的迁移，再加上 <strong>差分硬盘</strong> 功能，实现了VM的快速克隆和快照。对VM镜像管理的需求也就随之而来了。</p><a id="more"></a><hr><!-- TOC --><pre><code>- [title: 虚拟机及docker的存储，快照和镜像](#title-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%8F%8Adocker%E7%9A%84%E5%AD%98%E5%82%A8%EF%BC%8C%E5%BF%AB%E7%85%A7%E5%92%8C%E9%95%9C%E5%83%8F)</code></pre><ul><li><a href="#%E8%99%9A%E6%8B%9F%E7%A1%AC%E7%9B%98">虚拟硬盘</a></li><li><a href="#%E9%83%A8%E7%BD%B2%E5%92%8C%E8%BF%81%E7%A7%BB">部署和迁移</a></li><li><a href="#%E5%B7%AE%E5%88%86%E7%A1%AC%E7%9B%98">差分硬盘</a></li><li><a href="#%E5%BF%AB%E7%85%A7%E5%92%8C%E5%85%8B%E9%9A%86"><strong>快照和克隆</strong></a></li><li><a href="#%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86">镜像管理</a></li><li><a href="#docker%E7%9A%84%E9%95%9C%E5%83%8F%EF%BC%8C%E5%AE%B9%E5%99%A8">docker的镜像，容器</a></li><li><a href="#windows%E7%9A%84diskpart%E5%B7%A5%E5%85%B7"><strong>Windows的diskpart工具</strong></a></li><li><a href="#windows%E4%BB%8Evhd%E5%90%AF%E5%8A%A8%E5%AE%9E%E7%8E%B0%E6%9E%81%E9%80%9F%E5%BF%AB%E7%85%A7%E5%92%8C%E6%81%A2%E5%A4%8D"><strong>Windows从VHD启动实现极速快照和恢复</strong></a></li></ul><!-- /TOC --><h1 id="虚拟硬盘"><a href="#虚拟硬盘" class="headerlink" title="虚拟硬盘"></a>虚拟硬盘</h1><p>硬盘被称为 <strong>块设备</strong>，传统的机械硬盘最小的读写单元是一个扇区（sector），而文件系统（FS）在引入了逻辑存储块（block），可能与物理扇区大小相同，也可以是扇区的整数倍。一个文件对应了若干block，多个文件又以文件夹的形式组成一棵树，这是FS的两层结构。</p><p>虚拟硬盘只需要模拟出扇区结构就可以了，上层的逻辑块和文件树由VM里的Guest OS来负责。可以简单地创建一个空白的大文件（raw格式）来作为Guest OS的虚拟硬盘，Guest OS再将其格式化为某种FS。<br>虚拟硬盘在Host上是一个文件（vhd）或若干个相关的文件（vmdk），而其中又有Guest OS创建的文件。</p><p>下面是几种VMM的虚拟硬盘格式。</p><table><thead><tr><th>VMM</th><th>虚拟硬盘格式</th><th>备注</th></tr></thead><tbody><tr><td>VirtualBox</td><td><strong>vdi</strong> , virtual disk image</td><td>还支持vmdk，vhd，qcow等多种格式</td></tr><tr><td>VMware</td><td><strong>vmdk</strong> , vm disk</td><td>vmdk有单文件和多文件2种方式</td></tr><tr><td>Hyper-V</td><td><strong>vhd</strong> / <strong>vhdx</strong>, virutal hard disk</td><td></td></tr><tr><td>KVM</td><td><strong>qcow</strong>, qemu copy on write</td><td>还支持vmdk，raw格式</td></tr></tbody></table><p>各种虚拟硬盘格式大同小异，其中<a href="https://en.wikipedia.org/wiki/VHD_&#40;file_format&#41;" target="_blank" rel="external">vhd格式</a>不仅在vbox和hyper-v中得到支持，Windows7及以后版本的Windows系统中也内置了对它支持，所以用vbox创建VM建议选择vhd格式。Win7及以后的版本内置的<code>diskpart</code>工具（下面会介绍）可以创建vhd的差分镜像，Win8及以后版本可以通过双击挂载vhd镜像。如果vhd镜像的分区是windows支持的NTFS等格式，就可以在不用启动VM的情况下直接读写vhd，这就像把一台机器（VM）的硬盘拆下来拿到其它机器（Host）读写，然后再装回去。</p><p>VMM为了管理虚拟硬盘，还会在虚拟硬盘文件中增加一些元数据信息，比如作为硬盘唯一标识的GUID。在vbox中，如果直接复制一个虚拟硬盘镜像挂载到其它的VM，就会报告已经存在了相同GUID的虚拟硬盘。<br>重置一个虚拟硬盘GUID可以执行下面的命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">VBoxManage internalcommands sethduuid &quot;/path/file.vhd&quot;</div></pre></td></tr></table></figure><blockquote><p>vbox支持多种虚拟硬盘格式，使用<code>VBoxManage.exe</code>这个命令行工具还可以进行不同虚拟硬盘格式之间的转换，启停VM等很多操作。<br>VMware可以将物理硬盘转换为虚拟硬盘，这样实际上是把一个物理机器（部分地）克隆到了VM（PM-&gt;VM）。<br>VMware还可以将一个物理硬盘分区直接加载给VM，这样会稍许提高一下访问硬盘的性能。</p></blockquote><p>虚拟硬盘有固定和动态大小两种：</p><ul><li>前者直接分配了全部硬盘空间，创建的时候会比较费时，而且通常会有大部分硬盘空间是空闲的，优势是所需空间已经预先分配好了，访问性能会好一些；</li><li>而动态大小的虚拟硬盘在创建时是一个很小的文件，只有基本的信息。随着Guest OS使用硬盘，才会逐渐自动扩展，这样能极大节约硬盘空间，但动态分配会降低性能。要注意扩展是自动的，一旦扩展就不会收缩回去，哪怕Guest OS格式化了虚拟硬盘。</li></ul><h1 id="部署和迁移"><a href="#部署和迁移" class="headerlink" title="部署和迁移"></a>部署和迁移</h1><p>在VM出现之前，要想批量地安装OS，也就是部署OS的问题，有两种常用的方法</p><ul><li>使用OS本身支持的<code>应答文件</code></li><li>比较常用的方法就是先在一台机器上安装好系统、驱动、应用软件，修改一些配置，然后用ghost克隆操作系统的分区到一个.gho镜像（所谓的基准镜像golden image），然后分别拷贝这个.gho到其它机器上，启动到pe环境，再从.gho镜像恢复系统；也可以从局域网的某个ghost server服务器上获取.gho镜像来恢复系统。gho系统镜像一般有几个GB，就算自动安装的话也要十几分钟。<br>这两种方法效率上相差不多。</li></ul><p>VM将整个OS放在一个虚拟硬盘文件里，在加上一个很小的VM配置文件（比如xml格式的.vbox文件）就是一个完整的计算机了，而且是统一的虚拟设备驱动，把这些文件拷贝到另一个Host上就可以得到一个新的VM。虽然一个虚拟硬盘文件也有几个GB，但只需要拷贝过去，节省了ghost从镜像恢复这一步，还是方便一些的。不足之处是虚拟化带来的一些性能开销，随着硬件对虚拟化的支持，这些开销已经很小了。</p><p>基于虚拟硬盘实现的VM的封装性是虚拟化技术一个很重要的特性，这样就实现了VM的（离线）迁移。在此基础上，还开发出了VMM的 <strong>在线</strong> 迁移技术，就是在VM不停机的情况下迁移到其它Host上。在线迁移跟离线迁移一样需要拷贝虚拟硬盘文件，区别是在线迁移还要拷贝运行时刻的内存，一般有若干GB，经过网络传输比较耗时，所以内存迁移需要几次迭代才能完成。</p><p>VM的迁移虽然是一个可以提高VM可用性的功能，但毕竟虚拟硬盘还是太大了，传统上除了视频文件，网络上很少有这种大流量传输的需求。VM迁移对已有网络的承受能力是一个考验。还有一个问题是很多虚拟硬盘中的数据都是重复的，占用了大量的存储空间。为了解决这些问题，就出现了下面的差分硬盘功能。</p><h1 id="差分硬盘"><a href="#差分硬盘" class="headerlink" title="差分硬盘"></a>差分硬盘</h1><p>虽然可以预先制作一个标准VM镜像，然后批量拷贝得到很多VM，但VM运行后会在虚拟硬盘中写入不同的数据，所以每个VM的虚拟硬盘内的数据都是不同的，迁移时必须要分别拷贝。即便是给VM添加两个虚拟硬盘，一个OS（系统盘），另一个空白硬盘专门保存用户数据（数据盘），还是难以避免Guest OS在系统盘修改文件数据。</p><p>差分硬盘的思路类似内存管理中的copy on write (COW) 技术。基于父虚拟硬盘创建一个差分硬盘（子盘），VM挂载子盘后，看上去跟父盘完全一样，只是后续增删的数据都是保存在子盘上的。</p><p>子盘还可以再继续创建差分盘（孙盘），这样子子孙孙 <strong>纵向</strong> 延续下去。比如vhd可以支持创建127代，而且 <strong>每代差分硬盘的性能基本一致，不受代数层次的影响</strong>。另外，一个父盘还可以有多个子盘，也就是 <strong>横向</strong> 扩展，同一个父盘下面生成子盘的个数没有限制，因为父子关系信息，即父盘的位置和GUID，只保存在子盘里。横向和纵向的差分盘可能形成一棵树。下面就是VMware的快照管理器截图（快照实际上包括虚拟硬盘快照和VM配置快照两部分，显然硬盘快照是占大头的部分）。<img src="/img/vmware-snapshot.png" alt=""></p><p>差分硬盘相比普通虚拟硬盘在性能上会有一点损失，但给运维带来的便利是很大的。微软有一个关于vhd在windows server 2008和2008 R2多种应用场景下的<a href="http://go.microsoft.com/fwlink/p/?LinkId=186519" target="_blank" rel="external">性能测试的文档</a>，包括物理硬盘 vs 固定分配的vhd vs 动态分配的vhd vs 差分vhd。结果是动态分配的vhd性能下降比较明显，固定分配的vhd和差分vhd的性能都接近物理硬盘。</p><p>创建一个差分硬盘基本不到1秒钟，于是就可以实现秒级创建/克隆/离线迁移VM，秒级创建快照，克隆是针对多个VM，而快照是针对某一个VM的。<br>理论上来说，所有VM公用的父虚拟硬盘可以只保存一个，会减少很多原来虚拟硬盘占用的实际存储空间，当然这个父盘需要保存在可以远程访问的共享存储上，一般是samba/cifs或者NFS共享目录。</p><h1 id="快照和克隆"><a href="#快照和克隆" class="headerlink" title="快照和克隆"></a><strong>快照和克隆</strong></h1><p>上面截图是VMware的快照功能，可以创建出一棵快照树。VMware这个快照管理器中</p><blockquote><p>“删除”：会将选中快照对应的虚拟硬盘数据 <strong>合并</strong> 到它的差分子盘上，然后删除该差分盘。<strong>不会改变</strong> VM的当前状态，只是删除了选中层次的差分硬盘。合并差分盘是一个比较耗时的操作。要求当前的虚拟硬盘只有一个子盘才能合并。<br>“转到”：将当前位置恢复到选中的快照。</p></blockquote><p>vbox的也有类似的快照功能，不过用起来不太直观。</p><p><img src="/img/vbox-snapshot.png" alt="vbox快照"></p><blockquote><p>注：快照工具栏中的按钮分别是<br><code>快照</code>：只能对“当前状态”生成快照，实际是以“当前状态”的虚拟硬盘为父盘，创建一个差分盘，并将VM的虚拟硬盘设置为这个差分盘，即作为新的“当前状态”。<br><code>恢复</code>：基于选择快照的虚拟硬盘创建一个新的差分盘，并将VM的虚拟硬盘设置为这个差分盘，即作为新的“当前状态”。需要在VM停机状态下才能执行“恢复”操作<br><code>删除</code>：这个功能和VMware的“删除”一样。注意这个功能跟“恢复”的区别，我经常想恢复VM，结果搞不清这两个功能，选择了删除，不但耗时，还把中间的快照层次给弄丢了，相当于版本库中丢失了中间某个版本，虽然对当前版本没有影响，但还是丢掉了一些数据的。创建快照是为了尽可能快速恢复，快照越多，恢复的粒度就越细，重用的VM操作/数据就越多，当然管理起来会增加点工作量，通常情况是没有必要合并快照的，因此这个功能是不太常用的，而且称为 <strong>“合并快照”</strong> 比 <strong>删除快照</strong> 更合适些。<br><code>明细</code>：查看创建快照时写的备注和VM的参数变化。<br><code>克隆</code>：基于当前的VM克隆一个新的VM，既可以选择使用差分硬盘方式（连接），也可以使用传统的复制整个虚拟硬盘。</p></blockquote><h1 id="镜像管理"><a href="#镜像管理" class="headerlink" title="镜像管理"></a>镜像管理</h1><p>“镜像”现在有了两层含义，一个是前面一直说的虚拟硬盘，可以在VM运行中修改从而发生变化，一个是作为基准模板的VM，其中最重要的文件是差分硬盘的父盘。<br>所谓基准模板，或基准镜像 golden image ，与普通的VM并没有什么本质的区别，只是被标识了一个特别的身份而已，最多就是把相关的文件（配置，虚拟硬盘等）都打包在一起。</p><p>随之而来的是基准镜像的管理问题，因为基准镜像也不是一成不变的，其中的软件或者用户应用/数据可能需要更新，这一般都是通过在VM里执行系统维护命令来实现的。基准镜像也不只有一个，为了减少重复操作，应该为常见的功能特征都创建好基准镜像，甚至实现开箱可用的目标。镜像管理要给镜像关联尽可能详细的描述信息，让开发人员方便地找的合适的镜像，避免发明轮子。从基础镜像出发，相当于IaaS的模式，而从一个安装和配置了特定功能的镜像开始，则相当于提升到PaaS的层次了。</p><p>Openstack 中的Glance模块是镜像管理模块，vagrant使用的.box也算是一种简单的镜像管理方案，但与docker的镜像管理功能相比还有一定的差距。</p><h1 id="docker的镜像，容器"><a href="#docker的镜像，容器" class="headerlink" title="docker的镜像，容器"></a>docker的镜像，容器</h1><p>docker一开始就通过公共镜像仓库docker hub和私有仓库docker registery，以及<code>build</code>，<code>pull</code>，<code>push</code>命令内置了镜像管理功能。</p><p>docker为实现镜像管理采用了类似于差分硬盘的<a href="http://coolshell.cn/articles/17061.html" target="_blank" rel="external">aufs文件系统</a>。aufs是一个虚拟文件系统，是在某个实际的底层文件系统上重新组织的虚拟文件视图，aufs并不处理底层的硬盘数据块，只是把多个主机上已经存在的不同目录分层次挂载到同一个虚拟的目录下，上层会覆盖下层的同名文件，不同名的文件则会相安无事，这样每个层就类似于差分硬盘的一个子盘。然而aufs是在文件层次实现的，而VM的差分硬盘则是在更底层的数据块层实现的。<br>docker基于镜像创建容器container，容器类似于虚拟机实例，有创建、启动、运行、暂停、停止的生命周期。镜像的层是只读的，容器会重用这些层，然后在最上面新创建一个可读写的层。<br>通过<code>docker commit</code>命令，可以将容器固化成镜像。<br><a name="docker-image"></a><br>docker的镜像管理还有一些不足：</p><ul><li>虽然docker会管理单个Host下的镜像，利用graph功能重用已有的镜像层，但每个Host还是有重复的镜像拷贝。虚拟机为了实现在线迁移，要求VM镜像保存在一个共享目录，恰好实现了部分的去重效果。不过docker有私有的仓库，而且能跟共享目录结合起来就好了。<a href="https://www.usenix.org/node/194431" target="_blank" rel="external">FAST16 - Slacker: Fast Distribution with Lazy Docker Containers</a> 这篇文章为了加速Docker容器的启动速度，采用了集中的共享存储镜像仓库和惰性加载镜像的方式，只加载当前使用的镜像到本地，与我们的思路很相近了。另外，Docker镜像分层的结构还可以更精细，以提高重用的可能。</li><li>从dockerfile构造镜像时，每行<code>RUN</code>命令对应一层镜像，需要注意避免分层太多，而一个<code>RUN</code>命令太复杂又会限制了重用镜像的可能，如果能够 <strong>显示地指定分层点</strong> 可能会好些，这也算是镜像分层结构精细化的一部分。</li><li>从dockerfile构造镜像也不是完全可重现的，如果基于同一个dockerfile的2次构建相隔了比较长的时间，而dockerfile中又要从外部获取应用或数据，比如<code>apt</code>安装软件，有可能安装的软件版本就不同了，而且这还涉及到层的重用。docker目前已经通过计算镜像各层内文件的内容的Hash来唯一地确定该层，部分缓解了这一问题。</li></ul><p><code>docker run</code>命令基于镜像创建并运行一个容器。新手容易通过<code>docker run</code>创建很多临时的容器，占用过多物理硬盘空间，这需要在执行<code>docker run</code>命令时加上<code>--rm</code>参数，容器退出后就会被docker删除掉。</p><p>docker官方建议开发人员应尽量避免通过<code>docker exec</code>或<code>ssh</code>进入容器执行操作，而应通过dockerfile将所有操作<code>build</code>到镜像中。这样的好处是所有的操作都在dockerfile中，从而可以被版本管理系统跟踪。对VM的操作一般是经chef，puppet，ansible或salt这类自动化工具，在多个VM上批量执行的。自动化工具的问题是每个VM都需要重复执行，不同VM执行可能会产生错误或不同的结果，而dockerfile只要build一次，然后基于镜像创建容器即可，能更好的保证一致性，而且节约了重复执行的时间和网络流量（没有缓存的情况下）。<br>有人将VM比喻为宠物：一个Host只能支持若干个VM，所以开发人员都小心翼翼地像宠物一样对待VM；而一个Host可以支持多得多的容器，对待容器就像家畜一样，生死都不足惜。</p><p><a name="win-vhd-boot"></a></p><h1 id="Windows的diskpart工具"><a href="#Windows的diskpart工具" class="headerlink" title="Windows的diskpart工具"></a><strong>Windows的diskpart工具</strong></h1><p>因为曾被vbox的“恢复”和“删除”搞晕，所以之前都是用windows的<code>diskpart</code>来创建差分硬盘，然后重新设置VM的硬盘，使用下面的命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">C:\&gt;diskpart</div><div class="line"></div><div class="line">DISKPART&gt; create vdisk file=c:\diff.vhd parent=c:\parent.vhd</div></pre></td></tr></table></figure><h1 id="Windows从VHD启动实现极速快照和恢复"><a href="#Windows从VHD启动实现极速快照和恢复" class="headerlink" title="Windows从VHD启动实现极速快照和恢复"></a><strong>Windows从VHD启动实现极速快照和恢复</strong></h1><p>Windows对vhd的内置支持远不止创建，挂载。最大的特色是支持从vhd启动（vhd native boot）：就是说把windows系统安装到一个vhd中，向普通硬盘分区一样从这个vhd启动windows。</p><ul><li>一种做法是把windows安装文件写入vhd中，然后将其加入BCD启动项，重启后系统会安装vhd在内。</li><li>另一种做法比较费事，需要用安装盘中的维护工具，在命令行执行diskpart创建vhd，挂载并格式化，然后就可以像物理硬盘一样把系统装到vhd了。</li><li>还有一种做法，将已经安装在物理硬盘上的系统用ghost克隆到vhd上，这需要在另一个windows系统或PE下进行，然后添加这个vhd的启动项到BCD。</li></ul><p>vhd加上差分硬盘功能，就可以实现windows的快速恢复了。具体的一种做法是基于一个父vhd创建两个差分vhd，分别称为<code>Current.vhd</code>和<code>Recovery.vhd</code>，分别添加这两个差分vhd的启动项为 <strong>Current</strong> 和 <strong>Recovery</strong>，这样就相当于已经安装了两个windows系统。正常使用 <strong>Current</strong> 系统，需要恢复时重启到 <strong>Recovery</strong> 系统，删除原来的<code>Current.vhd</code>，然后再基于父vhd创建一个新的差分vhd并命名为<code>Current.vhd</code>，再重启到 <strong>Current</strong> 系统，就完成了系统恢复。其中Recovery系统不是必须的，因为只需要能够进行简单的文件操作，用一个PE系统代替也可以。</p><p>系统盘的文件布局如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">C:.</div><div class="line">│  Win8-Current.vhd    #正常工作系统</div><div class="line">│  Win8-Recovery.vhd   #恢复用系统</div><div class="line">│  Win8.vhd            #父vhd</div><div class="line">│  bootmgr             #启动管理器  </div><div class="line">└─boot                #系统启动相关文件</div><div class="line">        bcd</div><div class="line">        memtest.exe</div><div class="line">        ...</div><div class="line"></div></pre></td></tr></table></figure></p><p><code>bootice</code>是一个操作vhd和系统启动项的小工具。此外还有<code>EasyBCD</code>和一些PE工具。当然，可以直接使用Windows内置的命令行工具<code>bcdboot</code>和<code>bcdedit</code>。</p><p>Linux <strong>还没有</strong> 类似 vhd native boot的功能。虽然grub支持ramdisk或iso启动项，但功能上还是有些差距。<br>最近推出的CoreOS采用了双分区滚动升级的做法，实现了类似的功能，不过需要占用两倍的系统存储空间（没有差分），好在CoreOS本身比较精简。</p><p>前面提到可以将物理硬盘转换成虚拟硬盘，相当于实现了从物理机到虚拟机的转换 PM -&gt; VM；而 vhd native boot则相当于从 VM -&gt; PM。</p><hr><p>这个介绍数据中心的<a href="http://v.youku.com/v_show/id_XMTMyMTI1ODQ4NA==.html" target="_blank" rel="external">视频</a> 里面有一段关于替换故障硬盘的细节值得注意。服务器的CPU，内存，硬盘和网卡 这几个主要部件里，因为硬盘是机械部件，而且一直在运转，不像普通机器大部分时间关机或休眠，所以是最容易出故障的，换硬盘应该是数据中心的日常维护工作了。 即便升级成没有了机械转动部件的SSD，还是有写入寿命的限制。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;VM的虚拟硬盘让我们能够把整个操作系统和应用软件、配置、数据都 &lt;strong&gt;封装&lt;/strong&gt; 在一个（或多个）文件里，这样就实现了VM的迁移，再加上 &lt;strong&gt;差分硬盘&lt;/strong&gt; 功能，实现了VM的快速克隆和快照。对VM镜像管理的需求也就随之而来了。&lt;/p&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>虚拟机及docker的网络连接</title>
    <link href="https://ying-zhang.github.io/cloud/2016/vm-net/"/>
    <id>https://ying-zhang.github.io/cloud/2016/vm-net/</id>
    <published>2016-09-19T16:00:00.000Z</published>
    <updated>2017-10-30T03:41:35.085Z</updated>
    
    <content type="html"><![CDATA[<p>网络是云计算中重要的基础设施。这里比较了VirtualBox（简写为vbox），VMware，Hyper-V，KVM这些虚拟机管理器（VMM）及docker的网络连接方式。</p><a id="more"></a><hr><!-- TOC --><pre><code>- [title: 虚拟机及docker的网络连接](#title-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%8F%8Adocker%E7%9A%84%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5)</code></pre><ul><li><a href="#%E8%B7%AF%E7%94%B1%E5%99%A8%EF%BC%8Cnat%EF%BC%8C%E4%BA%A4%E6%8D%A2%E6%9C%BA">路由器，NAT，交换机</a></li><li><a href="#%E5%8D%95%E6%9C%BA%E7%9A%84%E7%BD%91%E7%BB%9C%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2%EF%BC%88nat%EF%BC%89">单机的网络地址转换（NAT）</a></li><li><a href="#nat%E7%BD%91%E7%BB%9C">NAT网络</a></li><li><a href="#host-only-%E4%BB%85%E4%B8%BB%E6%9C%BA">Host-Only 仅主机</a></li><li><a href="#%E6%A1%A5%E6%8E%A5">桥接</a></li><li><a href="#docker%E7%9A%84%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F">Docker的桥接模式</a></li><li><a href="#%E5%86%85%E9%83%A8%E7%BD%91%E7%BB%9Clan%E5%8C%BA%E6%AE%B5">内部网络/LAN区段</a></li><li><a href="#%E9%80%89%E6%8B%A9%E5%93%AA%E7%A7%8D%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F%EF%BC%9F">选择哪种连接方式？</a></li><li><a href="#bonus">Bonus</a></li></ul><!-- /TOC --><h1 id="路由器，NAT，交换机"><a href="#路由器，NAT，交换机" class="headerlink" title="路由器，NAT，交换机"></a>路由器，NAT，交换机</h1><p>先简单介绍一下<a href="https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2" target="_blank" rel="external">NAT</a>。<br>家用的无线路由器就使用了NAT。连接到同一个无线路由器的手机，电脑等设备在使私有IP网段的局域网（LAN）中的多个设备经路由器的外网（WAN）IP访问外网。常见的家用无线路由器一般会使用192.168.0.0，192.168.1.0这样的IP网段。下面是从TP-LINK网站上找的一个图。路由器的WAN口接到了电信或者联通这些运营商提供的接口上，计算机可以用有线连接到路由器的LAN口，手机、平板也可以通过无线wifi信号连接到路由器。</p><p>接到路由器上面的设备组成了一个局域网（LAN），这些设备彼此直接可以通过私网IP直接通信（Windows主机之间还可以通过机器名来访问），它们连接外网时则共享WAN口的IP，从外部来看，所有的连接都来自同一个IP。如果访问<a href="http://ip.cn" target="_blank" rel="external">ip.cn</a>，显示的IP跟当前计算机的IP是不同的。</p><p>网络基础课上讲到IP v4有三类私有IP地址：</p><ul><li>10.0.0.0 ~ 10.255.255.255      （A类）；</li><li>172.16.0.0 ~ 172.31.255.255    （B类）；</li><li>192.168.0.0 ~ 192.168.255.255  （C类）；</li></ul><p><img src="/img/wifi-router.png" alt="家用无线路由器拓扑"></p><p>接到运营商的路由器WAN口有一个IP地址，一般是一个Internet（公网）上的动态IP，这个WAN口的IP不是固定的，每次重新连接可能会变化。如果需要一个静态的公网IP，一般则需要向运营商申请。</p><p>在学校实验室的网络连接与此类似，每台机器都有一个公网的IP地址。访问<a href="http://ip.cn" target="_blank" rel="external">ip.cn</a> 或者在ubuntu上执行 <code>curl ip.cn</code> 可以查到这个公网IP，也可以执行<code>ip a</code> 、<code>ifconfig</code> 命令来查询。公网IP理论上是可以被外界访问到的，但学校出口的防火墙屏蔽了外部 <strong>发起</strong> 的访问，只有经授权的IP和端口才能被外部访问，这样就减少了被外部攻击的可能，也就不能随便搭网站了。<br>曾经家里使用中国移动的宽带，分配的WAN口是一个B类的私有IP地址，也就是相当于我家的路由器又连到了中国移动的路由器的局域网里了，这样外部是不能访问这个WAN口IP的。</p><blockquote><p>学校访问外网先要登陆“网络接入认证系统”，这个系统登陆后只允许登陆设备的IP访问外网，而且不允许多台设备（多个IP）同时登录。如果自己先用路由器接到学校的网络上，然后再把多台设备接到自己的路由器上，只要这些设备中有一台登陆了“网络接入认证系统”，其它设备也就可以访问外网了。因为从认证系统看来，这些设备的IP地址都是路由器WAN口的IP地址。</p></blockquote><p>另外，路由器本身除了有WAN口IP之外，还有一个LAN口IP，一般是192.168.0.1/192.168.1.1，这样我们才可以通过这个IP登陆到路由器上修改相关设置。</p><p>那么问题来了：<br>1、路由器LAN的设备可以访问外网，这是没有问题的，但外网怎么访问私有IP的内部设备呢？<br>2、外网怎么区分路由器LAN的不同设备呢？</p><p>实际上外部网络什么也不用干，它看到的只是路由器的WAN口IP。这两个问题都是路由器通过NAT（网络地址转换）来解决的。内网向外网发送一个数据包时，路由器把数据包的源地址（实际是上还有端口号，即IP:Port格式，比如默认的http是80端口），也就是内网设备的私网IP改成自己的WAN口IP，并给不同的设备（随机）分配不同的端口号，并把随机端口号与内网设备的IP:Port的映射关系保存下来。路由器接收到外部返回的应答数据包后，根据端口号查询到实际的内网设备IP:Port，再改写数据包的目标地址，转发给LAN上的内网设备。因此NAT是分两个方向的，分别称为源NAT (SNAT) 和 目标NAT (DNAT)。</p><p>如果外网直接访问路由器的WAN口IP，路由器找不到端口映射关系，就会直接drop掉这个数据包，导致外网不能直接访问内部设备。NAT默认要求一个网络通信必须由内部设备发起。也可以在路由器中设置好固定的端口映射，这样路由器就知道该转发给哪个设备，当然它还是可以判断出来是内部设备主动发起的通信，还是由外部设备发起的。还有一种DMZ技术，让路由器直接把一台内网设备暴露给外网，一般总还是有一些空闲的端口的，所以DMZ不会影响其它内网设备的联网。</p><p>把路由器的WAN口那一块去掉，剩下的LAN口那部分功能就是一个交换机了，交换机所有的端口都是对等的，所连接的设备组成了一个LAN。<br>网络基础课上会介绍交换机是二层设备，路由器是三层设备。<strong>所谓二层，即物理链路层，最常见的就是以太网，设备之间以MAC地址区分；三层是网络层，设备之间以IP地址区分。三层的数据是封装在二层之中的。交换机只看数据包的MAC首部，而路由器则只看IP首部。但要是交换机偷看了IP首部也不会爆炸，而是变成更高级的三层交换机来抢路由器的生意了。具体三层交换机跟路由器的差别，因水平有限就不谈了。</strong> 网络编程基本都是在三层（IP）四层（TCP/UDP），很少直接接触二层的（MAC）。<br>另外，家用路由器一端是公网，另一端是不能直接路由的私网IP段，而数据中心或电信路由器各端口（不止两个）连接的一般都是公网，可以直接路由，不需要使用NAT。</p><center>~</center><p>下面的表格是几种虚拟机管理器（VMM）支持的网络连接方式，同一行的网络连接方式实现的功能是基本相同的，不过在不同的VMM里叫法有所不同。</p><p><img src="/img/vnet.png" alt="几种虚拟机的网络连接方式"></p><blockquote><p>注：<strong>NAT网络</strong>，Host-&gt;VM的情况，VMware在Host创建了虚拟网卡，不需要端口映射，Host可以直接通过IP来访问VM。</p></blockquote><h1 id="单机的网络地址转换（NAT）"><a href="#单机的网络地址转换（NAT）" class="headerlink" title="单机的网络地址转换（NAT）"></a>单机的网络地址转换（NAT）</h1><p> 只有vbox支持 <strong>单机NAT</strong> 方式。这种方式与下面的 <strong>NAT网络</strong> 的唯一区别是，不同的VM之间不能互相通信。这个特性是为了方便创建多个单机VM，而不会彼此干扰。<br> 虽然是针对单机的，实际上NAT也有多个网络设备，包括一个网关，IP是<code>10.0.2.2</code>，一个DHCP服务器，也是<code>10.0.2.2</code>，VM的IP一般是<code>10.0.2.15</code>。<br> 不管使用NAT的有多少个VM，它们的IP都是一样的。</p><p> 一个VM可以设置多个网卡，如果这些网卡有多个使用NAT模式，那么它们的IP区段就会递增，分别为<code>10.0.2.0/24</code>，<code>10.0.3.0/24</code>等。不过同一个VM设置多个NAT网卡并没有什么必要。</p><p> VM可以直接访问Host，默认IP也是<code>10.0.2.2</code>，但这个IP在Host是看不到的，即Host跟VM不在同一个LAN，Host不能直接访问VM的IP。<br> Host要想访问VM，需要设置端口映射，即设置<code>HostIP:HostPort</code>与<code>VM-IP:VM-Port</code>的关联，这样Host就可以通过<code>HostIP:HostPort</code>来访问VM的端口了。如果映射的HostIP是可以从外部路由的，那么外部也可以通过<code>HostIP:Host-Port</code>来访问VM的指定端口。<br> 可以添加多个映射规则来暴露不同的端口。</p><h1 id="NAT网络"><a href="#NAT网络" class="headerlink" title="NAT网络"></a>NAT网络</h1><p> 在VMware中则直接称为 <strong>NAT</strong> 。vbox刻意把这 <strong>NAT</strong> 和 <strong>NAT网络</strong> 这2种方式区分开，可能是因为他们认为不仅需要实现网络连通，还要能够实现隔离吧。<br> vbox需要在 <strong>全局配置-&gt;网络</strong> 中增加一个 <strong>NAT网络</strong> 的虚拟网卡才能使用NAT网络，但这个网卡在Host的网络设备里是看不到的。添加的第一个NAT网络的网段是<code>10.0.2.0/24</code>，网关是<code>10.0.2.1</code>，DHCP服务器是<code>10.0.2.3</code>，Host是<code>10.0.2.2</code>，VM的IP是由DHCP自动分配的。<br> 连接到同一个NAT网络的VM组成了一个LAN。这种情况跟前面提到的家用路由器的连接是很相似的，连接到路由器的多个设备是在一个私有IP网段的LAN，路由器有一个外网的IP，各种设备可以通过路由器连接外网，不做端口映射的话，外网不能直接访问内网的设备。</p><blockquote><p>外网是可以与内网通信的，否则我们就不能看到网站返回的网页了，但必须要内网设备发起连接，外网响应。这是因为内网的IP是私有的，在 <strong>公网</strong> 上是不可路由的。<br>这里 <strong>外网</strong> 是Host之外的网络，它可能是一个Internet IP（<strong>公网</strong>），也可能是某个公司内部的私有IP地址的网络。<br>如果可以修改 <strong>外网</strong> 的路由的话，还是可以实现不做端口映射，通过路由协议来路由到内网设备的，但一般只有运营商才有这样的能力。</p></blockquote><p> 连接到不同的虚拟网卡的VM，即便IP网段是相同，也不能连通。<br> vbox可以添加多个NAT网络，它们可以使用相同的默认IP网段<code>10.0.2.0/24</code>，这不会彼此产生冲突，当然也可将其网段改为其它地址，如<code>10.0.3.0/24</code>，或<code>192.168.2.0/24</code>这样的，但<code>192.168.x.x</code>网段的DHCP可能不能正常工作，需设置静态IP。</p><p> VMware的NAT网络设置有所不同。VMware安装后，会在（Windows）Host添加一个VMnet8的虚拟网卡，工作在NAT模式。在VM中选择网卡为VMnet8和设置使用NAT模式的效果是一样的。有了这个虚拟网卡，Host也就在NAT网络的LAN里面了，所以VMware的NAT网络模式下Host是可以直接访问VM的，这点比vbox要方便些。</p><p> VMware内置了VMnet0 ~ VMnet19共20个虚拟网卡可用，每个虚拟网卡对应了一个虚拟LAN，可以工作在 <strong>NAT</strong>、<strong>仅主机</strong> 和 <strong>桥接</strong> 三种不同的模式之一，但又 <strong>限制只能有一个虚拟网卡工作在NAT模式</strong> （这个限制是很奇怪的）。不过VMware在NAT模式下可以更改 <strong>默认网关的IP</strong>，及DHCP、DNS的设置。</p><blockquote><p>注意，vbox在两种NAT模式下都有一个坑：<br>VM在使用DHCP分配IP时能正常访问外网，但如果在VM中设置静态IP，<strong>即便这些值与DHCP分配到的值一模一样，也不能访问外网！</strong><br>虽然不能访问外网，但内网能正常访问，说明可能是DNS设置的问题。</p><p>在这个<a href="http://geekynotebook.orangeonthewall.com/configure-static-ip-on-nat-in-oracle-virtualbox/" target="_blank" rel="external">博客</a> 中介绍了同样的问题（博客里的DNS IP <code>10.0.2.3</code>和<code>/etc/resolve.conf</code>设置在ubuntu上不能工作）。<br>对Ubuntu，需在 <code>/etc/network/interfaces</code> 设置网卡</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">auto eth0</div><div class="line">iface eth0 inet static</div><div class="line">address 10.0.2.15</div><div class="line">netmask 255.255.255.0</div><div class="line">gateway 10.0.2.2</div><div class="line"></div><div class="line">dns-nameservers 10.0.2.1</div></pre></td></tr></table></figure><blockquote><p><strong>设置静态IP后，VM还要执行下面2条命令才会使用Host的DNS！</strong><br>vbox在DHCP模式下会自动使用Host的DNS，但设置静态IP后默认不再使用Host的DNS，导致VM无法连接外网。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">VBoxManage modifyvm &quot;VM-Name&quot; --natdnsproxy1 on</div><div class="line">VBoxManage modifyvm &quot;VM-Name&quot; --natdnshostresolver1 on</div></pre></td></tr></table></figure><blockquote><p>其中<code>VBoxManage</code>是vbox的命令行管理工具，在vbox的安装目录下，默认位置是<code>C:\Program Files\Oracle\VirtualBox\</code>。</p><p>vbox 5.0.0 版有这个坑，而5.0.2 及以后的版本，<strong>NAT网络</strong> 方式已经不需要执行上面2条命令了，但<code>/etc/network/interfaces</code>里dns-nameservers的设置还是需要的。<br>另外，经过实验发现gateway设置为<code>10.0.2.1</code> 或 <code>10.0.2.2</code>都可以连网，但DNS必须是<code>10.0.2.1</code>。</p><p>VMware没有这个坑。</p></blockquote><h1 id="Host-Only-仅主机"><a href="#Host-Only-仅主机" class="headerlink" title="Host-Only 仅主机"></a>Host-Only 仅主机</h1><p> 这种连接方式是4种VMM都支持的，它的使用很简单。<br> Host-Only模式下，vbox、VMware和Hyper-V都会在Host添加一个虚拟网卡，使用同一个虚拟网卡的VM会连接到同一个虚拟LAN，而且Host也在这个LAN。Host，VM之间都可以方便的连通，不需要端口映射，但VM不能访问外网。<br> 当我们创建了多个VM，并把这些VM连接到一个虚拟LAN，这就算是一个小型的 <strong>虚拟 数据中心</strong> 了。一个现实的数据中心里，除了多台服务器，还有 <strong>交换机</strong>，集中式存储设备，比如iSCSI，SAN，NAS等。一个机架里的多台服务器连接到机架顶部的交换机（ToR），多个机架交换机再连接到核心交换机。<br> 实际上数据中心的网络结构是很复杂的，并不只有一个LAN，而是分成了外部网络、内部业务网络、管理网络、存储网络等多个网络，还会划分成多个子网。<br> Host-Only连接方式物相当于机架顶部的交换机（ToR），所以Hyper-V的叫法：<strong>内部交换机</strong> 是很贴切的。</p><p> 还可以添加多个虚拟网卡，组成多个彼此隔离的LAN（Host分别有一个虚拟网卡挂在每个LAN中）。</p><p> 利用Windows的网络共享功能，将外部网络共享给Host的虚拟网卡，然后将VM设置为静态IP，网关和DNS设置为<code>192.168.137.1</code>，这样就可以连接外网了，但只能共享给一个虚拟网卡，而且IP网段必须是<code>192.168.137.0/24</code>。<br> 复杂点的办法是在Host配置NAT，以Hyper-V为例，它没有NAT网络，参考<a href="https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/user-guide/setup-nat-network" target="_blank" rel="external">Set up a NAT network for Hyper-V</a>文档，在Powershell下以管理员权限执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">New-VMSwitch -SwitchName &quot;NAT&quot; -SwitchType Internal</div></pre></td></tr></table></figure></p><p>查看interfere index：<code>Get-NetAdapter</code>，即下面命令中虚拟网卡的<code>-InterfaceIndex 38</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">New-NetIPAddress -IPAddress 10.0.0.2 -PrefixLength 24 -InterfaceIndex 38</div><div class="line">New-NetNat -Name NATx -InternalIPInterfaceAddressPrefix 10.0.0.0/24</div></pre></td></tr></table></figure></p><p>这样就创建了一个可以通过Host以NAT方式访问外网的 <strong>内部交换机</strong>，不过这个内部交换机还缺少DHCP服务，需要为VM静态分配IP。</p><h1 id="桥接"><a href="#桥接" class="headerlink" title="桥接"></a>桥接</h1><p> 桥接是最方便的一种连接方式。它需要绑定到物理网卡。如果Host的网络连接正常，那么VM一般也就没什么问题了，但如果Host的网络出现了故障，那么VM也无法联网了，即便是同一个Host的VM之间也不能正常通信。<br> 桥接模式下VM与Host的地位是完全对等的。从外部看，VM就像一台真实的机器一样，有自己的MAC和IP。这样极大地简化了网络结构。<br> 不过，在学校的网络环境下，每个IP需要登录web认证后才能访问外网，每个桥接的VM不能都需要登录，而每个账号只能同时登录一个IP的限制导致只能有一个台机器能连上外网，所以不适合采用桥接。</p><p> 另外，虽然名叫 <strong>桥接</strong>，但vbox和VMware都没有用到“网桥”。桥接实际是用所谓的网卡的“混合模式”实现的，即一个网卡可以伪装成不同MAC地址的多个网卡。在Hyper-V中倒是添加了一个虚拟网桥和一个虚拟网卡，功能上是一样的。<br> KVM的各种网络连接方式都需要设置一个bridge，区别在于这个bridge与物理网卡（如eth0）的连接及路由设置。KVM的网络连接不是像vbox，VMware或Hyper-V那样由VMM实现的，而是利用了 <strong>已有的</strong> Linux的bridge-utils，TUN/TAP，iptables等功能。</p><p><a name="docker-bridge"></a></p><h1 id="Docker的桥接模式"><a href="#Docker的桥接模式" class="headerlink" title="Docker的桥接模式"></a>Docker的桥接模式</h1><blockquote><p>注意：docker的网络连接方式也有 <strong>桥接</strong>，但实际上它的工作模式是 <strong>NAT网络</strong>。docker在添加了一个docker0网桥，IP网段在<code>172.17.0.0</code>，如果外网要访问容器，需要做端口映射。这样的网络连接方式下，<strong>不同Host</strong> 的容器是不能直接通信的，这是个很大的局限。<br> 实际上docker也可以实现KVM那样的真正的 <strong>桥接</strong> ，具体可参考文章 <a href="http://my.oschina.net/astute/blog/293944" target="_blank" rel="external">桥接模式构建 docker 网络</a> 和<a href="http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/" target="_blank" rel="external">Four ways to connect a docker container to a local network</a> 。看起来这样的连接方式还是比较实现容易的，但目前docker还没有在官方的实现中直接支持这种连接方式。</p></blockquote><h1 id="内部网络-LAN区段"><a href="#内部网络-LAN区段" class="headerlink" title="内部网络/LAN区段"></a>内部网络/LAN区段</h1><p> 这种方式下只有连接到同一内部网络的VM之间能够通信，而VM与Host，与外网都不能通信。<br> 一个内部网络是由网络名来区分的。<br> VMware和Hyper-V的内部网络可以设置网络带宽，丢包率等，方便进行网络方面的实验，不过作为其它场景的实验环境就不太合适了。<br> 内部网络不支持DHCP，可以专门添加一个多网卡的VM作为网络服务器，完成DHCP，网关，路由等功能。</p><h1 id="选择哪种连接方式？"><a href="#选择哪种连接方式？" class="headerlink" title="选择哪种连接方式？"></a>选择哪种连接方式？</h1><ul><li><strong>NAT网络</strong>            ：优点是可以充分共享Host的网络连接，包括VPN；不足是Host和外部访问VM需要端口映射；VM的IP地址不受外部影响，适合搭建 <strong>实验环境</strong> 。</li><li><strong>Host-Only仅主机</strong>    ：优点是Host可以方便地访问VM；不足是不能访问外网；VM的IP地址也不受外部影响，如果NAT网络需要较多的端口映射，可以考虑每台VM设置2个网卡，一个工作在NAT网络模式，另一个工作在Host-Only模式。</li><li><p><strong>桥接</strong>                ：优点是设置简便，很容易实现互联互通；不足是不能共享Host的网络连接（web登录认证，VPN），而且VM的IP地址受外部影响，更换了网络环境，IP地址可能会发生变化。假设用笔记本电脑搭建的实验环境，VM使用桥接模式，在宿舍的IP地址和在会议室的IP地址是不同的，依赖IP的设置都要修改，是很不方便的。</p><p>在 <strong>数据中心</strong> 里，网络环境不经常变化，<strong>桥接</strong> 模式屏蔽了VMM的影响，可以直接应用已有的交换机设备和配置，简化了网络配置操作。如果交换机支持VLAN，那么可以启用VLAN作为VM的网络隔离，虽然有不能超过4096（ $ 2^{12} $，12 bit）个VLAN的数量限制，但对企业内部的私有云场景应该是足够的。<br>此外，虚拟网络还要考虑IP地址的容量，分配和管理方式避免冲突，多租户的隔离，以及故障转移后的IP重用策略等，了解的不多，这里就不多说了。</p></li></ul><h1 id="Bonus"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus</h1><p><strong><a href="/doc/The_Datacenter_as_a_Computer_An_Introduction_to_the_Design_of_Warehouse_Scale_Machines_2e_2013.pdf">The Datacenter as a Computer - An Introduction to the Design of Warehouse-Scale Machines - 2e - 2013</a></strong></p><p><img src="http://www.infoq.com/cn/presentations/aws-data-center-and-vpc-secret" alt="AWS 数据中心与 VPC 揭秘 - QCon Beijing 2017 - infoq"></p><p><img src="/img/google_dc1.jpg" alt="Google的数据中心"></p><p><img src="/img/google_dc2.jpg" alt="Google的数据中心"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;网络是云计算中重要的基础设施。这里比较了VirtualBox（简写为vbox），VMware，Hyper-V，KVM这些虚拟机管理器（VMM）及docker的网络连接方式。&lt;/p&gt;
    
    </summary>
    
      <category term="cloud" scheme="https://ying-zhang.github.io/categories/cloud/"/>
    
    
  </entry>
  
  <entry>
    <title>双拼输入法</title>
    <link href="https://ying-zhang.github.io/misc/2016/udpnuurufa/"/>
    <id>https://ying-zhang.github.io/misc/2016/udpnuurufa/</id>
    <published>2016-09-18T16:00:00.000Z</published>
    <updated>2017-10-30T02:46:13.717Z</updated>
    
    <content type="html"><![CDATA[<p>双拼输入法是一个很典型的例子：一直就在我们日常使用的软件之中“隐藏”的功能，不为人所熟知，但是一旦掌握了，用起来就会很方便。<br>除了介绍双拼输入法，还有一点关于语音输入的杂想。<br><a id="more"></a></p><hr><!-- TOC --><pre><code>- [title: 双拼输入法](#title-%E5%8F%8C%E6%8B%BC%E8%BE%93%E5%85%A5%E6%B3%95)</code></pre><ul><li><a href="#%E5%8F%8C%E6%8B%BC%E6%96%B9%E6%A1%88">双拼方案</a><ul><li><a href="#%E5%A3%B0%E6%AF%8D">声母</a></li><li><a href="#%E9%9F%B5%E6%AF%8D">韵母</a></li></ul></li><li><a href="#%E5%8F%8C%E6%8B%BC%E8%BE%93%E5%85%A5%E6%B3%95%E7%9A%84%E8%BD%AF%E4%BB%B6">双拼输入法的软件</a></li><li><a href="#%E5%85%B3%E4%BA%8E%E8%AF%AD%E9%9F%B3%E8%BE%93%E5%85%A5%E7%9A%84%E6%9D%82%E6%83%B3">关于语音输入的杂想</a></li></ul><!-- /TOC --><p><a href="https://zh.wikipedia.org/wiki/%E5%8F%8C%E6%8B%BC" target="_blank" rel="external">双拼</a>输入法利用了汉语拼音的一个基本属性：包括声母和韵母两个部分。双拼输入法每输入一个汉字都要（最多）两次击键，第一次为声母，第二次为韵母，平均起来比全拼减少了击键次数，比简拼可能相差不多，但简拼一般是的特定短语，否则重码太多，双拼则没有这个限制。<br>双拼能不能显著提高输入汉字的速度没有定论，但它能保证输入速度 <strong>不会太慢</strong>，最重要的一点是每个字都是敲两个键，有很好的节奏感，让思路能很自然流畅地敲出来。</p><blockquote><p>本文主要内容的<a href="/doc/shuang_pin_udpn_cheetsheet.pdf">一页纸PDF文件，亦称Cheetsheet</a></p></blockquote><h1 id="双拼方案"><a href="#双拼方案" class="headerlink" title="双拼方案"></a>双拼方案</h1><p>将汉语拼音中声母和韵母分别对应到键盘上的26个英文字母，这就是双拼方案。下面是微软双拼的方案。</p><h2 id="声母"><a href="#声母" class="headerlink" title="声母"></a>声母</h2><p><code>b p m f d t n l g k h j q x r z c s y w</code>这些单声母都是直接与各自的字母键对应的，对<code>zh ch sh</code>这三个，对应关系是<code>i：ch</code>，<code>u：sh</code>，<code>v：zh</code>；有的拼音不需要声母，比如<code>爱 ai</code>，只有韵母<code>ai</code>，为了保持编码规律，指定它们为 <strong>零声母</strong>，并对应为按键<code>o</code>。</p><h2 id="韵母"><a href="#韵母" class="headerlink" title="韵母"></a>韵母</h2><p>韵母的对应关系稍微复杂一点，</p><ul><li><code>a e i o u</code>这几个元音的对应比较简单；</li><li>对于复杂韵母，<ul><li><code>u o</code>相关的多在第一行键位，</li><li><code>a e</code>相关的多在第二行键位，</li><li><code>i</code>  相关的多在第三行键位。</li></ul></li></ul><p>下面的韵母键位需要记忆。初学时可以把这个图打印出来贴到屏幕边，或者拼到壁纸的一角，需要查看提示时按<code>Win+D</code>显示桌面即可。一般练习一两天就记住了。</p><p><img src="/img/udpn-ms-map.png" alt="微软双拼韵母键位"></p><p>例如，输入“我爱双拼输入法”，对应的击键如下表，</p><p><img src="/img/udpn-demo.png" alt="微软双拼示例"></p><p>说明：</p><ol><li>大部分声母都有直接对应的键位，只有<code>zh ch sh</code>需特别记忆一下。有的拼音没有声母，如<code>爱</code>，这时就需要零声母，即字母<code>o</code>，以表明后面的键是韵母；</li><li>韵母<code>a  e  i  o  u</code>直接与各自的字母键对应，其它韵母的对应关系则需要记忆，注意，韵母<code>ing</code>对应分号键<code>;</code>。</li></ol><p>双拼下也可以使用简拼，但需要用 <code>&#39;</code> 作为分隔符划分音节。如果你刚才 <strong>整句</strong> 地输入过“我爱双拼输入法”，那么试试<code>w&#39;i&#39;u&#39;p&#39;u&#39;r&#39;f</code> ，看看你的输入法软件够不够智能。说实话，简拼用<code>&#39;</code>实在不够方便。如果连续敲的两个键不能组成一个合法的拼音，输入法一般会自动把它识别为简拼的两个拼音。</p><h1 id="双拼输入法的软件"><a href="#双拼输入法的软件" class="headerlink" title="双拼输入法的软件"></a>双拼输入法的软件</h1><p>好像没有专门支持双拼的输入法软件啊？<br>其实Windows及Android上常见的 <strong>拼音输入法软件</strong>，如搜狗、百度、QQ、谷歌、紫光等，及Windows内置的微软拼音，Linux上的Fcitx，都支持双拼，只要在设置中选择一下即可。有的输入法支持 <strong>双拼展开提示</strong>，初学时有所帮助。</p><p>一般输入法软件都支持多种双拼方案，有的还支持自定义方案。上面提到的几款软件都支持微软双拼方案，所以建议使用该方案。百度手机输入法的默认方案就是微软双拼，但韵母<code>ing</code> 对应的是 <strong><code>,</code></strong>。</p><p><img src="/img/udpn-baidu-ime.png" alt="百度拼音设置"></p><hr><h1 id="关于语音输入的杂想"><a href="#关于语音输入的杂想" class="headerlink" title="关于语音输入的杂想"></a>关于语音输入的杂想</h1><p>iPhone上的siri当年是一大卖点（只是不知现在还有没有人用），Google，微软也相继发布类似的语音产品，不过一直都不感冒。前不久某著名相声演员在自家的手机发布会上大力赞扬友商讯飞输入法，看了该相声演员的演示后，马上在手机和PC上都安装了讯飞输入法来体验一下，感觉里预期还是有点差距，总结一些想法：</p><ul><li>首先不想对着手机或电脑大声说，特别是意识到是要讲给一个输入法软件时语气就会有点奇怪。</li><li>语音输入的连续性也需要加强，目前的体验类似于对讲机，讲一句识别一句，确认有没有错误，如果有错误再去修改，不够自然。显然软件是可以根据声音自动断句的，用户应该可以以自然的语速不间断的讲下去，后台进行语音识别，间断输出，同时软件会保存录音，供后期集中校对，校对时可以提供多个备选及原始语音。校对其实就是机器学习里的有监督学习了，众多用户免费进行训练，而且是最真实场景的数据。</li><li>提供上传mp3语音文件转换后输出文本的服务，就像已经有不少识图网站，可以输出上传图片的文字描述，这也是大数据啊~~ Youtube就有一个自动为视频生成字幕的功能；还有不少公司搞即时翻译； 或许讯飞认为他们的识别准确率已经很高，不需要再训练了。。。</li></ul><blockquote><p>手机最初的功能就是语言通话，所以内置的麦克用来做语音识别是足够的，而且可以拿到嘴边轻声说话；<br>但电脑上就不一定有堪用的麦克了，一般笔记本是有麦克的，但效果难说，台式机就需要另外购买麦克了。<br>想到这里，可能淘宝客服最适合用语音识别了;-)<br>要是有一个高灵敏度，降噪，带录音，小巧，长时续航，USB充电，能无线接入电脑的麦克就好了。<br>话说这样的麦克应该接近于一个录音笔了，但增加了可以实时接入电脑作为麦克的功能。<br>弄个新硬件还是比较有难度的，也可以利用手机已有的麦克，在PC上虚拟出一个麦克，或许更容易些。这种思路类似于<code>Sensor offloading</code>，而且已经有了这样的App：<a href="http://wirelessorange.com/womic-zh/" target="_blank" rel="external">WO Mic</a>，这个是通过wifi传数据的，需要PC客户端，有时间试用一下，这家还有WO Webcam和HiMic。另外的一个App则是需要一个两头都是3.5mm公头的音频线。</p></blockquote><p>再到<a href="http://www.iflyrec.com/" target="_blank" rel="external">讯飞听见</a>的网站，发现了下面这两项产品/服务，看来想法差不多嘛;-) 不过录音笔有点小贵（相对其它品牌来说算是比较便宜了），云转换需要收钱，只好看看<a href="http://cn.technode.com/post/2016-01-13/iflyrec-examination/" target="_blank" rel="external">别人的体验</a>了。话说能花钱把会议记录转成文本的，会不会考虑保密的问题？其实关于会议系统还可以做的很多，比如做网络视频会议的思科WebEx。讯飞讯飞智能会议系统可以算是关注了会议结束后，写会议纪要的工作。不过如果能在开会时就转换成文本，开完会就通过了会议纪要，效率不就更高了吗？</p><p><a href="http://z.jd.com/project/details/34481.html" title="讯飞录音笔众筹" target="_blank" rel="external"><img src="/img/udpn-iflyrec-rec.png" alt="讯飞录音笔"></a></p><p><img src="/img/udpn-iflyrec-meeting.png" alt="讯飞智能会议系统"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;双拼输入法是一个很典型的例子：一直就在我们日常使用的软件之中“隐藏”的功能，不为人所熟知，但是一旦掌握了，用起来就会很方便。&lt;br&gt;除了介绍双拼输入法，还有一点关于语音输入的杂想。&lt;br&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>局域网内的远程操作</title>
    <link href="https://ying-zhang.github.io/misc/2016/remote/"/>
    <id>https://ying-zhang.github.io/misc/2016/remote/</id>
    <published>2016-09-17T16:00:00.000Z</published>
    <updated>2017-10-30T02:47:45.881Z</updated>
    
    <content type="html"><![CDATA[<p>一些基础的远程操作，包括ssh，共享文件，远程桌面。</p><a id="more"></a><hr><!-- TOC --><ul><li><a href="#linux%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4%EF%BC%88ssh%EF%BC%89">Linux远程执行命令（ssh）</a><ul><li><a href="#linux%E4%B8%8B%E8%AE%BE%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8ssh">Linux下设置和使用ssh</a></li><li><a href="#windows%E5%AE%89%E8%A3%85%E5%92%8C%E8%AE%BE%E7%BD%AExshell%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%AF%86%E7%A0%81ssh%E7%99%BB%E5%BD%95">Windows安装和设置xshell，使用密码ssh登录</a></li><li><a href="#scp">scp</a></li><li><a href="#sftp">sftp</a></li><li><a href="#%E8%AE%BE%E7%BD%AEssh%E4%BD%BF%E7%94%A8%E5%AF%86%E9%92%A5%E7%99%BB%E5%BD%95">设置ssh使用密钥登录</a><ul><li><a href="#%E7%94%9F%E6%88%90%E5%AF%86%E9%92%A5%E5%AF%B9">生成密钥对</a></li><li><a href="#%E5%88%86%E5%8F%91%E5%AF%86%E9%92%A5%E5%AF%B9">分发密钥对</a><ul><li><a href="#ssh-copy-id">ssh-copy-id</a></li><li><a href="#%E5%A4%8D%E5%88%B6%E5%AF%86%E9%92%A5%E6%96%87%E6%9C%AC">复制密钥文本</a></li></ul></li><li><a href="#ssh-config%E8%AE%BE%E7%BD%AE">ssh config设置</a></li><li><a href="#%E8%B8%A2%E5%87%BAssh%E4%BC%9A%E8%AF%9D">踢出ssh会话</a></li></ul></li></ul></li><li><a href="#%E8%BF%9C%E7%A8%8B%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%EF%BC%88smbcifs%EF%BC%89">远程共享文件（SMB/CIFS）</a><ul><li><a href="#samba%E8%AE%BF%E9%97%AEwindows%E6%8F%90%E4%BE%9B%E7%9A%84%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6">Samba访问Windows提供的共享文件</a></li><li><a href="#windows%E8%AE%BF%E9%97%AEsamba%E7%9A%84%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6">Windows访问Samba的共享文件</a></li><li><a href="#%E6%9B%B4%E6%94%B9samba%E7%9A%84%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3%E5%8F%B7">更改Samba的默认端口号</a></li></ul></li><li><a href="#%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2">远程桌面</a><ul><li><a href="#mstsc">mstsc</a></li><li><a href="#vnc">vnc</a></li><li><a href="#%E5%85%B6%E5%AE%83">其它</a></li></ul></li></ul><!-- /TOC --><p>这里简单介绍局域网中Windows与Linux系统之间的一些基本远程操作，包括远程执行命令（<code>ssh</code>），共享文件（<code>Samba</code>）和远程桌面（<code>mstsc</code>和<code>vnc</code>）。<br>远程操作一般是“服务器-客户端”模式，有的服务程序或客户端是操作系统内置的，开箱即用，有的程序则需要手动安装。</p><p>为了方便配置，<strong>建议关闭系统的防火墙</strong>。下面例子使用的远程Linux主机是Ubuntu 16.04，IP是<code>10.1.1.5</code>，用户名<code>ying</code>；Windows的IP是<code>10.1.1.1</code>，用户名也是<code>ying</code>。</p><h1 id="Linux远程执行命令（ssh）"><a href="#Linux远程执行命令（ssh）" class="headerlink" title="Linux远程执行命令（ssh）"></a>Linux远程执行命令（ssh）</h1><p>ssh（<a href="https://en.wikipedia.org/wiki/Secure_Shell" target="_blank" rel="external">Secure Shell</a>）通过加密的网络通道在客户端和服务器之间传递命令及其输出。<br>在ssh之前，远程执行命令是通过<code>telnet</code>或<code>rsh</code>等程序实现的，数据是明文传输的，缺乏安全性。ssh提供了一个在网络上认证用户和加密数据的通道，执行命令是直接使用的系统内置的shell。<br>可以在ssh提供的加密通道上完成其它网络通信：如<code>scp</code>是在ssh加密通道上实现的远程拷贝（<code>rcp</code>）；<code>sftp</code>是在ssh加密通道上实现的<code>ftp</code>；<code>git</code>也有使用ssh加密通道传输文件的模式。<br>对于Linux这种主要通过shell命令行交互的系统来说，使用ssh远程登录到服务器上，就跟直接在机器上敲命令就没什么区别了。</p><h2 id="Linux下设置和使用ssh"><a href="#Linux下设置和使用ssh" class="headerlink" title="Linux下设置和使用ssh"></a>Linux下设置和使用ssh</h2><p>Linux系统的ssh服务程序是<code>OpenSSH Server</code>，执行命令<code>sudo apt install openssh-server</code>。<br>安装过程中会将ssh服务程序（<code>sshd</code>）添加为开机启动的系统服务，默认设置允许当前用户通过密码登录ssh。</p><blockquote><p>查看SSH Server状态，执行<code>systemctl status sshd</code><br>启动，停止或重启服务，执行<code>sudo systemctl start/stop/restart sshd</code></p></blockquote><p>一般Linux系统都内置了ssh客户端，执行<br><code>ssh 用户名@主机名或IP</code><br>登录到远程主机（如果用户名与当前登录的用户名相同，可以省略）。<br>登录到本机的命令是<code>ssh localhost</code><br>第一次登录某个主机会提示是否 <strong>信任</strong> 该主机，需要输入<code>yes</code>，之后才会提示输入远程主机的登录密码。</p><blockquote><p>修改<code>/etc/ssh/ssh_config</code>，将其中<code>#   StrictHostKeyChecking ask</code> 改为 <code>StrictHostKeyChecking no</code>，这样在第一次登录时就不会询问是否要信任该主机了。</p></blockquote><p>如果登录到远程主机只是执行一两条命令，可执行<br><code>ssh 用户名@主机名或IP 命令</code><br>当然，每次还是需要输入密码，参考下面的设置密钥登录后就方便多了。</p><h2 id="Windows安装和设置xshell，使用密码ssh登录"><a href="#Windows安装和设置xshell，使用密码ssh登录" class="headerlink" title="Windows安装和设置xshell，使用密码ssh登录"></a>Windows安装和设置xshell，使用密码ssh登录</h2><p>Windows目前没有内置的ssh客户端，可以安装Putty、SecureCRT、xshell等ssh客户端软件，或者使用Cygwin/MinGW，git（包含MinGW），Bash on Windows等附带的ssh命令。</p><blockquote><p>Windows版的<code>git</code>包含一个简化版<code>MinGW</code>，将<code>&lt;git安装目录&gt;\usr\bin</code>这个路径添加到Windows的<code>Path</code>环境变量，就可以在Windows的命令窗口执行<code>ssh</code>，<code>scp</code>及其它很多Linux命令了。<br><code>MinGW</code> 中也包含SSH Server程序<code>sshd</code>，不过估计很少会登录到Windows执行命令行操作吧。</p></blockquote><p>从官网下载并安装 <a href="http://www.netsarang.com/download/down_xsh.html" target="_blank" rel="external">xshell</a> （需要注册一个免费的账号，选择免费的Home/School许可），也可以在百度搜索“xshell”，第一条结果即是，注意要选择 <strong>普通下载</strong>。</p><p>启动xshell后，可以直接执行<code>ssh ying@10.1.1.5</code>，会提示输入密码，首次连接也会提示“未知的主机密钥”，选择保存即可。<br><img src="/img/xshell-ui.png" alt=""></p><blockquote><p>工具栏的打开会话按钮，可以从其中选择某个会话，也可以直接在xshell中执行<code>open &lt;会话名&gt;</code>。<br>工具栏的那个带小齿轮的按钮是“默认会话属性”，修改其中的设置会影响新建的会话。<br>每个会话即<code>&lt;用户文档&gt;\NetSarang\Xshell\Sessions</code>下的一个配置文件，会话也可以复制后修改。</p></blockquote><p>为方便后续使用，可以为这个虚拟机创建一个会话。单击工具栏的“新建”按钮，在打开的 <strong>“会话属性”</strong> 对话框中</p><ul><li>在“连接” 输入主机 <code>10.1.1.5</code>，在用户身份验证中选择方法为Password，输入用户名 <code>ying</code> 和 密码；</li><li>在“终端” 修改“编码”为UTF-8；</li><li>在“外观” 修改终端字体和配色方案，我比较习惯黑底绿字的配色，使用Consolas字体，使用闪烁的光标。</li></ul><p><img src="/img/xshell-prop.png" alt=""></p><blockquote><p>注意：Windows的快捷键与Linux终端的快捷键存在冲突，如“复制”<code>Ctrl+C</code>对应的是中断当前命令。<br>xshell中默认“复制”、“粘贴”的快捷键是<code>Ctrl+Ins</code>，<code>Shift+Ins</code>，而不是<code>Ctrl+C</code>，<code>Ctrl+V</code>。<br>可以打开 “工具”-&gt;“选项”，“键盘和鼠标”选项卡，“按键对应”-&gt;“编辑”，将其修改为<code>Ctrl+C</code>，<code>Ctrl+V</code>，而原来Linux终端的快捷键需要加<code>Shift</code>，如中断当前命令的<code>Ctrl+C</code>变成了<code>Ctrl+Shift+C</code>。</p></blockquote><h2 id="scp"><a href="#scp" class="headerlink" title="scp"></a>scp</h2><p>通过ssh加密的通道传输文件。文件路径格式为<code>用户名@主机名或IP:主机上的路径</code>。注意，Windows文件路径中的盘符<code>C:\</code>变成了<code>/c/</code>。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">scp ying@10.1.1.5:/home/ying/.ssh/id_rsa.pub /c/users/ying/.ssh/</div></pre></td></tr></table></figure></p><h2 id="sftp"><a href="#sftp" class="headerlink" title="sftp"></a>sftp</h2><p><code>OpenSSH Server</code>内置了一个<code>sftp</code>服务器，会随<code>sshd</code>服务自动启动。我们还需要一个<code>sftp</code>的客户端即可传送文件。<br>这里使用图形界面的，跨平台的，免费的，开源的<a href="https://filezilla-project.org/download.php?type=client" target="_blank" rel="external">Filezilla</a>。下载并安装后，在“快速连接”工具栏输入主机<code>sftp://10.1.1.5</code>，及用户名 <code>ying</code> 和密码，端口为22，单击“快速连接”，然后就可以进行文件传输和管理了。<br><img src="/img/sftp.png" alt=""></p><p>Android上的<code>ES文件浏览器</code>也支持<code>sftp</code>（还支持下面介绍的smb局域网文件共享）。</p><h2 id="设置ssh使用密钥登录"><a href="#设置ssh使用密钥登录" class="headerlink" title="设置ssh使用密钥登录"></a>设置ssh使用密钥登录</h2><p>更安全而且方便的ssh登录方式是使用密钥对(key)。密钥对包含公钥和私钥，其实是两个很长的整数（被编码为字符串）。比如采用<code>rsa</code>算法，公钥和私钥分别保存在两个 <strong>文本</strong> 文件<code>id_rsa.pub</code>和<code>id_rsa</code>中。</p><ul><li>公钥<code>id_rsa.pub</code>保存在要登录的目标机器上（服务器，Github等），</li><li>私钥<code>id_rsa</code>保存在 <strong>发起</strong> 登录的机器上（客户端），私钥要妥善保管，防止泄露。</li></ul><p>Linux主机的密钥对默认保存在<code>~/.ssh/</code>目录。<br>Windows是<code>C:\Users\&lt;Win用户名&gt;\.ssh\</code>目录。在图形界面的文件管理器中不能创建以<code>.</code>开头的文件夹，需要在命令窗口操作：打开Windows的命令窗口（<code>Win键+X，C</code>），执行命令<code>mkdir C:\Users\&lt;Win用户名&gt;\.ssh</code>。</p><h3 id="生成密钥对"><a href="#生成密钥对" class="headerlink" title="生成密钥对"></a>生成密钥对</h3><p>因为加密算法是公开的，有多种工具可以生成密钥。<br>对Linux或MinGW，执行<code>ssh-keygen -t rsa -P &quot;&quot;</code> ，会在<code>~/.ssh/</code>生成密钥对<code>id_rsa.pub</code>和<code>id_rsa</code>。</p><p>xshell也可以生成密钥对：</p><ul><li>打开 “工具”-&gt; “新建用户密钥生成向导” 或 “工具”-&gt; “用户密钥管理者” -&gt; “生成” 生成一个密钥类型为RSA的密钥，向导的最后一步会显示公钥，可以将其保存起来；</li><li>选择刚创建的密钥，单击“导出”按钮，保存私钥，默认的格式与OpenSSH相同；</li><li>选择刚创建的密钥，单击“属性”按钮，在“公钥”选项卡中保存公钥。</li></ul><p><img src="/img/xshell-key.png" alt=""></p><h3 id="分发密钥对"><a href="#分发密钥对" class="headerlink" title="分发密钥对"></a>分发密钥对</h3><p>要启用密钥，需清除其它用户访问私钥的权限（600），并公钥拷贝到远程目标Linux主机的<code>.ssh/authorized_keys</code>文件中。<br>分发密钥对其实就是在在Windows和Linux之间传送文件，可以使用上面提到的<code>scp</code>和<code>sftp</code>，也可以参考后面要介绍的smb文件共享；或者更复杂一些，搭建一个Web服务器，把文件放到上面，在Linux执行<code>wget</code>或<code>curl</code>命令下载，Windows可以通过浏览器下载。下面还有另外两种方法。</p><h4 id="ssh-copy-id"><a href="#ssh-copy-id" class="headerlink" title="ssh-copy-id"></a>ssh-copy-id</h4><p>执行命令<code>ssh-copy-id -i 公钥文件 用户名@主机名或IP</code>，将公钥拷贝到远程Linux主机的<code>/home/&lt;用户名&gt;/.ssh/authorized_keys</code>文件中。当然，这个命令需要用密码访问远程主机。</p><h4 id="复制密钥文本"><a href="#复制密钥文本" class="headerlink" title="复制密钥文本"></a>复制密钥文本</h4><p>如将Linux主机上生成的私钥<code>id_rsa</code>拷贝到Windows上：</p><ul><li>使用xshell用密码登录到Linux，执行<code>cat ~/.ssh/id_rsa</code>，输出私钥的内容，复制输出的文字。</li><li>在Windows文件管理器中打开路径<code>C:\Users\&lt;Win用户名&gt;\.ssh</code>，在其中创建一个名为<code>id_rsa.txt</code>的文本文件，将上一步复制的文字粘贴进去，然后把文件名的<code>.txt</code>扩展名去掉，即改为<code>id_rsa</code>。</li></ul><p>可以参考上面的方式将Windows上生成的公钥拷贝到远程Linux主机上。因为还要从远程Linux主机上执行<code>git</code>、<code>ssh</code>等命令，所以也要把私钥放拷过去。当然，也可以使用不同的密钥对。</p><h3 id="ssh-config设置"><a href="#ssh-config设置" class="headerlink" title="ssh config设置"></a>ssh config设置</h3><p>在<code>~/.ssh/config</code>文件中可以设置多个远程主机的别名，地址，端口，用户名和密钥，简化ssh命令。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Host   别名</div><div class="line">    HostName 主机名或IP</div><div class="line">    Port     端口</div><div class="line">    User     用户名</div><div class="line">    IdentityFile   私钥文件</div><div class="line"></div><div class="line">Host u</div><div class="line">    Hostname  10.1.1.5</div><div class="line">    Port      22</div><div class="line">    User      ying</div><div class="line"></div><div class="line">Host          10.1.1.6</div><div class="line">Port          2222</div><div class="line"></div></pre></td></tr></table></figure></p><p>这样就可以直接执行<code>ssh 别名</code>登录指定的主机，而且不同的主机可以使用不同的端口，用户，密钥配置。别名还可以用在<code>scp</code>的路径中。</p><h3 id="踢出ssh会话"><a href="#踢出ssh会话" class="headerlink" title="踢出ssh会话"></a>踢出ssh会话</h3><p>查看在线用户：<code>w</code> 或 <code>who</code>，两者输出格式有所不同。<br>查看自己的连接信息：<code>who am i</code>。<br>踢出其它会话：<code>pkill -9 -t pts/1</code>，其中<code>pts/1</code>是被踢会话的终端。</p><h1 id="远程共享文件（SMB-CIFS）"><a href="#远程共享文件（SMB-CIFS）" class="headerlink" title="远程共享文件（SMB/CIFS）"></a>远程共享文件（SMB/CIFS）</h1><p>“共享文件”（<a href="https://en.wikipedia.org/wiki/Server_Message_Block" target="_blank" rel="external">Server Message Block，SMB</a> )，改进的版本称为Common Internet File System，CIFS），是Windows上为局域网用户提供的远程访问文件的功能。Windows内置了smb的服务程序和客户端。<br>Samba是Linux上实现SMB/CIFS协议的开源服务程序及客户端。<br>Linux上与SMB/CIFS功能是类似的是“网络文件系统”（<a href="https://en.wikipedia.org/wiki/Network_File_System" target="_blank" rel="external">Network File System，NFS</a> ）。SMB和NFS功能相似，都是文件级别（相比于块级别iSCSI等方式）的远程存储服务。Windows默认没有安装NFS功能，但可以通过<code>控制面板→程序和功能→启用或关闭Windows功能</code>来添加NFS客户端和服务端软件。</p><blockquote><p>注意：只能共享某个文件夹，不能单独共享某个文件。Windows会限制能链接的共享用户数量，如果需要提供共享文件服务，Samba是更好的选择。<br>共享配合文件系统的权限设置，可以实现精细的权限控制，比如 “只能上传，不能下载，不能删除” 这样的需求（上传作业的文件服务器）。</p></blockquote><p>Linux一般内置了smb的客户端（mount.cifs模块）。如果没有，可以执行<code>sudo apt install cifs-utils</code>来安装。</p><h2 id="Samba访问Windows提供的共享文件"><a href="#Samba访问Windows提供的共享文件" class="headerlink" title="Samba访问Windows提供的共享文件"></a>Samba访问Windows提供的共享文件</h2><p>Windows上启用共享文件夹只要在文件夹的<code>属性对话框→共享选项卡→高级共享</code>中设置即可，在这个对话框中还可以指定用户和读写权限。共享名和实际的文件夹名可以不同。如果在共享文件名后添加<code>$</code>，就表示是隐藏的，必须通过输入完整路径才能打开。<br><img src="/img/win-share.png" alt="Windows上启用共享文件夹"></p><p>创建挂载点<code>mkdir ~/z</code>，并在<code>/etc/fstab</code>中添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//10.1.1.1/文档 /home/ying/z cifs username=Win用户名,password=Win密码,uid=1000,rw,iocharset=utf8,sec=ntlm 0 0</div></pre></td></tr></table></figure></p><p>执行<code>sudo mount -a</code>，挂载<code>/etc/fstab</code>中新增的设置。<br>执行<code>ls ~/z</code>，应列出共享文件夹中的内容，确认挂载成功。</p><blockquote><p>上面的命令将共享文件夹<code>文档</code>挂载到Ubuntu的<code>/home/ying/z</code>，有读写权限。因为设置了终端编码为UTF-8，中文的文件名也能正常显示。<br>其中uid是Ubuntu中用户<code>ying</code>的，具体的值可执行命令<code>id</code>，或在<code>/etc/passwd</code>中查看。</p></blockquote><h2 id="Windows访问Samba的共享文件"><a href="#Windows访问Samba的共享文件" class="headerlink" title="Windows访问Samba的共享文件"></a>Windows访问Samba的共享文件</h2><p>先要安装<code>Samba File Server</code>，执行<code>sudo apt install samba samba-common</code>。</p><blockquote><p>查看Samba Server的运行状态，执行<code>systemctl status smbd</code><br>启动，停止或重启服务，执行<code>sudo systemctl start/stop/restart smbd</code></p></blockquote><p>添加共享文件夹：执行 <code>sudo nano /etc/samba/smb.conf</code>，在末尾添加如下内容，添加了只读的根目录<code>/</code>和可读写的<code>/home/ying</code>目录，但显示为<code>all</code>和<code>ying</code>。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[all]</div><div class="line">    comment = fs root directory</div><div class="line">    path = /</div><div class="line">;   writeable = no</div><div class="line">;   browseable = yes</div><div class="line">    valid users = ying</div><div class="line"></div><div class="line">[ying]</div><div class="line">    comment = ying&apos;s home</div><div class="line">    path = /home/ying</div><div class="line">    writeable = yes</div><div class="line">    create mask = 0664</div><div class="line">    directory mask = 0775</div><div class="line">;   browseable = yes</div><div class="line">    valid users = ying</div></pre></td></tr></table></figure></p><p>将<code>ying</code>添加为smb的共享用户：<code>sudo smbpasswd -a ying</code>， 按提示设置<code>ying</code>的smb密码，<strong>可以与系统密码不同</strong>。<br>重启smbd，使设置生效：<code>sudo systemctl restart smbd</code>。</p><blockquote><p>Samba的权限问题：Samba中的用户需要是Ubuntu已有的用户，还要给Samba的用户设置相关文件和目录的读写权限。</p></blockquote><p>从Windows的文件管理器的地址栏访问 <code>\\10.1.1.5</code> ，会看到刚添加的两个共享文件夹。可以在文件夹上右击，快捷菜单中有 <strong>“映射网络驱动器”</strong> 的选项，也可以像普通文件夹一样创建快捷方式。除了IP地址，还可以通过Ubuntu的机器名来访问，若机器名为u，则地址为<code>\\u</code> 。</p><p>从macOS和Ubuntu访问共享文件（不论Windows或Ubuntu提供的）的路径格式是<code>smb://10.1.1.5</code>或<code>smb://u</code>。macOS会自动把共享文件挂载到<code>/Volumes</code>下。</p><blockquote><p>Windows可以通过机器名来访问Ubuntu是因为Samba默认开启了局域网内的<code>WINS</code>名字服务。<br>另一种方法是在<code>hosts</code>文件中为IP地址指定名字。</p></blockquote><p><img src="/img/smb.png" alt=""></p><blockquote><p>Samba共享文件与<code>sftp</code>的区别在于，<code>sftp</code>不能直接编辑文件，必须要把文件拷贝下来后才能处理，而操作共享文件跟本机的文件没有太大区别。<br>PS, <code>testparm</code>命令可以用来检查<code>smb.conf</code>的配置是否正确。</p><p>参考</p><ul><li><a href="https://wiki.samba.org/index.php/Setting_up_Samba_as_a_Standalone_Server" target="_blank" rel="external">Setting up Samba as a Standalone Server - samba wiki</a></li><li><a href="http://lybing.blog.51cto.com/3286625/1676515" target="_blank" rel="external">在CentOS 7中Samba服务安装和配置</a></li></ul></blockquote><h2 id="更改Samba的默认端口号"><a href="#更改Samba的默认端口号" class="headerlink" title="更改Samba的默认端口号"></a>更改Samba的默认端口号</h2><p>2017年5月份的勒索病毒WanaCrypt会扫描开放445文件共享端口的Windows设备，导致网络管理员禁封了445端口。如果客户端和服务器在同一局域网，通讯都是在二层，不会受到影响，可以正常使用共享文件，但如果经过路由器，就不能使用了。实际中发现即便是在同一个局域网，另外的实验室也无法访问我们实验室的共享文件，可能两个实验室各自的交换机又连到一个三层交换机上了吧。<br>估计445一封了之，是不会再有解封之日了。好在还可以变通一下，修改Samba的端口号，绕过封锁。不爽的是，Windows的默认端口号是无法修改的，而Linux，macOS，Android的ES文件管理器都支持指定端口号，地址格式是<code>smb://10.1.1.5:4455/home/</code>，其中4455是修改后的端口号。</p><p>修改Samba的端口号只需在<code>/etc/samba/smb.conf</code>中增加<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[global]</div><div class="line">   smb ports = 4455 445  # 可以同时监听多个端口号</div><div class="line">...</div></pre></td></tr></table></figure></p><h1 id="远程桌面"><a href="#远程桌面" class="headerlink" title="远程桌面"></a>远程桌面</h1><h2 id="mstsc"><a href="#mstsc" class="headerlink" title="mstsc"></a>mstsc</h2><p>Windows除家庭版之外均内置了远程桌面服务和客户端，使用的是远程桌面协议<a href="https://en.wikipedia.org/wiki/Remote_Desktop_Protocol" target="_blank" rel="external">Remote Desktop Protocol，RDP</a>。</p><ul><li>客户端在<code>所有程序→Windows附件→远程桌面连接</code>，或直接执行命令<code>mstsc</code>。</li><li>服务程序：依次打开<code>控制面板→所有控制面板项→系统</code>，或<code>Win+X，Y</code>，然后单击左侧的<code>高级系统设置</code>，打开<code>系统属性</code>对话框，在<code>远程</code>选项卡中的<code>远程桌面</code>部分选中<code>允许远程连接到此计算机</code>，并选择某个用户。<br><img src="/img/win-mstsc.png" alt="Windows上的远程桌面客户端"><br><img src="/img/win-mstsc-svr.png" alt="Windows上启用远程桌面"></li></ul><p>Ubuntu桌面版内置了可以访问Windows远程桌面的客户端；安卓和iOS系统也有远程桌面的App，但这三个系统都没有远程桌面的服务程序，Windows无法通过mstsc远程连接到它们的图形界面。<br>如果是在安卓平板或iPad上使用远程桌面连接到Windows系统，那么Windows会自动切换到触屏模式，就相当于在使用一个Windows系统的平板了，当然是台式机的性能。</p><p>Windows远程桌面一般只支持单个用户访问，如果有用户在使用远程桌面，那么本地的就会锁屏；但是服务器版可以设置支持多个用户同时使用远程桌面，彼此都是独立的窗口。</p><h2 id="vnc"><a href="#vnc" class="headerlink" title="vnc"></a>vnc</h2><p>VNC（<a href="https://en.wikipedia.org/wiki/Virtual_Network_Computing" target="_blank" rel="external">Virtual Network Computing</a>）是Linux上的远程桌面共享协议。Linux下有多个桌面环境，如Gnome，KDE，unity，xfce等，VNC对不同桌面系统的支持不同。此外，VNC的客户端及服务端也有多种实现，如x11vnc、realvnc、tigervnc、tightvnc、ultravnc等。Ubuntu Unity下自带了<code>远程共享</code>程序实现了VNC功能。由于vnc远比ssh占用的网络带宽大，而Linux上的大部分操作可以通过ssh来执行，所以不推荐使用VNC。<br>VNC默认桌面会话使用5900端口，可以开启多个桌面会话，新的VNC桌面会话的端口号依次增加。与Windows的远程桌面不同，VNC在远程访问时不会锁屏，而是同步显示默认桌面会话的显示。</p><p>可以参考教程[<a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-vnc-on-ubuntu-16-04" target="_blank" rel="external">https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-vnc-on-ubuntu-16-04</a>] ，在Ubuntu上安装和配置xfce桌面及tightvnc服务端。</p><p>Windows上没有内置的VNC客户端，有一些免费的<code>VNC-Viewer</code>程序。</p><blockquote><p>注意： 按上面的设置启用VNC后，使用的是xfce桌面环境，<br>默认的 <code>Tab</code> 键补全终端命令与窗口管理的快捷键冲突，需要在“Settings-&gt; Window Manager -&gt; Keyboard”中清除 <code>Switch Window from same application</code> 关联的快捷键<br>还可以在 “Settings-&gt; Keyboard -&gt; application shortcut” 中设置打开终端的快捷键 <code>exo-open --launch TerminalEmulator ~ Ctrl+Alt+T</code></p></blockquote><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>在局域网之外，如果网络连接比较复杂，mstsc或vnc可能都无法穿过机构强制的防火墙。有一些远程访问软件，比如 <strong><a href="https://www.teamviewer.com" target="_blank" rel="external">TeamViewer</a></strong>，<a href="http://sunlogin.oray.com/zh_CN/" target="_blank" rel="external">向日葵</a>等，可以实现广域网情形的远程访问，前提是两端的机器都能访问Internet公网，而mstsc和vnc则不要求必须能够访问公网。另一种方法是申请机构内的VPN。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一些基础的远程操作，包括ssh，共享文件，远程桌面。&lt;/p&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>安装和设置Windows 10</title>
    <link href="https://ying-zhang.github.io/misc/2016/install-win-10/"/>
    <id>https://ying-zhang.github.io/misc/2016/install-win-10/</id>
    <published>2016-09-16T16:00:00.000Z</published>
    <updated>2017-10-30T03:42:23.339Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下安装Windows 10 LTSB 及 基本设置作为备忘。</p><a id="more"></a><!-- TOC --><ul><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF-ltsb">为什么是 LTSB</a></li><li><a href="#%E5%88%9B%E5%BB%BAusb%E5%AE%89%E8%A3%85%E7%9B%98%EF%BC%8C%E5%AE%89%E8%A3%85%E7%B3%BB%E7%BB%9F">创建USB安装盘，安装系统</a></li><li><a href="#%E7%A1%AC%E7%9B%98%E5%88%86%E5%8C%BA%E8%A7%84%E5%88%92">硬盘分区规划</a></li><li><a href="#%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE">常用快捷键</a></li><li><a href="#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE">系统设置</a></li><li><a href="#%E5%AE%89%E8%A3%85linux%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7">安装Linux常用工具</a></li><li><a href="#windows-server-2016-%E7%9A%84%E8%AE%BE%E7%BD%AE">Windows Server 2016 的设置</a></li><li><a href="#reg%E6%96%87%E4%BB%B6">Reg文件</a></li><li><a href="#%E5%85%B6%E5%AE%83">其它</a><ul><li><a href="#%E5%BC%80%E5%90%AF%E8%99%9A%E6%8B%9Fwifi">开启虚拟wifi</a></li></ul></li></ul><!-- /TOC --><h1 id="为什么是-LTSB"><a href="#为什么是-LTSB" class="headerlink" title="为什么是 LTSB"></a>为什么是 LTSB</h1><p>1、Win10 LTSB企业版没有Edge浏览器<br>2、无应用商店<br>3、无任何系统自带磁贴程序<br>4、无Cortana<br>5、在系统更新方面，用户能完全手动控制更新，选择和决定自己要的更新和驱动，更新内容和更新时间可以随意控制，但不能无限期推迟。<br>总结起来就是微软官方精简版啊。而且学校的KMS可以正常激活2015版的LTSB。<br>此外，还可以使用有学校后缀的Email在微软的[<a href="https://imagine.microsoft.com/zh-cn" target="_blank" rel="external">https://imagine.microsoft.com/zh-cn</a>] 认证一个学生帐号，即可免费获得一个Windows Server 2016版的密钥。Win Server也是精简的，还可以彻底卸载Defender。</p><h1 id="创建USB安装盘，安装系统"><a href="#创建USB安装盘，安装系统" class="headerlink" title="创建USB安装盘，安装系统"></a>创建USB安装盘，安装系统</h1><p>搜索<code>cn_windows_10_enterprise_2016_ltsb_x64_dvd_9060409.iso</code> 或在[<a href="http://msdn.itellyou.cn" target="_blank" rel="external">http://msdn.itellyou.cn</a>] 找到下载的ED2K链接，下载后用<a href="http://rufus.akeo.ie/" target="_blank" rel="external">Rufus</a> 制作USB启动安装盘，注意要 <strong>取消</strong> <code>高级选项</code> 中的 <code>使用Rufus MBR配合 BIOS ID...</code>。<br>然后重启机器，选择从U盘启动，按界面一步步安装系统即可。新建的用户一定要设置密码，方便后面远程操作。</p><h1 id="硬盘分区规划"><a href="#硬盘分区规划" class="headerlink" title="硬盘分区规划"></a>硬盘分区规划</h1><p>买一个SSD，只分一个区，即C盘，系统装在上面，以后的应用程序一般也按默认路径装在C盘。机器自带的机械硬盘，也只分一个区，保存个人数据。如果只有一块硬盘，也只要分一个系统分区和一个数据分区即可。多分无用，平添麻烦。如果需要Linux，装在虚拟机里，不要搞双系统。</p><h1 id="常用快捷键"><a href="#常用快捷键" class="headerlink" title="常用快捷键"></a>常用快捷键</h1><ul><li>Win + D : 切换显示桌面</li><li>Win + E : 打开<code>文件资源管理器</code></li><li>Win + R : 打开<code>运行</code>对话框</li><li>Win + X : 打开<code>开始</code>按钮的右键快捷菜单，<em>基本</em> 对应于<code>C:\Users\Ying\AppData\Local\Microsoft\Windows\WinX</code>目录的快捷方式<ul><li>F : 程序和功能（即弹出上面的菜单后，松开手，再按小写的 <code>F</code> 键，或写成<code>Win+X, F</code>，这些键都在每个菜单项后面标出来了）</li><li>P : 控制面板</li><li>Y : 系统属性</li><li>C/A : 命令窗口</li><li>G : 计算机管理</li><li>U, I : 注销</li><li>U, R : 重启</li><li>U, U : 关机</li></ul></li></ul><blockquote><p><a href="http://blog.sina.com.cn/s/blog_a0c06a350102y239.html" target="_blank" rel="external">向Win+X添加快捷方式</a>。为了方便使用git附带的minGW，创建了一个指向<code>git-bash.exe</code>的快捷方式。可以在<code>C:\Users\Ying\.bash_profile</code>增加一个<code>cd /E/Code</code>的命令来修改<code>git-bash</code>的默认启动目录。</p></blockquote><h1 id="系统设置"><a href="#系统设置" class="headerlink" title="系统设置"></a>系统设置</h1><ol><li>安装系统更新（会自动安装大部分驱动）</li><li>在<code>系统属性</code>对话框中<ul><li>更改计算机名</li><li>调整视觉效果，禁用虚拟内存，修改默认启动项和等待时间，修改环境变量</li><li>禁用系统保护</li><li>禁用远程协助，开启远程桌面</li></ul></li><li>关闭休眠：Win+X, A，然后再命令行输入 <code>powercfg /h off</code></li><li>在每个分区，打开<code>分区属性</code>对话框，取消<code>除了文件属性外，还允许索引此驱动器上的文件内容</code>；磁盘清理，删除<code>Windows.old</code></li><li>安装字体，如<code>Microsoft Yahei Mono</code>，更改命令行窗口的默认字体（使用Microsoft Yahei Mono，见下面的注册表项）、颜色和半透明效果；更改记事本的字体</li><li>更改IE主页为<code>about:tabs</code>，删除内置的加速器</li><li>在<code>文件夹选项</code><ul><li>设置<code>打开文件资源管理器时打开</code>此电脑</li><li>取消隐私相关选项</li><li><code>查看</code>中将大图标文件夹视图<code>应用到文件夹</code></li><li>取消<code>使用共享向导</code></li><li>取消<code>始终显示图标，从不显示缩略图</code></li><li>取消<code>隐藏受保护的操作系统文件</code>，选择<code>显示隐藏的文件、文件夹或驱动器</code></li><li>取消<code>隐藏已知文件类型的扩展名</code></li></ul></li><li>在<code>个性化-&gt;主题-&gt;桌面图标设置</code>中选中显示<code>计算机</code>桌面图标</li><li>任务栏上不显示搜索和任务视图按钮，使用小图标，显示所有托盘图标</li><li>禁用防火墙，取消<code>安全性与维护</code>中的所有消息</li><li>禁用Defender，：组策略，计算机配置，管理模板，Windows组件，Windows Defender</li><li>禁用contana：组策略，计算机配置，管理模板，Windows组件，搜索</li><li>安装<code>百度输入法</code>，设置使用双拼；安装<code>7zip</code>，<code>迅雷精简版</code>，<code>Everything</code></li><li>移动系统默认文件夹<ul><li>在D盘新建<code>D:\桌面</code>，<code>D:\下载</code>，<code>D:\App</code>，<code>D:\文档\音乐</code>，<code>D:文档\图片</code>，<code>D:\文档\视频</code>，<code>D:文档\收藏夹</code></li><li>在<code>C:\Users\Ying\</code>各系统默认文件夹的属性对话框的<code>位置</code>选项卡中，将其移动到对应的D盘新位置</li><li><code>D:\App</code>存放一些绿色免安装的程序</li><li><strong>不要</strong> 使用以前的更改注册表的方法来移动默认文件夹</li></ul></li><li>删除/合并开始菜单项<ul><li><code>C:\ProgramData\Microsoft\Windows\Start Menu</code></li><li><code>C:\Users\Ying\AppData\Roaming\Microsoft\Windows\Start Menu</code></li></ul></li><li>禁用某些服务</li><li>安装360，检查/清理系统，优化设置，然后卸掉；下载魔方PC Master绿色版，删掉自动更新等无关程序，使用<code>cleanmaster.exe</code>清理磁盘和隐私项，使用<code>winmaster.exe</code>清理右键菜单</li></ol><p>Ghost制作系统备份。之后安装Office，Acrobat，QQ影音，Paint.net，xshell，Chrome，QQ，微信，百度云，Visual Studio Code，VMWare，MikTex，TexStudio，Jetbrains全家桶，打印机驱动等。</p><h1 id="安装Linux常用工具"><a href="#安装Linux常用工具" class="headerlink" title="安装Linux常用工具"></a>安装Linux常用工具</h1><p>安装git（不是Github，当然也可以），其中自带<code>MinGW</code>，其中有<code>bash</code>、<code>curl</code>、<code>vim</code>、<code>ssh</code>等工具。<br>在<code>PATH</code>环境变量中添上其路径（<code>D:\App\git\mingw64\bin</code>，<code>D:\App\Git\usr\bin</code>和<code>D:\App\Git\bin</code>）即可。<br>还可以安装 <a href="https://msdn.microsoft.com/en-us/commandline/wsl/install_guide" target="_blank" rel="external">Linux Subsystem on Windows（WSL）</a> 和 ‎Bash on Ubuntu on Windows。</p><h1 id="Windows-Server-2016-的设置"><a href="#Windows-Server-2016-的设置" class="headerlink" title="Windows Server 2016 的设置"></a>Windows Server 2016 的设置</h1><ul><li>安装系统更新，鲁大师安装不能识别的设备驱动</li><li>设置Windows audio服务为自动启动</li><li>卸载Defender</li><li>组策略-&gt; 计算机配置<ul><li>Windows设置-&gt; 安全设置-&gt; 帐户策略-&gt; 密码策略-&gt; 密码必须符合复杂性要求</li><li>Windows设置-&gt; 安全设置-&gt; 本地策略-&gt; 安全选项-&gt; 交互式登录：无须按Ctrl+Alt+Del</li><li>管理模板-&gt; 系统-&gt; 显示“关闭事件跟踪程序”</li></ul></li></ul><h1 id="Reg文件"><a href="#Reg文件" class="headerlink" title="Reg文件"></a>Reg文件</h1><p><a href="/doc/win10.reg">直接下载Win10.reg</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">Windows Registry Editor Version 5.00</div><div class="line"></div><div class="line">;更改命令行窗口字体</div><div class="line">[HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Console\TrueTypeFont]</div><div class="line">&quot;0&quot;=&quot;Lucida Console&quot;</div><div class="line">&quot;936&quot;=&quot;*Microsoft YaHei Mono&quot;</div><div class="line">&quot;00&quot;=&quot;Consolas&quot;</div><div class="line"></div><div class="line">;开启的分区共享(C$, D$...)</div><div class="line">[HKEY_LOCAL_MACHINE\Software\Microsoft\Windows\CurrentVersion\Policies\System]</div><div class="line">&quot;LocalAccountTokenFilterPolicy&quot;=dword:00000001</div><div class="line"></div><div class="line">;清除通知区域图标，需重启文件资源管理器</div><div class="line">[-HKEY_CURRENT_USER\Software\Classes\LocalSettings\Software\Microsoft\Windows\CurrentVersion\TrayNotify\PastIconsStream]</div><div class="line">[-HKEY_CURRENT_USER\Software\Classes\LocalSettings\Software\Microsoft\Windows\CurrentVersion\TrayNotify\IconStreams]</div><div class="line"></div><div class="line">;删除“此电脑”下的6个文件夹</div><div class="line">;视频</div><div class="line">[-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\&#123;f86fa3ab-70d2-4fc7-9c99-fcbf05467f3a&#125;]</div><div class="line">;文档</div><div class="line">[-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\&#123;d3162b92-9365-467a-956b-92703aca08af&#125;]</div><div class="line">;桌面</div><div class="line">[-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\&#123;B4BFCC3A-DB2C-424C-B029-7FE99A87C641&#125;]</div><div class="line">;音乐</div><div class="line">[-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\&#123;3dfdf296-dbec-4fb4-81d1-6a3438bcf4de&#125;]</div><div class="line">;下载</div><div class="line">[-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\&#123;088e3905-0323-4b02-9826-5d99428e115f&#125;]</div><div class="line">;图片</div><div class="line">[-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\&#123;24ad3ad4-a569-4530-98e1-ab02f9417aa8&#125;]</div><div class="line"></div><div class="line">;删除文件资源管理器左侧的OneDrive</div><div class="line">[HKEY_CLASSES_ROOT\CLSID\&#123;018D5C66-4533-4307-9B53-224DE2ED1FE6&#125;\ShellFolder]</div><div class="line">&quot;FolderValueFlags&quot;=dword:00000028</div><div class="line">&quot;Attributes&quot;=dword:f090004d</div><div class="line"></div><div class="line">;使用“照片查看器”打开图片</div><div class="line">[HKEY_CURRENT_USER\Software\Classes\.jpg]</div><div class="line">@=&quot;PhotoViewer.FileAssoc.Tiff&quot;</div><div class="line">[HKEY_CURRENT_USER\Software\Classes\.jpeg]</div><div class="line">@=&quot;PhotoViewer.FileAssoc.Tiff&quot;</div><div class="line">[HKEY_CURRENT_USER\Software\Classes\.gif]</div><div class="line">@=&quot;PhotoViewer.FileAssoc.Tiff&quot;</div><div class="line">[HKEY_CURRENT_USER\Software\Classes\.png]</div><div class="line">@=&quot;PhotoViewer.FileAssoc.Tiff&quot;</div><div class="line">[HKEY_CURRENT_USER\Software\Classes\.bmp]</div><div class="line">@=&quot;PhotoViewer.FileAssoc.Tiff&quot;</div><div class="line">[HKEY_CURRENT_USER\Software\Classes\.tiff]</div><div class="line">@=&quot;PhotoViewer.FileAssoc.Tiff&quot;</div></pre></td></tr></table></figure><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><h2 id="开启虚拟wifi"><a href="#开启虚拟wifi" class="headerlink" title="开启虚拟wifi"></a>开启虚拟wifi</h2><p>设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netsh wlan set hostednetwork mode=allow ssid=Ying key=12345678</div></pre></td></tr></table></figure></p><p>开启<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netsh wlan start hostednetwork</div></pre></td></tr></table></figure></p><p>连接外网<br>有线网卡 的属性中选择<code>共享</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下安装Windows 10 LTSB 及 基本设置作为备忘。&lt;/p&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
  <entry>
    <title>安装Ubuntu Server 16.04 lts</title>
    <link href="https://ying-zhang.github.io/misc/2016/install-ubuntu/"/>
    <id>https://ying-zhang.github.io/misc/2016/install-ubuntu/</id>
    <published>2016-09-15T16:00:00.000Z</published>
    <updated>2017-10-30T02:47:52.591Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下安装Ubuntu Server 16.04 lts 及 基本设置作为备忘。</p><a id="more"></a><hr><!-- TOC --><ul><li><a href="#%E8%AF%B4%E6%98%8E">说明</a><ul><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF1604">为什么是16.04</a></li><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AFserver%E7%89%88">为什么是Server版</a></li><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-vm">为什么使用 VM</a></li></ul></li><li><a href="#%E5%9C%A8%E8%99%9A%E6%8B%9F%E4%B8%AD%E5%AE%89%E8%A3%85ubuntu">在虚拟中安装Ubuntu</a><ul><li><a href="#virtualbox%E7%9A%84%E5%85%A8%E5%B1%80%E8%AE%BE%E5%AE%9A">Virtualbox的全局设定</a></li><li><a href="#%E5%AE%89%E8%A3%85ubuntu">安装Ubuntu</a></li></ul></li><li><a href="#%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE">基本配置</a><ul><li><a href="#%E8%AE%BE%E7%BD%AE%E7%BD%91%E7%BB%9C">设置网络</a></li><li><a href="#%E7%B3%BB%E7%BB%9F%E6%9B%B4%E6%96%B0%E5%92%8C%E5%8D%87%E7%BA%A7">系统更新和升级</a></li><li><a href="#%E5%8F%AF%E9%80%89-%E5%88%87%E6%8D%A2%E4%B8%BAroot%E7%94%A8%E6%88%B7">[可选] 切换为root用户</a></li><li><a href="#%E5%8F%AF%E9%80%89-%E8%AE%BE%E7%BD%AEsudo%E5%85%8D%E5%AF%86%E7%A0%81">[可选] 设置sudo免密码</a></li><li><a href="#%E6%9D%82%E9%A1%B9%E8%AE%BE%E7%BD%AE">杂项设置</a></li><li><a href="#tips">Tips</a><ul><li><a href="#path%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">PATH环境变量</a></li><li><a href="#which%E5%91%BD%E4%BB%A4"><code>which</code>命令</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%B3%BB%E7%BB%9F%E7%89%88%E6%9C%AC">查看系统版本</a></li></ul></li></ul></li></ul><!-- /TOC --><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><h2 id="为什么是16-04"><a href="#为什么是16-04" class="headerlink" title="为什么是16.04"></a>为什么是16.04</h2><ul><li>16.04是lts版，相比非lts要稳定一些(不会瞎折腾)；虽然14.04也是lts版，但毕竟2014年都已经过去很久了。</li><li>16.04的软件源更新比较快，而且可以用更简洁的<code>apt</code>命令代替<code>apt-get</code>来安装程序。</li><li>另一个原因是<a href="https://zh.wikipedia.org/wiki/Systemd" target="_blank" rel="external">systemd</a>。<code>systemd</code>参考了Mac OS X的<code>launchd</code>，是一个替代<code>init</code>程序的系统服务管理组件。其它发行版，如RHEL/CentOS，CoreOS都已经采用了<code>systemd</code>。Ubuntu 14.04 lts采用的是<code>upstart</code>，直到15.04版才转用<code>systemd</code>。在运行一些 <strong>dokcer</strong>，<strong>etcd</strong> 等服务程序的示例时，经常会看到用<code>systemd</code>的unit文件定义的系统服务(service)，不能直接拿来用在Ubuntu 14.04 lts。当然在15.04版及以后的版本就可以了，由于一些默认路径不同，不同发行版的unit文件可能还是需要适当修改。</li></ul><h2 id="为什么是Server版"><a href="#为什么是Server版" class="headerlink" title="为什么是Server版"></a>为什么是Server版</h2><p>因为在Ubuntu下只是在 <strong>敲命令</strong>，极少有非使用图形界面不可的情况，又何必忍受Ubuntu臃肿的GUI呢。ssh远程登录到系统上，所有操作都通过命令行搞定，不必费心某个软件没有Linux版，或者QQ、输入法的功能弱，也不用被 <strong>Ubuntu桌面版默认强制</strong> 安装一堆充满bug的办公软件，多媒体软件等。</p><h2 id="为什么使用-VM"><a href="#为什么使用-VM" class="headerlink" title="为什么使用 VM"></a>为什么使用 VM</h2><p>如果有两台机器，一个装Ubuntu Server，连上网络然后扔在一边（只是一个机箱，都不需要显示器、键盘鼠标），另一个装Windows，远程访问即可。<br>如果只有一台机器，那么用Virtulbox创建虚拟机，安装一个Server版，平时可以选择“无界面启动”(<code>headless</code>)，没有多余的窗口，然后就跟使用物理服务器一样ssh远程登录到系统上。</p><blockquote><p>如果通过ssh远程执行命令，就不必使用vbox的虚拟机窗口了。启动虚拟机时，可以选择“无界面启动”，也可以在Windows命令行启动虚拟机<br>“C:\Program Files\Oracle\VirtualBox\VBoxManage.exe” startvm &lt;虚拟机名&gt; –type headless<br>其中<code>C:\Program Files\Oracle\VirtualBox\</code>是vbox的安装路径，可以把它添加到<code>PATH</code>环境变量中。<br><img src="/img/vbox-headless.png" alt=""></p></blockquote><p>使用VM比物理机器更好的地方，</p><ul><li>一是可以使用主机的网络，对学校网络这种外网帐号只能在一处使用的场景很方便；</li><li>另一点是可以利用VM的 <strong>快照功能</strong> 方便地进行全系统的备份和恢复。</li></ul><p>VM相比物理机器的性能损失，或者较少的CPU、内存资源其实影响并不大，毕竟云计算中都在普遍使用VM嘛。<br>即便是Mac OS X这样的Unix环境，也建议使用VM安装Ubuntu，除了上面两个优点，还可以防止误操作弄乱系统，另外可以避免Mac OS X内置命令与Linux不兼容的困扰。<br>不建议在物理机器上安装双系统。</p><h1 id="在虚拟中安装Ubuntu"><a href="#在虚拟中安装Ubuntu" class="headerlink" title="在虚拟中安装Ubuntu"></a>在虚拟中安装Ubuntu</h1><h2 id="Virtualbox的全局设定"><a href="#Virtualbox的全局设定" class="headerlink" title="Virtualbox的全局设定"></a>Virtualbox的全局设定</h2><p>安装好Virtualbox后，可以在<code>管理-&gt;全局设定</code>中修改<code>默认虚拟电脑位置</code>；另外在<code>网络</code>中添加<code>Nat网络</code>，并修改IP网段；修改已经默认添加的<code>仅主机(Host-Only)网络</code>的IP网段。<br>如下图，设置Nat网络的IP网段为<code>10.0.1.0/24</code>，设置Host-Only网络的<code>主机虚拟网络界面</code>(即vbox的虚拟网卡)的IP为<code>10.1.1.1</code>，并启用DHCP服务器，设置其IP网段。当然使用Host-Only虚拟网卡默认的 <code>192.168.56.0</code>网段也是可以的。这里是为了说明如何设置任意的（私网）IP网段，二是为了以后敲命令时IP较简短。<br><img src="/img/vboxconf-NatNetwork.png" alt="添加Nat网络，并修改IP网段"></p><p><img src="/img/vboxconf-hostonly.png" alt="修改默认Host-Only网络的IP网段"></p><h2 id="安装Ubuntu"><a href="#安装Ubuntu" class="headerlink" title="安装Ubuntu"></a>安装Ubuntu</h2><p>从Ubuntu官网下载<a href="http://releases.ubuntu.com/16.04/ubuntu-16.04.2-server-amd64.iso" target="_blank" rel="external">Server 16.04.2 的.iso镜像</a>。<br>在vbox中新建Linux类型虚拟机，选择Ubuntu 64位，虚拟磁盘使用vhd格式，动态扩展，将下载的.iso镜像挂载到虚拟机，启动虚拟机后开始安装。<br><img src="/img/vboxsetting-store.png" alt=""></p><ul><li><strong>[可选]</strong> 安装系统前先 <strong>不接入网络</strong>，即在VM设置中不勾选<code>启用网络连接</code>，以免安装过程中联网更新耗时较长。</li><li><strong>[强烈建议]</strong> 安装时语言选择为 <strong>英文</strong>：如果选择默认语言为中文，安装后一些命令会显示中文的帮助信息，结果因为终端不能显示中文而变成乱码，造成不便。英文系统也是使用UTF8编码，可以在ssh客户端正常显示中文。但<code>Location</code>要选择实际的区域，以匹配正确的时区。</li></ul><p>在最后的步骤中选中<code>Samba File Server</code>，<code>Standard System Utility</code>和<code>OpenSSH Server</code>（<code>空格键</code>选择或取消选择，<code>回车键</code>确认并继续下一步）。</p><blockquote><p>如果安装前没有接入网络，则安装后关闭虚拟机，添加“NAT网络”和“仅主机(Host-Only)网卡”两个网卡。</p></blockquote><p><img src="/img/vboxsetting-net.png" alt=""></p><p>Server没有图形界面，启动系统，进入的是下面这样一个黑乎乎的界面。输入安装时设置的用户名（这里是<code>ying</code>），回车，输入密码（没有任何显示），回车，进入系统的<code>shell</code>。<br><img src="/img/vm-tty.png" alt=""></p><h1 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h1><h2 id="设置网络"><a href="#设置网络" class="headerlink" title="设置网络"></a>设置网络</h2><p>如果安装时没有接入网络，添加网卡后需要在<code>/etc/network/interfaces</code>中设置一下。<br>Ubuntu 16.04的网卡命名不是以前的类似<code>eth0</code>，<code>eth1</code>，而是<code>enp0s?</code>这样。<code>?</code>代表一个数字，<br>执行<code>ip a</code>，如上图，输出的<code>2: enp0s3...</code>和<code>3: enp0s8...</code>就是已经识别出的网卡名。</p><p>设置网卡：执行<code>sudo nano /etc/network/interfaces</code>，使用<code>nano</code>编辑<code>/etc/network/interfaces</code>，<strong>增加</strong> 下面的内容<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">auto  enp0s3       # NAT网络，用于连接外网</div><div class="line">iface enp0s3 inet dhcp</div><div class="line"></div><div class="line">auto  enp0s8       # Host-Only网卡，设置静态IP，内网</div><div class="line">iface enp0s8 inet static</div><div class="line">address 10.1.1.5</div><div class="line">netmask 255.255.255.0</div></pre></td></tr></table></figure></p><p>按 <code>Ctrl+X</code> 快捷键，再按 <code>Y</code> 键，回车，保存并退出<code>nano</code>。</p><blockquote><p><code>nano</code> 是一个简单的命令行文本编辑器，功能比较弱，但比<code>vim</code>直观一些。</p></blockquote><p>启动网卡：执行<code>sudo ifup enp0s3 enp0s8</code>，再次执行<code>ip a</code>，可以看到这两个网卡已经获取了IP地址。</p><p>确认网卡工作正常：</p><ul><li>Nat网络连接外网：执行<code>curl ip.cn</code>，应返回一个IP地址和乱码（乱码是IP地址对应的中文的地理位置）</li><li>Host-only连接内网（主机）：执行<code>ping 10.1.1.1 -c 5</code>，应该能ping通。</li></ul><blockquote><p>网络工作正常后，就可以通过ssh登录到虚拟机来执行命令了。</p></blockquote><h2 id="系统更新和升级"><a href="#系统更新和升级" class="headerlink" title="系统更新和升级"></a>系统更新和升级</h2><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">sudo apt update   # 更新apt</div><div class="line">sudo apt upgrade  # 升级系统，可能耗时较长。</div></pre></td></tr></table></figure><p>如果网络速度较慢，可将apt源更换为<a href="http://mirrors.163.com/.help/ubuntu.html" target="_blank" rel="external">国内163</a>的，或者[<a href="http://mirrors.nju.edu.cn]，其中Ubuntu" target="_blank" rel="external">http://mirrors.nju.edu.cn]，其中Ubuntu</a> 16.04的代号是<code>xenial</code>。</p><h2 id="可选-切换为root用户"><a href="#可选-切换为root用户" class="headerlink" title="[可选] 切换为root用户"></a>[可选] 切换为root用户</h2><p>安装系统时设置的的用户有<code>sudo</code>权限。直接使用<code>root</code> <strong>不是</strong> 一种好的做法，不过可以省去很多权限相关的问题和很多命令前面的<code>sudo</code> 。<br>执行<code>sudo passwd</code>，先输入当前用户的密码以授权<code>sudo</code>，然后输入两次<code>root</code>的登录密码。执行<code>exit</code>注销当前用户，以<code>root</code>和刚才设置的密码登录。</p><p>ssh默认禁止<code>root</code>用密码登录。可以修改<code>/etc/ssh/sshd_config</code>允许<code>root</code>使用密码登录，或为<code>root</code>设置使用密钥登录。</p><h2 id="可选-设置sudo免密码"><a href="#可选-设置sudo免密码" class="headerlink" title="[可选] 设置sudo免密码"></a>[可选] 设置sudo免密码</h2><p>如果不想直接使用<code>root</code>用户，还可以设置执行<code>sudo</code>时 <strong>免输密码</strong>。执行<code>sudo visudo</code>(实际上是<code>nano</code>编辑器)，找到 <code>%sudo    ALL=(ALL:ALL) ALL</code>这一行，改为<code>%sudo    ALL=(ALL:ALL) NOPASSWD: ALL</code> 。</p><h2 id="杂项设置"><a href="#杂项设置" class="headerlink" title="杂项设置"></a>杂项设置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">; 安装常用软件</div><div class="line">sudo apt install git zsh tree</div><div class="line"></div><div class="line">; 设置Git的全局用户名和Email</div><div class="line">git config --global user.name  &quot;ZHANG Ying&quot;</div><div class="line">git config --global user.email &quot;your@email.com&quot;</div><div class="line"></div><div class="line">; 设置Git换行符转换规则，input选项会把Windows下的CRLF换行符转换成Linux下的LF</div><div class="line">; 参考https://git-scm.com/book/be/v2/Customizing-Git-Git-Configuration</div><div class="line">git config --global core.autocrlf input  # true会将LF转换为CRLF，false则不做任何处理</div><div class="line"></div><div class="line">; 设置zsh和oh-my-zsh</div><div class="line">sudo chsh ying -s /usr/bin/zsh</div><div class="line">wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh</div><div class="line">; 使用短网址  </div><div class="line">wget https://git.io/SM81Wg -O - | sh</div><div class="line"></div><div class="line">; 注销后重新登录，默认的shell已经从bash（dash）切换到了zsh。</div><div class="line"></div><div class="line">; 以下修改~/.zshrc，执行 source ~/.zshrc 应用更改</div><div class="line">; 禁用oh-my-zsh的自动更新，取消下面一行的注释</div><div class="line">DISABLE_AUTO_UPDATE=&quot;true&quot;</div><div class="line"></div><div class="line">; 默认已禁用oh-my-zsh的PATH，可在/etc/enviroment改PATH</div><div class="line"></div><div class="line">; 增加alias</div><div class="line">alias cls=&quot;clear&quot;</div><div class="line">alias dir=&quot;ls -alF&quot;</div><div class="line">alias ipconfig=&quot;ifconfig&quot;</div><div class="line">alias ping=&quot;ping -c 3&quot;</div><div class="line"></div><div class="line">alias ll=&quot;ls -alF&quot;</div><div class="line">alias ps=&quot;ps -af&quot;</div><div class="line">alias netstat=&quot;netstat -nap&quot;</div><div class="line">alias json=&quot;python -m json.tool&quot;</div></pre></td></tr></table></figure><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><h3 id="PATH环境变量"><a href="#PATH环境变量" class="headerlink" title="PATH环境变量"></a>PATH环境变量</h3><p>Linux下的绝大多数命令，其实是对应着一个可执行程序的 <strong>文件名</strong>。这些可执行文件分散在 <code>PATH</code> 环境变量设置的目录列表中，比如<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">echo $PATH</div><div class="line">; 输出为</div><div class="line">; /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games</div></pre></td></tr></table></figure></p><p>可以查看这些目录中都有哪些可执行程序，也就是系统支持的命令了。比如Ubuntu系统安装后已经内置了Python2，Python3，Ruby，Perl，它们的可执行程序都在<code>/usr/bin</code>，可以通过<code>ls /usr/bin</code>来确认。<br>如果在上面的目录中都找不到要执行的程序文件名，那么就会得到 <em>无法找到命令</em> 的错误。如果不是敲错了命令，那么就需要完整的程序路径，或者将程序所在目录加到<code>PATH</code>。<br>当前目录（<code>.</code>）默认没有加入到<code>PATH</code>中去，这是出于安全考虑。要执行当前目录下的程序或脚本，需要<code>./foo.sh</code>这样。</p><h3 id="which命令"><a href="#which命令" class="headerlink" title="which命令"></a><code>which</code>命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ which which</div><div class="line">which: shell built-in command</div><div class="line"></div><div class="line">$ which echo</div><div class="line">echo: shell built-in command</div><div class="line"></div><div class="line">$ which python</div><div class="line">/usr/bin/python</div></pre></td></tr></table></figure><h3 id="查看系统版本"><a href="#查看系统版本" class="headerlink" title="查看系统版本"></a>查看系统版本</h3><p>查看发行版本和代号，执行<code>cat /etc/os-release</code><br>查看内核版本，执行<code>uname -a</code></p><hr><p>设置完成后，关闭VM，在vbox中创建一个快照，另外可以再将虚拟磁盘压缩备份。<br><img src="/img/vbox-snapshot.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下安装Ubuntu Server 16.04 lts 及 基本设置作为备忘。&lt;/p&gt;
    
    </summary>
    
      <category term="misc" scheme="https://ying-zhang.github.io/categories/misc/"/>
    
    
  </entry>
  
</feed>
